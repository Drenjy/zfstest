#
# CDDL HEADER START
#
# The contents of this file are subject to the terms of the
# Common Development and Distribution License (the "License").
# You may not use this file except in compliance with the License.
#
# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
# or http://www.opensolaris.org/os/licensing.
# See the License for the specific language governing permissions
# and limitations under the License.
#
# When distributing Covered Code, include this CDDL HEADER in each
# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
# If applicable, add the following below this CDDL HEADER, with the
# fields enclosed by brackets "[]" replaced with your own identifying
# information: Portions Copyright [yyyy] [name of copyright owner]
#
# CDDL HEADER END
#

#
# Copyright 2009 Sun Microsystems, Inc.  All rights reserved.
# Use is subject to license terms.
#
###############################################################################
# __stc_assertion_start
#
# ID: slog_stress_001
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	mirrored pools. Create new filesystem and write file at the same time
#	shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several mirrored pools each with a
#	   separate intent log
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per mirror in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per mirror in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per mirror in the
# 	   background.
#	6. Create three datasets in each pool and start writing files in
#	   background.
#	7. Wait for 30 seconds, then repeat the operation at step 2 - 8.
#	8. Start writing to the same files, making holes in those files in
#	   background.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: raidz_stress_002
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	raidz pools. Create new filesystem and write file at the same time.
# 	At the same time, synchronize it every 10 seconds.
#	The system shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several raidz pools
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per pool in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per pool in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per pool in the
# 	   background.
#	5. Create three datasets in each pool and start writing files in
#	   background.
#	6. Wait for 30 seconds, then repeat the operation at step 2 - 7.
#	7. Start writing to the same files, making holes in those files in
#	   background.
#	8. Sync every 10 seconds.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mirror_stress_003
#
# DESCRIPTION:
#	running multiple copies of zfs_dataset_create_write_destroy and
#       zfs_dataset_create_write_destroy_attr on separate mirrored pools
#       shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	the setup phase will have created several mirrored pools
#	multiple copies of zfs_dataset_create_write_destroy and 
#         zfs_dataset_create_write_destroy_attr are fired off
#	  one per mirror in the background
#	Wait for our stress timeout value to finish, and kill any remaining
#       tests.
#	The test is considered to have passed if the machine stays up during the
#       time the stress tests are running and doesn't hit the stf time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_stress_002
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	raidz pools. Create new filesystem and write file at the same time.
# 	At the same time, synchronize it every 10 seconds.
#	The system shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several raidz pools each with
#	   a separate intent log.
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per pool in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per pool in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per pool in the
# 	   background.
#	5. Create three datasets in each pool and start writing files in
#	   background.
#	6. Wait for 30 seconds, then repeat the operation at step 2 - 7.
#	7. Start writing to the same files, making holes in those files in
#	   background.
#	8. Sync every 10 seconds.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: raidz_stress_001
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	raidz pools. Create new filesystem and write file at the same time
#	shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several raidz pools
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per raidz in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per raidz in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per raidz in the
# 	   background.
#	6. Create three datasets in each pool and start writing files in
#	   background.
#	7. Wait for 30 seconds, then repeat the operation at step 2 - 8.
#	8. Start writing to the same files, making holes in those files in
#	   background.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mirror_stress_004
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	mirrored pools shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several mirrored pools
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per mirror in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per mirror in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per mirror in the
# 	   background.
#	5. Wait for 10 seconds, then repeat the operation at step 2,3,4.
#	6. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mirror_stress_006
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	mirrored pools. Create new filesystem and write file at the same time.
# 	At the same time, synchronize it every 10 seconds.
#	The system shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several mirrored pools
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per mirror in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per mirror in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per mirror in the
# 	   background.
#	5. Create three datasets in each pool and start writing files in
#	   background.
#	6. Wait for 30 seconds, then repeat the operation at step 2 - 7.
#	7. Start writing to the same files, making holes in those files in
#	   background.
#	8. Sync every 10 seconds.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mirror_stress_005
#
# DESCRIPTION:
#	Running multiple copies of dataset_create_write_destroy,
#	dataset_create_write_destroy_attr and dataset_xattr on separate 
#	mirrored pools. Create new filesystem and write file at the same time
#	shall not cause the system to fail, hang or panic.
#
# STRATEGY:
#	1. Setup phase will have created several mirrored pools
#	2. Multiple copies of dataset_create_write_destroy are fired off
#	   one per mirror in the background.
#	3. Multiple copies of dataset_create_write_destroy_attr are filed off
#	   one per mirror in the background.
# 	4. Multiple copies of dataset_xattr are filed off one per mirror in the
# 	   background.
#	6. Create three datasets in each pool and start writing files in
#	   background.
#	7. Wait for 30 seconds, then repeat the operation at step 2 - 8.
#	8. Start writing to the same files, making holes in those files in
#	   background.
#	9. Wait for our stress timeout value to finish, and kill any remaining
#          tests. The test is considered to have passed if the machine stays up
#	   during the time the stress tests are running and doesn't hit the stf
#	   time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mirror_stress_002
#
# DESCRIPTION:
#	running multiple copies of zfs_dataset_create_write_destroy on
#	separate mirrored pools shall not cause the system to fail, hang
#	or panic.
#
# STRATEGY:
#	the setup phase will have created several mirrored pools
#	multiple copies of zfs_dataset_create_write_destroy are fired off
#	  one per mirror in the background
#	wait a minute
#	Fire off a second round of the same tests
#	Wait for our stress timeout value to finish, and kill any remaining
#       tests.
#	The test is considered to have passed if the machine stays up during the
#       time the stress tests are running and doesn't hit the stf time limit.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: compress_002_pos
#
# DESCRIPTION:
# Create two files of exactly the same size. One with compression
# and one without. Ensure the compressed file is smaller.
#
# NOTE: This test uses a dataset rather than a simple file system.
#
# STRATEGY:
# Create a dataset, turn on compression and create files before
# and after the property change. The compressed file should be smaller.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: compress_003_pos
#
# DESCRIPTION:
# With 'compression' or 'compress'  set, changing filesystem blocksize cannot 
# cause system panic
#
# STRATEGY:
#	1. Set 'compression' or "compress" to on
#	2. Set different blocksize with ZFS filesystem
#	3. Use 'mkfile' create single block and multi-block files
#	4. Verify the system continued work
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: compress_001_pos
#
# DESCRIPTION:
# Create two files of exactly the same size. One with compression
# and one without. Ensure the compressed file is smaller.
#
# STRATEGY:
# Use "zfs set" to turn on compression and create files before
# and after the set call. The compressed file should be smaller.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: compress_004_pos
#
# DESCRIPTION:
# With 'compression' set, a file with non-power-of-2 blocksize storage space 
# can be freed as will normally.
#
# STRATEGY:
#	1. Set 'compression' or 'compress' to on or lzjb
#	2. Set different recordsize with ZFS filesystem
#	3. Repeatedly using 'randfree_file' to create a file and then free its  
#	   storage space with different range, the system should work normally.  
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: link_count_001
#
# DESCRIPTION:
# Verify file link count is zero on zfs
#
# STRATEGY:
# 1. Make sure this test executes on multi-processes system
# 2. Make zero size files and remove them in the background
# 3. Call the binary
# 4. Make sure the files can be removed successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userspace_002_pos
#
# DESCRIPTION:
#       Check the user used size and quota in zfs userspace
#
#
# STRATEGY:
#       1. set zfs userquota to a fs
#       2. write some data to the fs with specified user and size
#	3. use zfs userspace to check the used size and quota size
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: groupspace_001_pos
#
# DESCRIPTION:
#       Check the zfs groupspace with all parameters
#
#
# STRATEGY:
#       1. set zfs groupquota to a fs
#       2. write some data to the fs with specified user and group
#	3. use zfs groupspace with all possible parameters to check the result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_003_pos
#
# DESCRIPTION:
#       Check the basic function of set/get userquota and groupquota on fs
#
#
# STRATEGY:
#       1. Set userquota on fs and check the zfs get 
#       2. Set groupquota on fs and check the zfs get 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_007_pos
#
# DESCRIPTION:
#       
#      userquota/groupquota can be set beyond the fs quota
#      userquota/groupquota can be set at a smaller size than its current usage.
#
# STRATEGY:
#       1. set quota to a fs and set a larger size of userquota and groupquota
#       2. write some data to the fs and set a smaller userquota and groupquota  
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_012_neg
#
# DESCRIPTION:
#       userquota and groupquota can not be set against snapshot
#
#
# STRATEGY:
#       1. Set userquota on snap and check the zfs get 
#       2. Set groupquota on snap and check the zfs get 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_002_pos
#
# DESCRIPTION:
#       the userquota and groupquota can be set during zpool or zfs creation"
#
#
# STRATEGY:
#       1. Set userquota and groupquota via "zpool -O or zfs create -o"
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_006_pos
#
# DESCRIPTION:
#       Check the invalid parameter of zfs get user|group quota
#
#
# STRATEGY:
#       1. check the invalid zfs get user|group quota to fs 
#       2. check the valid zfs get user|group quota to snapshots
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_011_pos
#
# DESCRIPTION:
#       the userquota and groupquota will not change during zfs actions, such as
#	snapshot,clone,rename,upgrade,send,receive.
#
#
# STRATEGY:
#       1. Create a pool, and create fs with preset user,group quota
#       2. Check set user|group quota via zfs snapshot|clone|list -o
#       3. Check the user|group quota can not change during zfs rename|upgrade|promote
#       4. Check the user|group quota can not change during zfs clone
#       5. Check the user|group quota can not change during zfs send/receive
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userspace_001_pos
#
# DESCRIPTION:
#       Check the zfs userspace with all parameters
#
#
# STRATEGY:
#       1. set zfs userspace to a fs
#       2. write some data to the fs with specified user
#	3. use zfs userspace with all possible parameters to check the result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: groupspace_002_pos
#
# DESCRIPTION:
#       Check the user used and groupspace size in zfs groupspace
#
#
# STRATEGY:
#       1. set zfs groupquota to a fs
#       2. write some data to the fs with specified user and size
#	3. use zfs groupspace to check the used size and quota size
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_005_neg
#
# DESCRIPTION:
#       Check the invalid parameter of zfs set user|group quota
#
#
# STRATEGY:
#       1. check the invalid zfs set user|group quota to fs 
#       1. check the valid zfs set user|group quota to snapshots
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_004_pos
#
# DESCRIPTION:
#       Check the basic function user|group used
#
#
# STRATEGY:
#       1. Write some data to fs by normal user and check the user|group used
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_008_pos
#
# DESCRIPTION:
#       
#      zfs get all <fs> does not print out userquota/groupquota
#
# STRATEGY:
#       1. set userquota and groupquota to a fs
#       2. check zfs get all fs
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_009_pos
#
# DESCRIPTION:
#       Check user|group quota to snapshot that:
#	1) can not set user|group quota to snapshot directly
#	2) snapshot can inherit the parent fs's user|groupquota
#	3) the user|group quota will not change even the parent fs's quota changed.
#
#
# STRATEGY:
#       1. create a snapshot of a fs
#       2. set the user|group quota to snapshot and expect fail
#	3. set user|group quota to fs and check the snapshot
#	4. re-set user|group quota to fs and check the snapshot's value
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_001_pos
#
# DESCRIPTION:
#       Check the basic function of the userquota and groupquota
#
#
# STRATEGY:
#       1. Set userquota and overwrite the quota size
#       2. The write operation should fail with Disc quota exceeded
#       3. Set groupquota and overwrite the quota size
#       4. The write operation should fail with Disc quota exceeded
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: userquota_010_pos
#
# DESCRIPTION:
#       Check userquota and groupquota be overwrited at same time
#
#
# STRATEGY:
#       1. Set userquota and groupquota to a fs
#       2. write to exceed the userquota size to check the result
#       3. write to exceed the groupquota size to check the result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: redundancy_002_pos
#
# DESCRIPTION:
#	A raidz2 pool can withstand 2 devices are failing or missing.
#
# STRATEGY:
#	1. Create N(>3,<5) virtual disk files.
#	2. Create raidz2 pool based on the virtual disk files.
#	3. Fill the filesystem with directories and files.
#	4. Record all the files and directories checksum information.
#	5. Damaged at most two of the virtual disk files.
#	6. Verify the data is correct to prove raidz2 can withstand 2 devices 
#	   are failing.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: redundancy_003_pos
#
# DESCRIPTION:
#	A mirrored pool can withstand N-1 device are failing or missing.
#
# STRATEGY:
#	1. Create N(>2,<5) virtual disk files.
#	2. Create mirror pool based on the virtual disk files.
#	3. Fill the filesystem with directories and files.
#	4. Record all the files and directories checksum information.
#	5. Damaged at most N-1 of the virtual disk files.
#	6. Verify the data are correct to prove mirror can withstand N-1 devices
#	   are failing.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: redundancy_004_neg
#
# DESCRIPTION:
#	Striped pool have no data redundancy. Any device errors will
#	cause data corruption.
#
# STRATEGY:
#	1. Create N virtual disk file.
#	2. Create stripe pool based on the virtual disk files.
#	3. Fill the filesystem with directories and files.
#	4. Record all the files and directories checksum information.
#	5. Damage one of the virtual disk file.
#	6. Verify the data is error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: redundancy_001_pos
#
# DESCRIPTION:
#	A raidz pool can withstand at most 1 device failing or missing.
#
# STRATEGY:
#	1. Create N(>2,<5) virtual disk files.
#	2. Create raidz pool based on the virtual disk files.
#	3. Fill the filesystem with directories and files.
#	4. Record all the files and directories checksum information.
#	5. Damaged one of the virtual disk file.
#	6. Verify the data is correct to prove raidz can withstand 1 devicd is
#	   failing.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_tar_001_pos
#
# DESCRIPTION:
# Verify that '$TAR' command with -p option supports to archive ZFS ACLs
#
# STRATEGY:
# 1. Create file and directory in zfs filesystem
# 2. Add new ACE in ACL of file and directory
# 3. Use $TAR to archive file and directory
# 4. Extract the archive file
# 5. Verify that the restored ACLs of file and directory identify
#    with the origional ones. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_inherit_004_pos
#
# DESCRIPTION:
#	Verify aclinherit=passthrough-x will inherit the 'x' bits while mode request.
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Set aclinherit=passthrough-x
#	4. Verify only passthrough-x will inherit the 'x' bits while mode request.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_rwx_001_pos
#
# DESCRIPTION:
#	chmod A{+|-|=} have the correct behaviour to the ACL list. 	
#
# STRATEGY:
#	1. loop check root and non-root users
#	2. chmod file or dir with specified options
#	3. get ACE after behaviours of chmod
#	4. compare specified ACE and excpect ACE
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_rwx_004_pos
#
# DESCRIPTION:
#	Verify that explicit ACL setting to specified user or group will
#	override existed access rule.
#
# STRATEGY:
#	1. Loop root and non-root user.
#	2. Loop the specified access one by one.
#	3. Loop verify explicit ACL set to specified user and group.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_xattr_002_pos
#
# DESCRIPTION:
#	Verify that the write_xattr for remove the extended attributes of
#	owner/group/everyone are correct.
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Set special write_xattr ACE to the file and directory
# 3. Try to remove the extended attributes of the file and directory
# 4. Verify above operation is successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cp_001_pos
#
# DESCRIPTION:
# 	Verify that '/usr/bin/cp [-p]' supports ZFS ACL
#
# STRATEGY:
# 	1. Create file and  directory in zfs filesystem
# 	2. Set special ACE to the file and directory
# 	3. Copy the file/directory within and across zfs file system
# 	4. Verify that the ACL of file/directroy is not changed, when you are
# 	   inserting an ACL with a user: or group: entry on the top.
#	   (abstractions entry are treated special, since they represent the 
#	   traditional permission bit mapping.)
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_mv_001_pos
#
# DESCRIPTION:
# Verify that '/usr/bin/mv' supports ZFS ACL
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Set special ACE to the file and directory
# 3. Copy the file/directory within and across zfs file system
# 4. Verify that the ACL of file/directroy is not changed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_inherit_001_pos
#
# DESCRIPTION:
#	Verify chmod have correct behaviour to directory and file when setting
#	different inherit strategy to them.
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Separately chmod basedir with different inherite options.
#	4. Then create nested directories and files like the following.
#	
#                                                   _ odir4
#                                                  |_ ofile4
#                                         _ odir3 _|
#                                        |_ ofile3
#                               _ odir1 _|
#                              |_ ofile2
#                     basefile |
#          chmod -->  basedir -| 
#                              |_ nfile1
#                              |_ ndir1 _ 
#                                        |_ nfile2
#                                        |_ ndir2 _
#                                                  |_ nfile3
#                                                  |_ ndir3
#
#	5. Verify each directories and files have the correct access control
#	   capability.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cpio_002_pos
#
# DESCRIPTION:
# Verify that '$CPIO' command with -P@ option supports to archive ZFS ACLs
#
# STRATEGY:
# 1. Create file and directory in zfs filesystem
# 2. Add new ACE in ACL or change mode of file and directory
# 3. Create xattr of the file and directory
# 4. Use $CPIO to archive file and directory
# 5. Extract the archive file
# 6. Verify that the restored ACLs of file and directory identify
#    with the origional ones. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_002_pos
#
# DESCRIPTION:
# 	Verify acl after upgrading.
# STRATEGY:
#	1. Create a low version fs.
#	2. Create a directory and chmod it.
#	3. Upgrade the fs.
#	4. Create a file under the directory and list it.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-06-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_aclmode_001_pos
#
# DESCRIPTION:
#	Verify chmod have correct behaviour to directory and file when
#	filesystem has the different aclmode setting
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Separately chmod basedir with different aclmode options,
#	 	combine with the variable setting of aclmode:
#		"discard", "groupmask", or "passthrough".
#	4. Verify each directories and files have the correct access control
#	   capability.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-03-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_rwx_002_pos
#
# DESCRIPTION:
#	chmod A{+|-|=} read_data|write_data|execute for owner@ group@ or everyone@
#	correctly alters mode bits .
#
# STRATEGY:
#	1. Loop root and non-root user.
#	2. Get the random initial map.
#	3. Get the random ACL string.
#	4. Separately chmod +|-|= read_data|write_data|execute
#	5. Check map bits 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_delete_001_pos
#
# DESCRIPTION:
#	Verify that the combined delete_child/delete permission for 
#	owner/group/everyone are correct.
#
#        -------------------------------------------------------
#        |   Parent Dir  |           Target Object Permissions |
#        |  permissions  |                                     |
#        -------------------------------------------------------
#        |               | ACL Allows | ACL Denies| Delete     |
#        |               |  Delete    |  Delete   | unspecified|
#        -------------------------------------------------------
#        |  ACL Allows   | Permit     | Permit    | Permit     |
#        |  DELETE_CHILD |                                     |
#        -------------------------------------------------------
#        |  ACL Denies   | Permit     | Deny      | Deny       |
#        |  DELETE_CHILD |            |           |            |
#        -------------------------------------------------------
#        | ACL specifies |            |           |            |
#        | only allows   | Permit     | Permit    | Permit     |
#        | write and     |            |           |            |
#        | execute       |            |           |            |
#        -------------------------------------------------------
#        | ACL denies    |            |           |            |
#        | write and     | Permit     | Deny      | Deny       |
#        | execute       |            |           |            |
#        ------------------------------------------------------- 
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Set special ACE combination to the file and directory
# 3. Try to remove the file
# 4. Verify that combined permissions for owner/group/everyone are correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_compact_001_pos
#
# DESCRIPTION:
#	chmod A{+|-|=} could set compact ACL correctly.
#
# STRATEGY:
#	1. Loop root and non-root user.
#	2. Get the random compact ACL string.
#	4. Separately chmod +|-|=
#	5. Check compact ACL display as expected 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_001_neg
#
# DESCRIPTION:
# 	Verify  1) Illegal options to chmod should fail.
#		2) Delete all the ACE will lead to fail.
#		3) Add ACE exceed 1024 will cause to fail.
#
# STRATEGY:
#	1. Loop root and non-root users
#	2. Verify all kinds of illegal option will lead to chmod failed.
#	3. Verify 'chmod A0-' will fail when try to delete all the ACE.
#	4. Verify 'chmod A+' will succeed when the ACE number exceed 1024.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_tar_002_pos
#
# DESCRIPTION:
# Verify that '$TAR' command with -p@ option supports to archive ZFS ACLs 
#	& xattrs
#
# STRATEGY:
# 1. Create file and directory in zfs filesystem
# 2. Add new ACE in ACL of file and directory
# 3. Create xattr of the file and directory
# 4. Use $TAR cf@ to archive file and directory
# 5. Use $TAR xf@ to extract the archive file
# 6. Verify that the restored ACLs & xttrs of file and directory identify
#    with the origional ones. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_inherit_003_pos
#
# DESCRIPTION:
#	Verify chmod have correct behaviour to directory and file when
#	filesystem has the different aclinherit setting
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Separately chmod basedir with different inherite options,
#	 	combine with the variable setting of aclinherit:
#		"discard", "noallow", "secure" or "passthrough".
#	4. Then create nested directories and files like the following.
#	
#                     ofile    
#                     odir     	
#          chmod -->  basedir -| 
#                              |_ nfile1
#                              |_ ndir1 _ 
#                                        |_ nfile2
#                                        |_ ndir2 _
#                                                  |_ nfile3
#                                                  |_ ndir3
#
#	5. Verify each directories and files have the correct access control
#	   capability.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_rwacl_001_pos
#
# DESCRIPTION:
#	Verify assigned read_acl/write_acl to owner@/group@/everyone@,
#	specificied user and group. File have the correct access permission.
#
# STRATEGY:
#	1. Separatedly verify file and directory was assigned read_acl/write_acl
#	   by root and non-root user.
#	2. Verify owner always can read and write acl, even deny.
#	3. Verify group access permission, when group was assigned 
#	   read_acl/write_acl.
#	4. Verify access permission, after everyone was assigned read_acl/write.
#	5. Verify everyone@ was deny except specificied user, this user can read
#	   and write acl.
#	6. Verify the group was deny except specified user, this user can read
#	   and write acl
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_owner_001_pos
#
# DESCRIPTION:
#	Verify that the write_owner for 
#	owner/group/everyone are correct.
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Set special write_owner ACE to the file and directory
# 3. Try to chown/chgrp of the file and directory to take owner/group
# 4. Verify that the owner/group are correct. Follow these rules:
#  	(1) If uid is granted the write_owner permission, 
#		then it can only do chown to its own uid, 
#		or a group that they are a member of.
#	(2) Owner will ignore permission of (1) even write_owner not granted.
#	(3) Superuser will always permit whatever they do.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cpio_001_pos
#
# DESCRIPTION:
# Verify that '$CPIO' command with -P option supports to archive ZFS ACLs
#
# STRATEGY:
# 1. Create file and directory in zfs filesystem
# 2. Add new ACE in ACL or change mode of file and directory
# 3. Use $CPIO to archive file and directory
# 4. Extract the archive file
# 5. Verify that the restored ACLs of file and directory identify
#    with the origional ones. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_inherit_002_pos
#
# DESCRIPTION:
#	Verify chmod have correct behaviour to directory and file when
#	filesystem has the different aclinherit setting
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Separately chmod basedir with different inherite options,
#	 	combine with the variable setting of aclinherit:
#		"discard", "noallow", "secure" or "passthrough".
#	4. Then create nested directories and files like the following.
#	
#                     ofile    
#                     odir     	
#          chmod -->  basedir -| 
#                              |_ nfile1
#                              |_ ndir1 _ 
#                                        |_ nfile2
#                                        |_ ndir2 _
#                                                  |_ nfile3
#                                                  |_ ndir3
#
#	5. Verify each directories and files have the correct access control
#	   capability.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-03-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_ls_001_pos
#
# DESCRIPTION:
# Verify that '/usr/bin/ls' command option supports ZFS ACL 
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Verify that 'ls [-dv]' can list the ACEs of ACL of 
#    file/directroy
# 3. Change the file/directory's acl
# 4. Verify that 'ls -l' can use the '+' to indicate the non-trivial
#    acl. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_xattr_001_pos
#
# DESCRIPTION:
#	Verify that the read_xattr/write_xattr for 
#	owner/group/everyone are correct.
#
# STRATEGY:
# 1. Create file and  directory in zfs filesystem
# 2. Set special read_xattr ACE to the file and directory
# 3. Try to list the extended attributes of the file and directory
# 4. Set special write_xattr ACE to the file and directory
# 5. Try to add new extended attributes to the file and directory
# 6. Verify above operation is successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_rwx_003_pos
#
# DESCRIPTION:
#	Verify that the read_data/write_data/execute permission for 
#	owner/group/everyone are correct.
#
# STRATEGY:
#	1. Loop root and non-root user.
#	2. Separated verify type@:access:allow|deny to file and directory
#	3. To super user, read and write deny was override.
#	4. According to ACE list and override rule, expect that 
#	   read/write/execute file or directory succeed or fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-09)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cp_002_pos
#
# DESCRIPTION:
# 	Verify that '/usr/bin/cp [-p@]' supports ZFS ACL & xattrs
#
# STRATEGY:
# 	1. Create file and  directory in zfs filesystem
# 	2. Set special ACE to the file and directory
#	3. Create xattr of the file and directory
# 	4. Copy the file/directory within and across zfs file system
# 	5. Verify that the ACL & xattrs of the file/directroy is not changed, 
#	   when you are inserting an ACL with user: or group: entry on the top.
#	   (abstractions entry are treated special, since they represent the 
#	   traditional permission bit mapping.)
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_find_001_pos
#
# DESCRIPTION:
# Verify that '$FIND' command with '-ls' and '-acl' options supports ZFS ACL 
#
# STRATEGY:
# 1. Create 5 files and 5 directories in zfs filesystem
# 2. Select a file or directory and add a few ACEs to it 
# 3. Use $FIND -ls to check the "+" existen only with the selected file or 
#    directory
# 4. Use $FIND -acl to check only the selected file/directory in the list
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_find_002_neg
#
# DESCRIPTION:
#	Verifies ability to find files with attribute with -xattr flag and using
#	"-exec runat ls".
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Delete all the attribute files.
#	2. Verify all the specified files can not be found with '-xattr', 
#	3. Verify all the attribute files can not be found with '-exec runat ls'
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pack_001_pos
#
# DESCRIPTION:
#	Verifies that pack will keep file attribute intact afterthe file is
#	packed and unpacked.
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Save all files and their attribute files cksum value, then pack
#	   all the files.
#	3. Move them to another directory B.
#	4. Unpack them and calculate all the files and attribute files cksum
#	5. Verify all the cksum are identical
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_004_pos
#
# DESCRIPTION:
#	Verify files include attribute in pax archive and restore with pax
#	should succeed.
#
# STRATEGY:
#	1. Create several files which contains contribute files in directory A.
#	2. Enter into directory A and record all files cksum.
#	3. pax all the files to directory B.
#	4. Then pax the pax file to directory C.
#	5. Record all the files cksum in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_tar_001_pos
#
# DESCRIPTION:
#	Verifies that tar will include file attribute when @ flag is present.
#
# STRATEGY:
#	1. Use mktree create a set of directories in directory A.
#	2. Enter into directory A and record all directory information.
#	3. tar all the files to directory B.
#	4. Then tar the tar file to directory C.
#	5. Record all the directories informat in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_ls_002_neg
#
# DESCRIPTION:
#	Verifies that ls doesn't display @ in the file permissions using ls -@
#	for files without attribute.
#
# STRATEGY:
#	1. Create files with attribute files in directory A.
#	2. Removed all attribute files.
#	3. Verify 'ls -l' can't display @ in file permission.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_001_pos
#
# DESCRIPTION:
#	Verify directories include attribute in pax archive and restore with pax
#	should succeed.
#
# STRATEGY:
#	1. Use mktree create a set of directories in directory A.
#	2. Enter into directory A and record all directory information.
#	3. pax all the files to directory B.
#	4. Then pax the pax file to directory C.
#	5. Record all the directories informat in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_005_pos
#
# DESCRIPTION:
#	Verify files include attribute in cpio archive and restore with cpio
#	should succeed.
#
# STRATEGY:
#	1. Create several files which contains contribute files in directory A.
#	2. Enter into directory A and record all files cksum.
#	3. pax all the files to directory B.
#	4. Then pax the pax file to directory C.
#	5. Record all the files cksum in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cp_001_pos
#
# DESCRIPTION:
#	Verifies that cp will include file attribute when using the -@ flag
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Save all files and their attribute files cksum value, then 'cp -@p' 
#	   all the files to to another directory B.
#	3. Calculate all the cksum in directory B.
#	4. Verify all the cksum are identical
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_mv_001_pos
#
# DESCRIPTION:
#	Verifies that mv will include file attribute.
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Save all files and their attribute files cksum value
#	3. Move them to another directory B.
#	4. Calculate all the files and attribute files cksum
#	5. Verify all the cksum are identical
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_chmod_001_pos
#
# DESCRIPTION:
#	Verify chmod permission settings on files and directories, as both root
#	and non-root users.
#
# STRATEGY:
#	1. Loop root and $ZFS_ACL_STAFF1 as root and non-root users.
#	2. Create test file and directory in zfs filesystem.
#	3. Execute 'chmod' with specified options.
#	4. Check 'ls -l' output and compare with expect results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cp_002_neg
#
# DESCRIPTION:
#	Verifies that cp will not include file attribute when the -@ flag is not
#	present.
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Implement cp to files without '-@'
#	3. Verify attribute files will not include file attribute
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_003_pos
#
# DESCRIPTION:
#	Verify directories which include attribute in pax archive and restore
#	with cpio should succeed.
#
# STRATEGY:
#	1. Create several files in directory A.
#	2. Enter into directory A and record all directory cksum.
#	3. pax all the files to directory B.
#	4. Then cpio the pax file to directory C.
#	5. Record all the files cksum in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_compress_001_pos
#
# DESCRIPTION:
#	The function verifies that compress will keep file attribute intact
#	after the file is compressed and uncompressed.
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Save all files and their attribute files cksum value, then compress 
#	   all the files.
#	3. Move them to another directory B.
#	4. Uncompress them and calculate all the files and attribute files cksum
#	5. Verify all the cksum are identical
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_find_001_pos
#
# DESCRIPTION:
#	Verifies ability to find files with attribute with -xattr flag and using
#	"-exec runat ls".
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. Verify all the specified files can be found with '-xattr', 
#	3. Verify all the attribute files can be found with '-exec runat ls'
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_cp_003_neg
#
# DESCRIPTION:
#	Verifies that cp will not be able to include file attribute when
#	attribute is unreadable (unless the user is root)
#
# STRATEGY:
#	1. In directory A, create several files and add attribute files for them
#	2. chmod all files'the attribute files to '000'.
#	3. Implement 'cp -@p' to files.
#	4. Verify attribute files are not existing for non-root user.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_ls_001_pos
#
# DESCRIPTION:
#	Verifies that ls displays @ in the file permissions using ls -@ 
#	for files with attribute.
#
# STRATEGY:
#	1. Create files with attribute files in directory A.
#	2. Verify 'ls -l' can display @ in file permissions.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_002_pos
#
# DESCRIPTION:
#	Verify directories which include attribute in pax archive and restore 
#	with tar should succeed.
#
# STRATEGY:
#	1. Use mktree create a set of directories in directory A.
#	2. Enter into directory A and record all directory information.
#	3. pax all the files to directory B.
#	4. Then tar the pax file to directory C.
#	5. Record all the directories informat in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_pax_006_pos
#
# DESCRIPTION:
#	Verify files include attribute in tar archive and restore with tar
#	should succeed.
#
# STRATEGY:
#	1. Create several files which contains contribute files in directory A.
#	2. Enter into directory A and record all files cksum.
#	3. 'pax ustar' all the files to directory B.
#	4. Then 'pax ustar' the pax file to directory C.
#	5. Record all the files cksum in derectory C.
#	6. Verify the two records should be identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_acl_tar_002_neg
#
# DESCRIPTION:
#	Verifies that tar will not include files attribute when @ flag is not
#	present.
#
# STRATEGY:
#	1. Create several files with attribute files.
#	2. Enter into directory A and record all files cksum
#	3. tar all the files to directory B.
#	4. Then tar the tar file to directory C.
#	5. Record all the files cksum in derectory C.
#	6. Verify the two records should be not identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cifs_attr_001_pos
#
# DESCRIPTION:
#	Verify the user with write_attributes permission or
#	PRIV_FILE_OWNER privilege could set/clear DOS attributes.
#	(Readonly, Hidden, Archive, System)
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Grant user has write_attributes permission or
#		PRIV_FILE_OWNER privilege
#	4. Verify set/clear DOS attributes should succeed.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cifs_attr_002_pos
#
# DESCRIPTION:
#	Verify the user with PRIV_FILE_FLAG_SET/PRIV_FILE_FLAG_CLEAR
#	could set/clear BSD'ish attributes.
#	(Immutable, nounlink, and appendonly)
#	
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Grant user has PRIV_FILE_FLAG_SET/PRIV_FILE_FLAG_CLEAR separately.
#	4. Verify set/clear BSD'ish attributes should succeed.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cifs_attr_003_pos
#
# DESCRIPTION:
#	Verify the DOS attributes (Readonly, Hidden, Archive, System) 
#	and BSD'ish attributes (Immutable, nounlink, and appendonly) 
#	will provide the proper access limitation as expected.
#
#	Readonly means that the content of a file can't be modified, but
#	timestamps, mode and so on can.
#
#	Archive - Indicates if a file should be included in the next backup
#	of the file system.  ZFS will set this bit whenever a file is
#	modified.
#
#	Hidden and System (ZFS does nothing special with these, other than
#	letting a user/application set them.
#
#	Immutable (The data can't, change nor can mode, ACL, size and so on)
#	The only attribute that can be updated is the access time.
#
#	Nonunlink - Sort of like immutable except that a file/dir can't be
#	removed.
#	This will also effect a rename operation, since that involes a
#	remove.
#
#	Appendonly - File can only be appended to.
#
#	nodump, settable, opaque (These are for the MacOS port) we will
#	allow them to be set, but have no semantics tied to them.
#
# STRATEGY:
#	1. Loop super user and non-super user to run the test case.
#	2. Create basedir and a set of subdirectores and files within it.
#	3. Set the file/dir with each kind of special attribute.
#	4. Verify the access limitation works as expected.
#	
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: threadsappend_001_pos
#
# DESCRIPTION:
#
# Ensure multiple threads performing write appends to the same ZFS
# file succeed.
#
# STRATEGY:
#	1) Verify this is a multi-processor system
#	2) Create multiple threads with each appending to a file
#       3) Verify that the resulting file is the expected size
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refreserv_003_pos
#
# DESCRIPTION:
#	Verify a snapshot will only be allowed if there is enough free pool 
#	space outside of this refreservation.
#
# STRATEGY:
#	1. Setting quota and refservation
#	2. Verify snapshot can be created, when used =< quota - refreserv
#	3. Verify failed to create snapshot, when used > quota - refreserv
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refreserv_002_pos
#
# DESCRIPTION:
#	Setting full size as refreservation, verify no snapshot can be created.
#
# STRATEGY:
#	1. Setting full size as refreservation on pool
#	2. Verify no snapshot can be created on this pool
#	3. Setting full size as refreservation on filesystem
#	4. Verify no snapshot can be created on it and its subfs
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refreserv_004_pos
#
# DESCRIPTION:
#	Verify refreservation is limited by available space.
#
# STRATEGY:
#	1. Setting quota and refreservation on parent filesystem.
#	2. Get available space on sub-filesystem.
#	3. Verify refreservation is limited by available on it.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refreserv_005_pos
#
# DESCRIPTION:
#	Volume refreservation is limited by volsize
#
# STRATEGY:
#	1. Create volume on filesystem
#	2. Setting quota for parenet filesytem
#	3. Verify volume refreservation is only limited by volsize
#	4. Verify volume refreservation can be changed when volsize changed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refreserv_001_pos
#
# DESCRIPTION:
#	Reservations are enforced using the maximum of 'reserv' and 'refreserv'
#
# STRATEGY:
#	1. Setting quota for parent filesystem.
#	2. Setting reservation and refreservation for sub-filesystem.
#	3. Verify the sub-fs reservation are enforced by the maximum of 'reserv'
#	   and 'refreserv'.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_008_pos
#
# DESCRIPTION:
# Newfs will interfere with devices and spare devices that are in use 
# by exported pool.
#
# STRATEGY:
# 1. Create a regular|mirror|raidz|raidz2 pool with the given disk
# 2. Export the pool
# 3. Try to newfs against the disk, verify it succeeds as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_004_pos
#
# DESCRIPTION:
# format will disallow modification of a mounted zfs disk partition or a spare
# device
#
# STRATEGY:
# 1. Create a ZFS filesystem
# 2. Add a spare device to the ZFS pool
# 3. Attempt to format the disk and the spare device.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_005_pos
#
# DESCRIPTION:
# newfs will not interfere with devices and spare devices that are in use 
# by active pool.
#
# STRATEGY:
# 1. Create a regular|mirror|raidz|raidz2 pool with the given disk
# 2. Try to newfs against the disk, verify it fails as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-12-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_001_pos
#
# DESCRIPTION:
# ZFS will not interfere with devices that are in use by dumpadm.
#
# STRATEGY:
# 1. Create crash dump device using 'dumpadm'
# 2. Try to create a ZFS pool using the 'dumpadm' crash dump device.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_009_pos
#
# DESCRIPTION:
# format command will interfere with devices and spare devices that are in use 
# by exported pool.
#
# STRATEGY:
# 1. Create a regular|mirror|raidz|raidz2 pool with the given disk
# 2. Export the pool
# 3. Try to format against the disk, verify it succeeds as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_007_pos
#
# DESCRIPTION:
# dumpadm will interfere with devices and spare devices that are in use 
# by exported pool.
#
# STRATEGY:
# 1. Create a regular|mirror|raidz|raidz2 pool with the given disk
# 2. Export the pool
# 3. Try to dumpadm against the disk, verify it succeeds as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-12-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_003_pos
#
# DESCRIPTION:
# ZFS will not interfere with devices that are in use by ufsdump or
# ufsrestore.
#
# STRATEGY:
# 1. newfs a disk
# 2. mount the disk
# 3. create files and dirs on disk
# 4. umount the disk
# 5. ufsdump this disk to a backup disk
# 6. Try to create a ZFS pool with same disk (also as a spare device)
# 7. ufsrestore the disk from backup
# 8. try to create a zpool during the ufsrestore
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_006_pos
#
# DESCRIPTION:
# dumpadm will not interfere with devices and spare devices that are in use 
# by active pool.
#
# STRATEGY:
# 1. Create a regular|mirror|raidz|raidz2 pool with the given disk
# 2. Try to dumpadm against the disk, verify it fails as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-12-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inuse_002_pos
#
# DESCRIPTION:
# ZFS will not interfere with devices that are in use by SVM
#
# STRATEGY:
# 1. Create SVM device d99 with a disk
# 2. Try to create a ZFS pool with same disk
# 3. Try a use the same disk as a spare device
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: grow_replicas_001_pos
#
# DESCRIPTION:
# A ZFS file system is limited by the amount of disk space
# available to the pool. Growing the pool by adding a disk
# increases the amount of space.
#
# STRATEGY:
# 1) Fill a ZFS filesystem mirror/raidz until ENOSPC by creating lots
# of files
# 2) Grow the mirror/raidz by adding a disk
# 3) Verify that more data can now be written to the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_013_pos
#
# DESCRIPTION:
# The noxattr mount option functions as expected
#
# STRATEGY:
#	1. Create a file on a filesystem and add an xattr to it
#	2. Unmount the filesystem, and mount it -o noxattr
#	3. Verify that the xattr cannot be read and new files
#	   cannot have xattrs set on them.
#	4. Unmount and mount the filesystem normally
#	5. Verify that xattrs can be set and accessed again
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_006_pos
#
# DESCRIPTION:
# Xattrs present on a file in a snapshot should be visible.
#
# STRATEGY:
#	1. Create a file and give it an xattr
#       2. Take a snapshot of the filesystem
#	3. Verify that we can take a snapshot of it.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_007_neg
#
# DESCRIPTION:
# Creating and writing xattrs on files in snapshot directories fails. Also,
# we shouldn't be able to list the xattrs of files in snapshots who didn't have
# xattrs when the snapshot was created (the xattr namespace wouldn't have been
# created yet, and snapshots are read-only) See fsattr(5) for more details.
#
# STRATEGY:
#	1. Create a file and add an xattr to it.
#	2. Create another file, but don't add an xattr to it.
#       3. Snapshot the filesystem
#	4. Verify we're unable to alter the xattr on the first file
#	5. Verify we're unable to list the xattrs on the second file
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_003_neg
#
# DESCRIPTION:
#
# Attempting to read an xattr on a file for which we have no permissions
# should fail.
#
# STRATEGY:
#	1. Create a file, and set an with an xattr
#       2. Set the octal file permissions to 000 on the file.
#	3. Check that we're unable to read the xattr as a non-root user
#	4. Check that we're unable to write an xattr as a non-root user
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_002_neg
#
# DESCRIPTION:
#
# Trying to read a non-existent xattr should fail.
#
# STRATEGY:
#	1. Create a file
#       2. Try to read a non-existent xattr, check that an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_012_pos
#
# DESCRIPTION:
# xattr file sizes count towards normal disk usage
# 
# STRATEGY:
#	1. Create a file, and check pool and filesystem usage
#       2. Create a 200mb xattr in that file
#	3. Check pool and filesystem usage, to ensure it reflects the size
#	   of the xattr
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_001_pos
#
# DESCRIPTION:
#
# Creating, reading and writing xattrs on ZFS filesystems works as expected
#
# STRATEGY:
#	1. Create an xattr on a ZFS-based file using runat
#	2. Read an empty xattr directory
#       3. Write the xattr using runat and cat
#	3. Read the xattr using runat
#	4. Delete the xattr
#	5. List the xattr namespace successfully, checking for deletion
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_005_pos
#
# DESCRIPTION:
# read/write/create/delete xattr on a clone filesystem
# 
#
# STRATEGY:
#	1. Create an xattr on a filesystem
#	2. Snapshot the filesystem and clone it
#       3. Verify the xattr can still be read, written, deleted
#	4. Verify we can create new xattrs on new files created on the clone
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_010_neg
#
# DESCRIPTION:
# Verify that mkdir and various mknods fail inside the xattr namespace
#
# STRATEGY:
#	1. Create a file and add an xattr to it (to ensure the namespace exists)
#       2. Verify that mkdir fails inside the xattr namespace
#	3. Verify that various mknods fails inside the xattr namespace
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_011_pos
#
# DESCRIPTION:
#
# Basic applications work with xattrs: cpio cp find mv pax tar
# 
# STRATEGY:
#	1. For each application
#       2. Create an xattr and archive/move/copy/find files with xattr support
#	3. Also check that when appropriate flag is not used, the xattr
#	   doesn't get copied
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_008_pos
#
# DESCRIPTION:
# We verify that the special . and .. dirs work as expected for xattrs.
#
# STRATEGY:
#	1. Create a file and an xattr on that file
#	2. List the . directory, verifying the output
#	3. Verify we're unable to list the ../ directory
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_004_pos
#
# DESCRIPTION:
#
# Creating files on ufs and tmpfs, and copying those files to ZFS with
# appropriate cp flags, the xattrs will still be readable.
#
# STRATEGY:
#	1. Create files in ufs and tmpfs with xattrs
#       2. Copy those files to zfs
#	3. Ensure the xattrs can be read and written
#	4. Do the same in reverse.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  xattr_009_neg
#
# DESCRIPTION:
# links between xattr and normal file namespace fail
# 
# STRATEGY:
#	1. Create a file and add an xattr to it (to ensure the namespace exists)
#       2. Verify we're unable to create a symbolic link
#	3. Verify we're unable to create a hard link
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_007_pos
#
# DESCRIPTION:
#	Rename parent filesystem name will not change the dependent order.
#
# STRATEGY:
#	1. Separately rename pool clone, filesystem and volume name.
#	2. Send -R all the POOL
#	3. Verify renamed dataset will not change the snapshot dependent order.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_003_pos
#
# DESCRIPTION:
#	zfs send -I dataset@init to clone@snap can create a clone
#
# STRATEGY:
#	1. Setup test model
#	2. send -I pool@init to clone@snap
#	3. Verify the clone and snapshot can be recovered via receive
#	4. Verify the similar operating in filesystem and volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_012_pos
#
# DESCRIPTION:
#	zfs send -R will backup all the filesystem properties correctly.
#
# STRATEGY:
#	1. Setting properties for all the filesystem and volumes randomly
#	2. Backup all the data from POOL by send -R
#	3. Restore all the data in POOL2
#	4. Verify all the perperties in two pools are same
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_013_pos
#
# DESCRIPTION:
#	zfs receive -dF with incremental stream will destroy all the 
#	dataset that not exist on the sender side.
#
# STRATEGY:
#	1. Setup test model
#	2. Send -R @final on pool
#	3. Destroy some dataset within the @final, and create @destroy
#	4. Send -R -I @final @destroy on pool
#	5. Verify receive -dF will destroy all the dataset that not exist
#	   on the sender side.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_006_pos
#
# DESCRIPTION:
#	Rename snapshot name will not change the dependent order.
#
# STRATEGY:
#	1. Set up a set of datasets.
#	2. Rename part of snapshots.
#	3. Send -R all the POOL
#	4. Verify snapshot name will not change the dependent order.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_002_pos
#
# DESCRIPTION:
#	zfs send -I sends all incrementals from fs@init to fs@final.
#
# STRATEGY:
#	1. Create several snapshots in pool2 
#	2. Send -I @snapA @final
#	3. Destroy all the snapshot except @snapA
#	4. Make sure all the snapshots and content are recovered
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_011_pos
#
# DESCRIPTION:
#	Changes made by 'zfs inherit' can be properly received.
#
# STRATEGY:
#	1. Inherit property for filesystem and volume
#	2. Send and restore them in the target pool
#	3. Verify all the datasets can be properly backup and receive
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-10-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_004_pos
#
# DESCRIPTION:
#	zfs send -R -i send incremental from fs@init to fs@final.
#
# STRATEGY:
#	1. Create a set of snapshots and fill with data.
#	2. Create sub filesystems.
#	3. Create final snapshot
#	4. Verify zfs send -R -i will backup all the datasets which has 
#	   snapshot suffix @final
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_008_pos
#
# DESCRIPTION:
#	Changes made by 'zfs promote' can be properly received.
#
# STRATEGY:
#	1. Seperatly promote pool clone, filesystem clone and volume clone.
#	2. Recursively backup all the POOL and restore in POOL2
#	3. Verify all the datesets and property be properly received.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_009_pos
#
# DESCRIPTION:
#	zfs receive can handle out of space correctly.
#
# STRATEGY:
#	1. Create two pools, one is big and another is small.
#	2. Fill the big pool with data.
#	3. Take snapshot and backup the whole pool.
#	4. Receive this stream in small pool.
#	5. Verify zfs receive can handle the out of space error correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-10-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_005_pos
#
# DESCRIPTION:
#	zfs send -R -I send all the incremental between fs@init with fs@final
#
# STRATEGY:
#	1. Setup test model
#	2. Send -R -I @init @final on pool
#	3. Destroy all the snapshots which is later than @init
#	4. Verify receive can restore all the snapshots and data
#	5. Do the same test on filesystem and volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_001_pos
#
# DESCRIPTION:
#	zfs send -R send replication stream up to the named snap.
#
# STRATEGY:
#	1. Back up all the data from POOL/FS
#	2. Verify all the datasets and data can be recovered in POOL2
#	3. Back up all the data from root filesystem POOL2
#	4. Verify all the data can be recovered, too
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rsend_010_pos
#
# DESCRIPTION:
#	ZFS can handle stream with multiple identical (same GUID) snapshots
#
# STRATEGY:
#	1. Recursively backup snapshot
#	2. Restore it to the given filesystem
#	3. Resend the snapshot again
#	4. Verify this stream can be restore to this filesystem again
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_ENOSPC_001_pos
#
# DESCRIPTION:
# A zvol volume will return ENOSPC when the underlying pool runs out of
# space.
#
# STRATEGY:
# 1. Create a pool
# 2. Create a zvol volume
# 3. Create a ufs file system ontop of the zvol
# 4. Mount the ufs file system
# 5. Fill volume until ENOSPC is returned
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_004_pos
#
# DESCRIPTION:
#	The minimum volume size for swap should be a multiple of 2 pagesize
#	bytes.
#
# STRATEGY:
#	1. Get test system page size.
#	2. Create different size volumes.
#	3. Verify 'swap -a' has correct behaviour.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_001_pos
#
# DESCRIPTION:
# Verify that a zvol can be used as a swap device
#
# STRATEGY:
# 1. Create a pool
# 2. Create a zvol volume
# 3. Use zvol as swap space
# 4. Create a file under /tmp
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_005_pos
#
# DESCRIPTION:
#	swaplow + swaplen must be less than or equal to the volume size.
#
# STRATEGY:
#	1. Get test system page size and test volume size.
#	2. Random get swaplow and swaplen.
#	3. Verify swap -a should succeed when swaplow + swaplen <= volume size.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_003_pos
#
# DESCRIPTION:
# Verify that a zvol device can be used as a swap device
# through /etc/vfstab configuration.
#
# STRATEGY:
# 1. Create a pool
# 2. Create a zvol volume
# 3. Save current swaps info and delete current swaps
# 4. Modify /etc/vfstab to add entry for zvol as swap device
# 5. Use /sbin/swapadd to zvol as swap device throuth /etc/vfstab
# 6. Create a file under /tmp
# 7. Verify the file
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_002_pos
#
# DESCRIPTION:
# Using a zvol as swap space, fill with files until ENOSPC returned.
#
# STRATEGY:
# 1. Create a pool
# 2. Create a zvol volume
# 3. Add zvol to swap space
# 4. Fill swap space until ENOSPC is returned
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_swap_006_pos
#
# DESCRIPTION:
#	 A volume can be add as several segments, but overlapping are not
#	 allowed.
#
# STRATEGY:
#	1. Figure out three groups swaplow and swaplen.
#	2. Verify different volume segments can be added correctly.
#	3. Verify overlapping swap volume are not allowed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_cli_002_pos
#
# DESCRIPTION:
# Creating a volume with a 50 letter name should work.
#
# STRATEGY:
# 1. Using a very long name, create a zvol
# 2. Verify volume exists
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_cli_003_neg
#
# DESCRIPTION:
# Try each ZFS volume sub-command without parameters to make sure
# it returns an error.
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute the sub-command
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_cli_001_pos
#
# DESCRIPTION:
# Executing well-formed 'zfs list' commands should return success
#
# STRATEGY:
# 1. Create an array of valid options.
# 2. Execute each element in the array.
# 3. Verify success is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_005_neg
#
# DESCRIPTION:
# 	Verify a device cannot be dump and swap at the same time.
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Set it as swap device.
# 3. Verify dumpadm with this zvol will fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-03-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_001_neg
#
# DESCRIPTION:
# Verify that using ZFS volume as a dump device fails until 
# dumpswap supported.
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Use dumpadm add the volume as dump device
# 3. Verify the return code as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-03-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_004_pos
#
# DESCRIPTION:
# Verify permit to create snapshot over active dumpswap zvol.
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Set the volume as dump or swap
# 3. Verify create snapshot over the zvol succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_003_neg
#
# DESCRIPTION:
# Verify create storage pool or newfs over volume as dump device is denied.
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Use dumpadm set the volume as dump device
# 3. Verify create pool & newfs over the volume return an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_002_pos
#
# DESCRIPTION:
# Verify that ZFS volume snapshot could be fscked
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Copy some files and create snapshot
# 3. Verify fsck on the snapshot is OK
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zvol_misc_006_pos
#
# DESCRIPTION:
# ZFS volume as dump device, it should always have 128k volblocksize
#
# STRATEGY:
# 1. Create a ZFS volume
# 2. Use dumpadm set the volume as dump device
# 3. Verify the volume's volblocksize=128k
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zones_002_pos
#
# DESCRIPTION:
#
# A zone created where the zonepath parent dir is the top level of a ZFS
# file system has a new ZFS filesystem created for it.
#
# STRATEGY:
#	1. The setup script should have created the zone.
#       2. Verify that a new ZFS filesystem has been created.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zones_003_pos
#
# DESCRIPTION:
#
# Zone cloning via ZFS snapshots works as expected.
# We can clone zones where the zonepath is the top level of a ZFS filesystem
# using snapshots. Where the zone is not at the top level of a ZFS filesystem,
# cloning the zone uses the normal method of copying the files when
# performing the clone operation.
#
# STRATEGY:
#	1. The setup script should have created the zone.
#       2. Clone a zone-on-ZFS
#	3. Verify that ZFS snapshots were taken and used for the clone and that
#	   the new zone is indeed a clone (in the ZFS sense)
#	4. Clone a normal zone & verify that no snapshots were taken.
#	5. Clone a zone-on-ZFS, but specify the "copy" method & verify that no
#	   snapshots were taken.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zones_001_pos
#
# DESCRIPTION:
#
# The zone created by the default zones setup should have ZFS zvols,
# datasets and filesystems present.
#
# STRATEGY:
#	1. For each ZFS object type
#       2. Perform a basic sanity check for that object in the local zone.
#	3. Check that the top level dataset is read only.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zones_005_pos
#
# DESCRIPTION:
#
# Pool properties can be read but can't be set within a zone
#
# STRATEGY:
# 1. Verify we can read pool properties in a zone
# 2. Verify we can't set a pool property in a zone
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-03)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zones_004_pos
#
# DESCRIPTION:
#
# Deleting a zone, where the zonepath parent dir is the top level of a ZFS
# file system, causes that underlying filesystem to be deleted. Deleting
# the non-ZFS zone does not delete any filesystems.
#
# STRATEGY:
#	1. The setup script should have created the zone.
#       2. Delete our ZFS rooted zone, verify the filesystem has been deleted.
#	3. Delete our non-ZFS rooted zone, the zonepath dir should still exist.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_003_neg
#
# DESCRIPTION:
#	Verify invalid value of shareiscsi can not be set
#
# STRATEGY:
#	1) verify a set of invalid value of shareiscsi can not be set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_002_neg
#
# DESCRIPTION:
#	Verify file systems and snapshots can not be shared via iSCSI
#
# STRATEGY:
#	1) Turn on shareiscsi property directly on the filesystem
#	2) Check if the target is created or not
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_006_neg
#
# DESCRIPTION:
#	Verify iscsioptions can not be changed by zfs command
#
# STRATEGY:
#	1) Save iscsioptions first, then change it on purpose
#	2) Check if the value is really changed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_005_pos
#
# DESCRIPTION:
#	Verify export/import pool with iSCSI
#
# STRATEGY:
#	1) Create a volume, turn on shareiscsi directly on the volume
#	2) Export the pool, check the target is gone after the operation
#	3) Import the pool, check the target is back and its scsi name
#	   not changed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_001_pos
#
# DESCRIPTION:
#	Verify setting shareiscsi property on volume will make it an iSCSI
#	target	
#
# STRATEGY:
#	1) Create a volume, turn on shareiscsi directly on the volume
#	2) Check if the target is created or not
#	3) Destroy the volume, then turn on shareiscsi property on parent
#	   filesystem at first
#	4) Then create the volume, check if the target is created or not
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: iscsi_004_pos
#
# DESCRIPTION:
#	Verify renaming a volume does not change target's iSCSI name
#
# STRATEGY:
#	1) Create a volume, turn on shareiscsi directly on the volume
#	2) Save the target's iSCSI name
#	3) Rename the volume, compare the target's iSCSI name with the original
#	   one
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rebooting_001_pos
#
# DESCRIPTION:
#	Do some I/O work in zfs filesystem in a remote machine, reboot it and  
#	and verify the system boots up fine.
#
# STRATEGY:
#	1. Create lots of empty directories in remote zfs filesystem and unlink
#	   these directories.
#	2. Reboot the system
#	3. Verify the system boots up correctly
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cross_endian_001_pos
#
# DESCRIPTION:
#	storage pool can be exported and imported between two any architecture
#	machines.
#
# STRATEGY:
#	1. Create a bunch of block files and then create different types of 
#	  storage pool with these files, populate some data to the storage pool
#	2. Exported the storage pool and rcp the block files to a remote host  
#	3. Imported the pool in the remote host and verify the data integrity
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cross_endian_002_pos
#
# DESCRIPTION:
#	ZFS filesystem data can be backuped to remote host with any architecture.
#
# STRATEGY:
#	1. Create a zfs filesystem and populate some data in the filesystem
#	2. Backup the data and restore it in a remote host  
#	3. verify the data integrity
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: sharing_001_pos
#
# DESCRIPTION:
#	Verify .zfs support with NFS version 3 & 4, but not support with NFS 
#	version 2.
#
# STRATEGY:
#	1. Create three zfs filesystems in remote host, populate them with 
#	   snapshot, and share them   
#	2. Mount the shared filesystems from remote host to local host with 
#	   with different NFS version: 2, 3 and  4
#	3. Verify the data in the file system and the snapshot directory
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-04-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: large_files_001_pos
#
# DESCRIPTION:
# Write a file to the allowable ZFS fs size.
#
# STRATEGY:
# 1. largest_file will write to a file and increase its size
# to the maximum allowable.
# 2. The last byte of the file should be accessbile without error.
# 3. Writing beyond the maximum file size generates an 'errno' of
# EFBIG.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  privilege_001_pos
#
# DESCRIPTION:
#
# The RBAC profile "ZFS Storage Management" works
#
# STRATEGY:
#	(create)
#	1. As a normal user, try to create a pool - which should fail.
#       2. Assign "ZFS Storage Management" profile, try to create pool again,
#	   which should succeed.
#
#	(works well with other ZFS profile tests)
#	3. Attempt to create a ZFS filesystem, which should fail.
#	4. Add the "ZFS File System Management" profile, attempt to create a FS
# 	   which should succeed.
#
#	(destroy)
#       5. Remove the FS profile, then attempt to destroy the pool, which
# 	   should succeed.
#	6. Remove the Storage profile, then attempt to recreate the pool, which
#	   should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  privilege_002_pos
#
# DESCRIPTION:
#
# The RBAC profile "ZFS File System Management" works
#
# STRATEGY:
#
#	The following actions are taken, both using profile execution (pfexec)
#	and without profile execution - we make sure that the latter should
#	always fail.
#
#	(create)
#	1. As a normal user, try to create a filesystem - which should fail.
#       2. Assign "ZFS File System Management" profile, try to create fs again,
#	   which should succeed.
#
#	(pools)
#	3. Ensure a user with this profile can't perform pool administration
#	   by attempting to destroy a pool.
#
#	(destroy)
#       5. Remove the FS profile, then attempt to destroy the fs, which
# 	   should fail.
#	6. Assign the FS profile, then attempt to destroy the fs, which
#	   should succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_006_pos
#
# DESCRIPTION:
#
# Pools of correct vdev types accept boot property
#
# STRATEGY:
# 1. create pools of each vdev type (raid, raidz, raidz2, mirror + hotspares)
# 2. verify we can set bootfs on each pool type according to design
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_007_neg
#
# DESCRIPTION:
#
# Setting bootfs on a pool which was configured with the whole disk
# (i.e. EFI) will fail
#
# STRATEGY:
# 1. create a pool with a whole disk
# 2. create a filesystem on this pool
# 3. verify we can not set bootfs on the filesystem we just created.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_003_pos
#
# DESCRIPTION:
#
# Valid pool names are accepted
#
# STRATEGY:
# 1. Using a list of valid pool names
# 2. Create a filesystem in that pool
# 2. Verify we can set the bootfs to that filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_002_neg
#
# DESCRIPTION:
#
# Invalid datasets are rejected as boot property values
#
# STRATEGY:
#
# 1. Create a snapshot and a zvol
# 2. Verify that we can't set the bootfs to those datasets
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_001_pos
#
# DESCRIPTION:
#
# Valid datasets are accepted as bootfs property values
#
# STRATEGY:
# 1. Create a set of datasets in a test pool
# 2. Try setting them as boot filesystems
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_004_neg
#
# DESCRIPTION:
#
# Invalid pool names are rejected by zpool set bootfs
#
# STRATEGY:
#	1. Try to set bootfs on some non-existent pools
#
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_008_neg
#
# DESCRIPTION:
#
# setting bootfs on a dataset which has gzip compression enabled will fail
#
# STRATEGY:
# 1. create pools based on a valid vdev
# 2. create a filesytem on this pool and set the compression property to gzip1-9
# 3. set the pool's bootfs property to filesystem we just configured which should
#    fail
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_009_neg
#
# DESCRIPTION:
#
# Valid encrypted datasets can't be set bootfs property values
#
# STRATEGY:
# 1. Create encrypted datasets in a test pool
# 2. Try setting encrypted datasets as boot filesystems
# 3. Verify failures.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  bootfs_005_neg
#
# DESCRIPTION:
#
# Boot properties cannot be set on pools with older versions
#
# STRATEGY:
# 1. Copy and import some pools of older versions
# 2. Create a filesystem on each
# 3. Verify that zpool set bootfs fails on each
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_004_pos
#
# DESCRIPTION:
#	When device replacement fails, the original device's state will be
#	'UNAVAIL' and an FMA fault will be generated.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Synchronise with device in the background.
#	3. Create a small device which is smaller than the minimal ZFS device.
#	4. Replace one device with the small device.
#	5. Verify the device replacement failed
#	6. Verify that an FMA fault was generated
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_008_pos
#
# DESCRIPTION:
#	After hot spare device is revoved, the devices state will be 'REMOVED'. 
#	No FMA faults was generated.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool with hot spare device.
#	2. Synchronise with device in the background.
#	3. Remove the hotspare device.
#	4. Verify the device's status is 'REMOVED'.
#	5. Verify no FMA fault was generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_011_pos
#
# DESCRIPTION:
#	Removing device offlined, verify device status is UNAVAIL, when the 
#	system is onlined.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool w/a hot spare device.
#	2. Synchronise with device in the background.
#	3. Set or unset autoreplace
#	4. Unmount all filesystems and disable syseventd and fmd.
#	5. Unload ZFS module and remove devices.
#	6. Load ZFS module and verify device the device's status is 'UNAVAIL'.
#	7. Verify no FMA fault was generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_010_pos
#
# DESCRIPTION:
#	Removing device offlined and reinsert onlined, verify device status is 
#	'ONLINE'.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool w/a hot spare device.
#	2. Synchronise with device in the background.
#	3. Set or unset autoreplace
#	4. Unmount all filesystems and disable syseventd and fmd.
#	5. Unload ZFS module and remove a device.
#	6. Load ZFS module and insert the device again.
#	7. Verify device's status is 'ONLINE'. No FMA fault is generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_009_pos
#
# DESCRIPTION:
#	We unload ZFS module to simulate system is powered off. Replacing device
#	and verify the device's status is 'ONLINE' when autoreplace is 'on', the
#	status is 'OFFLINE' when autoreplace is 'off'.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool with or without hot spare device.
#	2. Synchronise with device in the background.
#	3. Set autoreplace = off
#	4. Unmount all filesystems and disable syseventd and fmd.
#	5. Unload ZFS module and replace one of devices.
#	6. Load ZFS module and verify device status is 'UNAVAIL'.
#	7. Verify an FMA fault was generated.
#	8. Set autoreplace = on, redo steps 4 - 5. 
#	9. Verify the device's status is 'ONLINE'. No FMA fault was generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_001_pos
#
# DESCRIPTION:
#	When removing a device from a redundant pool, the device's state will
#	be indicated as 'REMOVED'. No FMA faulty message.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Synchronise with device in the background.
#	3. Remove one of device of pool.
#	4. Detect removed devices status is 'REMOVED'.
#	5. Detect no FMA faulty message.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_005_pos
#
# DESCRIPTION:
#	Regarding of autoreplace, when removing offline device and reinserting
#	again. This device's status is 'ONLINE' . No FMA fault was generated.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Synchronise with device in the background.
#	3. Offline one of device, remove it and reinsert again.
#	4. Verify device status is 'ONLINE'.
#	5. Verify no FMA faultwas generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_003_pos
#
# DESCRIPTION:
#	Set/Unset autoreplace, remove device from redundant pool and insert new
#	device, this new device state will be indicated as 'ONLINE/UNAVAIL'. 
#	No FMA faulty message.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Synchronise with device in the background.
#	3. Set autoreplace=on
#	4. Remove device from pool and insert a new device.
#	5. Verify the new devices status is 'ONLINE'.
#	6. Verify that no FMA faults have been generated.
#	7. Set autoreplace=off, redo steps 4 - 6, verify the new device's
#	   status is 'UNAVAIL'. There are FMA faulty.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_007_pos
#
# DESCRIPTION:
#	When autoreplace is 'on', replacing the device with a smaller one.
#	Verify the device's status is 'UNAVAIL'. FMA fault has been generated.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Set autoreplace = on
#	3. Synchronise with device in the background.
#	4. Offline and remove one of device, insert a new device.
#	5. Verify the device's status is 'UNAVAIL'.
#	6. Verify FMA fault has been generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_002_pos
#
# DESCRIPTION:
#	When removing a device from a redundant pool, then reinserting  it 
#	again, this device's state will be indicated as 'ONLINE' regardless of 
#	autoreplace was set or unset. No FMA faulty message.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Synchronise with device in the background.
#	3. Set autoreplace is on or off
#	4. Remove device from pool and reinsert again.
#	5. Detect removed devices status is 'ONLINE'.
#	6. Detect no FMA faulty message.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotplug_006_pos
#
# DESCRIPTION:
#	When unsetting/setting autoreplace, then replacing offlined device,
#	verify device's status is 'UNAVAIL/ONLINE'. No FMA fault is generated.
#
# STRATEGY:
#	1. Create mirror/raidz/raidz2 pool.
#	2. Unsetting/Setting autoreplace
#	3. Synchronise with device in the background.
#	4. Offline one of device, remove it and insert a new device.
#	5. Verify device status's is 'UNAVAIL/ONLINE' with/without FMA fault.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_003_pos
#
# DESCRIPTION:
#	Verify option '-l' only allow permission to the dataset itself.
#
# STRATEGY:
#	1. Create descendent datasets of $ROOT_TESTFS
#	2. Select user, group and everyone and set local permission separately.
#	3. Set locally permissions to $ROOT_TESTFS or $ROOT_TESTVOL.
#	4. Verify the permissions are only allow on $ROOT_TESTFS or
#	   $ROOT_TESTVOL.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_007_pos
#
# DESCRIPTION:
#	Verify the permissions set will be masked on its descendent
#	datasets by same name set.
#
# STRATEGY:
#	1. Create $ROOT_TESTFS/childfs
#	2. Set permission $perms1 to @set on $ROOT_TESTFS
#	3. Reset permission $perms2 to @set on $ROOT_TESTFS/childfs
#	4. Allow @set to $STAFF1 on $ROOT_TESTFS/childfs
#	5. Verify $perms2 is delegated on $ROOT_TESTFS/childfs and its
#	   descendent.
#	6. Allow @set to $STAFF1 on $ROOT_TESTFS
#	7. Verify $perms1 is not appended to $STAFF1 on $ROOT_TESTFS/childfs and
#	   its descendent since it is masked
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_012_neg
#
# DESCRIPTION:
#	Scan all permissions one by one to verify privileged user 
#	can not use permissions properly when delegation property is set off
#
# STRATEGY:
#	1. Delegate all the permission one by one to user on dataset.
#	2. Verify privileged user can not use permissions properly when
#	delegation property is off
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_002_pos
#
# DESCRIPTION:
# <user|group> argument is interpreted as a user if possible, then as a group as
# possible.
#
# STRATEGY:
#	1. Create user $STAFF_GROUP
#	2. Delegate permissions to $STAFF_GROUP
#	3. Verify user $STAFF_GROUP has the permissions.
#	4. Delete user $STAFF_GROUP and allow the permission to $STAFF_GROUP
#	5. Verify $STAFF_GROUP is interpreted as group.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_006_pos
#
# DESCRIPTION:
#	Changing permissions in a set will change what is allowed wherever the
#	set is used.
#
# STRATEGY:
#	1. Set create as set @basic.
#	2. Allow set @basic to $STAFF1 on $ROOT_TESTFS or $ROOT_TESTVOL
#	3. Verify $STAFF1 has create permissions.
#	4. Reset snapshot,allow to $basic
#	5. Verify now $STAFF1 have create,allow,destroy permissions.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_009_neg
#
# DESCRIPTION:
#	zfs allow can deal with invalid arguments.(Invalid options or combination)
#
# STRATEGY:
#	1. Verify invalid argumets will cause error.
#	2. Verify non-optional argument was missing will cause error.
#	3. Verify invalid options cause error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_008_pos
#
# DESCRIPTION:
#	non-root user can allow any permissions which he is holding to
#	other else user when it get 'allow' permission.
#
# STRATEGY:
#	1. Set two set permissions to two datasets locally.
#	2. Verify the non-root user can allow permission if he has allow
#	   permission.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_004_pos
#
# DESCRIPTION:
#	Verify option '-d' allow permission to the descendent datasets, and not
#	for this dataset itself.
#
# STRATEGY:
#	1. Create descendent datasets of $ROOT_TESTFS
#	2. Select user, group and everyone and set descendent permission 
#	   separately.
#	3. Set descendent permissions to $ROOT_TESTFS or $ROOT_TESTVOL.
#	4. Verify those permissions are allowed to $ROOT_TESTFS's 
#	   descendent dataset.
#	5. Verify the permissions are not allowed to $ROOT_TESTFS or
#	   $ROOT_TESTVOL.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_001_pos
#
# DESCRIPTION:
# 	"everyone" is interpreted as the keyword "everyone" whatever the same
# 	name user or group is existing.
#
# STRATEGY:
#	1. Create user 'everyone'.
#	2. Verify 'everyone' is interpreted as keywords.
#	3. Create group 'everyone'.
#	4. Verify 'everyone' is interpreted as keywords.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_005_pos
#
# DESCRIPTION:
#	Verify option '-c' will be granted locally to the creator on any
#	newly-created descendent file systems.
#
# STRATEGY:
#	1. Allow create permissions to everyone on $ROOT_TESTFS locally.
#	2. Allow '-c' create to $ROOT_TESTFS.
#	3. chmod 777 the mountpoint of $ROOT_TESTFS
#	4. Verify only creator can create descendent dataset on 
#	   $ROOT_TESTFS/$user.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_010_pos
#
# DESCRIPTION:
#	Scan the following permissions one by one to verify privileged user 
#	has correct permission delegation in datasets.
#
# STRATEGY:
#	1. Delegate all the permission one by one to user on dataset.
#	2. Verify privileged user has correct permission without any other
#	   permissions allowed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-11-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_011_neg
#
# DESCRIPTION:
#	Verify zpool subcmds and system readonly properties can't be delegated.
#
# STRATEGY:
#	1. Loop all the zpool subcmds and readonly properties, except permission
#	   'create' & 'destroy'.
#	2. Verify those subcmd or properties can't be delegated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_003_pos
#
# DESCRIPTION:
#	Verify options '-r' or '-l' + '-d' will unallow permission to this 
#	dataset and the descendent datasets.
#
# STRATEGY:
#	1. Set up unallow test model.
#	2. Implement unallow -l -d to $ROOT_TESTFS or $ROOT_TESTVOL without
#	   options.
#	3. Verify '-l' + '-d' will unallow local + descendent permission.
#	4. Verify '-r' will unallow local + descendent permission.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_007_neg
#
# DESCRIPTION:
#	zfs unallow will not remove those permissions which inherited from
#	its parent filesystem.
#
# STRATEGY:
#	1. Assign perm1 to $ROOT_TESTFS
#	2. Create $SUBFS and assign perm2 to it.
#	3. Verify unallow can not affect perm1 on $SUBFS
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_006_pos
#
# DESCRIPTION:
#	Verify option '-u', '-g' and '-e' only removed the specified type
#	permissions set.
#
# STRATEGY:
#	1. Allow '-u' '-g' & '-e' to $STAFF1 on ROOT_TESTFS or $ROOT_TESTVOL.
#	2. Unallow '-u' '-g' & '-e' on $ROOT_TESTFS or $ROOT_TESTVOL separately.
#	3. Verify permissions on $ROOT_TESTFS or $ROOT_TESTVOL separately.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_002_pos
#
# DESCRIPTION:
#	Verify '-d' only remove the permissions on descendent filesystem.

# STRATEGY:
#	1. Set up unallow test model.
#	2. Implement unallow -d to $ROOT_TESTFS
#	3. Verify '-d' only remove the permissions on descendent filesystem.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_004_pos
#
# DESCRIPTION:
#	Verify '-s' will remove permissions from the named set.
#
# STRATEGY:
#	1. Set @basic set to $ROOT_TESTFS or $ROOT_TESTVOL and allow @basic
#	   to $STAFF1
#	2. Verify $STAFF1 have @basic permissions.
#	3. Verify '-s' will remove permission from the named set.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_008_neg
#
# DESCRIPTION:
#	zfs unallow can handle invalid arguments.
#
# STRATEGY:
#	1. Set up basic test environment.
#	2. Verify zfs unallow handle invalid arguments correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_005_pos
#
# DESCRIPTION:
#	Verify option '-c' will remove the created permission set.
#
# STRATEGY:
#	1. Set created time set to $ROOT_TESTFS.
#	2. Allow permission create to $STAFF1 on $ROOT_TESTFS.
#	3. Create $SUBFS and verify $STAFF1 have created time permissions.
#	4. Verify $STAFF1 has created time permission.
#	5. Unallow created time permission with option '-c'.
#	6. Created $SUBFS and verify $STAFF1 have not created time permissions.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_001_pos
#
# DESCRIPTION:
#	Verify '-l' only removed the local permissions.
#
# STRATEGY:
#	1. Set up unallow test model.
#	2. Implement unallow -l to $ROOT_TESTFS or $TESTVOL
#	3. Verify '-l' only remove the local permissions.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_list_002_neg
#
# DESCRIPTION:
# Executing 'zpool list' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool list' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_list_001_pos
#
# DESCRIPTION:
# Verify that 'zpool list' succeeds as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify the command succeeds.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_002_pos
#
# DESCRIPTION:
# The sort functionality in 'zfs list' works as expected.
#
# STRATEGY:
# 1. Using several zfs datasets with names, creation dates, checksum options
# 2. Sort the datasets by name, checksum options, creation date.
# 3. Verify that the datasets are sorted correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_006_pos
#
# DESCRIPTION:
#	Verify 'zfs list' exclude list of snapshot.
#
# STRATEGY:
#	1. Verify snapshot not shown in the list:
#		zfs list [-r]
#	2. Verify snapshot will be shown by following case:
#		zfs list [-r] -t snapshot
#		zfs list [-r] -t all
#		zfs list <snapshot>
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_003_pos
#
# DESCRIPTION:
# 	Verify 'zfs list -r' could recursively display any children 
#	of the dataset.
#
# STRATEGY:
# 1. Prepare a set of datasets by hierarchy.
# 2. Execute 'zfs list -r' at the top of these datasets.
# 3. Verify all child datasets are all be shown.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-05-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_007_pos
#
# DESCRIPTION:
#	'zfs list -d <n>' should get expected output.
#
# STRATEGY:
#	1. 'zfs list -d <n>' to get the output.
#	2. 'zfs list -r|egrep' to get the expected output.
#	3. Compare the two outputs, they shoud be same.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_001_pos
#
# DESCRIPTION:
# Executing well-formed 'zfs list' commands should return success.
#
# STRATEGY:
# 1. Create an array of valid options.
# 2. Execute each element in the array.
# 3. Verify success is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_005_pos
#
# DESCRIPTION:
#	Verify 'zfs list' evaluate multiple '-s' options from left to right
#	in decreasing order of importance.
#
# STRATEGY:
#	1. Setting user property f:color for filesystem and volume.
#	2. Setting user property f:amount for filesystem and volume.
#	3. Setting reservation for filesystem and volume
#	3. Verify 'zfs list' evaluated multiple -s options from left to right.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-23)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_008_neg
#
# DESCRIPTION:
# A negative depth or a non numeric depth should fail in 'zfs list -d <n>'
#
# STRATEGY:
# 1. Run zfs list -d with negative depth or non numeric depth
# 2. Verify that zfs list returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_list_004_neg
#
# DESCRIPTION:
# 	Verify 'zfs list [-r]' should fail while 
#		* the given dataset does not exist
#		* the given path does not exist.
#		* the given path does not belong to zfs.
#
# STRATEGY:
# 1. Create an array of invalid options.
# 2. Execute each element in the array.
# 3. Verify failure is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-05-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_iostat_001_neg
#
# DESCRIPTION:
# Verify that 'zpool iostat' can be executed as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify that a success is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_iostat_003_neg
#
# DESCRIPTION:
# Executing 'zpool iostat' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool iostat' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_iostat_002_pos
#
# DESCRIPTION:
# Verify that 'zpool iostat [interval [count]' can be executed as non-root.
#
# STRATEGY:
# 1. set the interval=5 and  count=6
# 2. sleep 30 seconds
# 3. Verify that the output have 6 record.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_001_neg
#
# DESCRIPTION:
#
# zfs get works when run as a user
#
# STRATEGY:
# 1. Run zfs get with an array of different arguments
# 2. Verify for each property, we get the value that's expected
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_001_neg
#
# DESCRIPTION:
#
# zfs promote returns an error when run as a user
#
# STRATEGY:
#
# 1. Verify we don't have permissions to promote a clone
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_001_neg
#
# DESCRIPTION:
#
# zpool shows a usage message when run as a user
#
# STRATEGY:
# 1. Run the zpool command
# 2. Verify that a usage message is produced
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_001_neg
#
# DESCRIPTION:
#
# zpool import returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to import an exported pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_replace_001_neg
#
# DESCRIPTION:
#
# zpool replace returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to replace a device in a pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_export_001_neg
#
# DESCRIPTION:
#
# zpool export returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to export a pool
# 2. Verify the command fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_001_neg
#
# DESCRIPTION:
#
# zfs receive returns an error when run as a user
#
# STRATEGY:
#
# 1. Attempt to receive a datastream as a user
# 2. Verify that the dataset wasn't created
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_history_001_neg
#
# DESCRIPTION:
#
# zpool history works when run as a user
#
# STRATEGY:
# 1. Attempt to get history on a test pool
# 2. Verify the command fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_001_neg
#
# DESCRIPTION:
#
# zfs share returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to share a dataset
# 2. Verify the dataset was not shared.
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_remove_001_neg
#
# DESCRIPTION:
#
# zpool remove returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to remove a device from a pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_set_001_neg
#
# DESCRIPTION:
#
# zfs set returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to set an array of properties on a dataset
# 2. Verify that those properties were not set and retain their original values.
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_001_neg
#
# DESCRIPTION:
# Verify that 'zpool create' fails as a non-root user.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify that an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_001_neg
#
# DESCRIPTION:
#
# zfs unshare returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to unshare a shared dataset
# 2. Verify the dataset is still shared
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rollback_001_neg
#
# DESCRIPTION:
#
# zfs rollback returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to rollback a snapshot
# 2. Verify that a file which doesn't exist in the snapshot still exists
#    (showing the snapshot rollback failed)
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_online_001_neg
#
# DESCRIPTION:
#
# zpool online returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to online a device in a pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_001_neg
#
# DESCRIPTION:
#
# zpool upgrade returns an error when run as a user
#
# STRATEGY:
#
# 1. Attempt to upgrade a pool
# 2. Verify the command fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_cli_006_neg
#
# DESCRIPTION:
# Verify that 'zfs destroy' fails as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_001_neg
#
# DESCRIPTION:
#
# zfs rename returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to rename a dataset
# 2. Verify that the renamed dataset does not exist.
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_get_001_neg
#
# DESCRIPTION:
#
# zpool get works when run as a user
#
# STRATEGY:
#
# 1. For each property, get that property
# 2. Verify the property was the same as that set in setup
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_001_neg
#
# DESCRIPTION:
#
# zfs mount returns an error when run as a user
#
# STRATEGY:
#
# 1. Verify that we can't mount the unmounted filesystem created in setup
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_attach_001_neg
#
# DESCRIPTION:
#
# zpool attach returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to attach a disk to a pool
# 2.Verify that the attach failed
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_allow_001_neg
#
# DESCRIPTION:
#
# zfs allow returns an error when run as a user
#
# STRATEGY:
#
# 1. Verify that trying to show allows works as a user
# 2. Verify that trying to set allows fails as a user
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_detach_001_neg
#
# DESCRIPTION:
#
# zpool detach returns an error when run as a user
#
# STRATEGY:
#
# 1. Attempt to detach a device from a pool
# 2. Verify the command fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_001_neg
#
# DESCRIPTION:
#
# zpool scrub returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to start a scrub on a pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_offline_001_neg
#
# DESCRIPTION:
#
# zpool offline returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to offline a device in a pool
# 2. Verify that the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_001_neg
#
# DESCRIPTION:
#
# zfs clone returns an error when run as a user
#
# STRATEGY:
#
# 1. Verify that we're unable to clone snapshots as a user
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_002_neg
#
# DESCRIPTION:
# Executing 'zpool import' as regular user should denied.
#
# STRATEGY:
# 1. Create an array of options try to detect exported/destroyed pools.
# 2. Execute 'zpool import' with each element of the array by regular user.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_001_neg
#
# DESCRIPTION:
# Verify that 'zpool add' fails as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify that an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_destroy_001_neg
#
# DESCRIPTION:
# Verify that 'zpool destroy' fails as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_001_neg
#
# DESCRIPTION:
#
# zfs shows a usage message when run as a user
#
# STRATEGY:
# 1. Run zfs as a user
# 2. Verify it produces a usage message
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_001_neg
#
# DESCRIPTION:
#
# zfs snapshot returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to snapshot a dataset
# 2. Verify the snapshot wasn't taken
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_status_001_neg
#
# DESCRIPTION:
#
# zpool status works when run as a user
#
# STRATEGY:
#
# 1. Run zpool status as a user
# 2. Verify we get output
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_001_neg
#
# DESCRIPTION:
# Executing various badly formed 'zfs create' should fail.
#
# STRATEGY:
# 1. Create an array of badly formed sub-commands.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_001_neg
#
# DESCRIPTION:
#
# zfs upgrade returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to upgrade a version1 dataset
# 2. Verify the dataset wasn't upgraded
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_set_001_neg
#
# DESCRIPTION:
#
# zpool set returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to set some properties on a pool
# 2. Verify the command fails
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_send_001_neg
#
# DESCRIPTION:
#
# zfs send returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to send a dataset to a file
# 2. Verify the file created has zero-size
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zdb_001_neg
#
# DESCRIPTION:
#
# zdb can't run as a user on datasets, but can run without arguments
#
# STRATEGY:
# 1. Run zdb as a user, it should print information
# 2. Run zdb as a user on different datasets, it should fail
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_clear_001_neg
#
# DESCRIPTION:
#
# zpool clear returns an error when run as a user
#
# STRATEGY:
#
# 1. Attempt to clear errors on a zpool
# 2. Verify that the command fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unallow_001_neg
#
# DESCRIPTION:
#
# zfs unallow returns an error when run as a user
#
# STRATEGY:
# 1. Attempt to unallow a set of permissions
# 2. Verify the unallow wasn't performed
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_001_neg
#
# DESCRIPTION:
# Verify that 'zfs umount' and its variants fail as non-root.
#
# STRATEGY:
# 1. Create an array of options.
# 2. Execute each element of the array.
# 3. Verify that the commands fail with an error code.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_inherit_001_neg
#
# DESCRIPTION:
#
# zfs inherit returns an error when run as a user
#
# STRATEGY:
#
# 1. Verify that we can't inherit a property when running as a user
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: truncate_002_pos
#
# DESCRIPTION:
# Tests file truncation within ZFS while a sync operation is in progress.
#
# STRATEGY:
# 1. Copy a file to ZFS filesystem
# 2. Copy /dev/null to same file on ZFS filesystem
# 3. Execute a sync command
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: truncate_001_pos
#
# DESCRIPTION:
# 	Tests file truncation within ZFS.
#
# STRATEGY:
# 	1. Open file
# 	2. Write random blocks in random places
# 	3. Truncate the file
# 	4. Repeat steps 2 and 3 lots of times
# 	5. Close the file.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: nestedfs_001_pos
#
# DESCRIPTION:
# Given a pool create a nested file system and a ZFS file system
# in the nested file system. Populate the file system.
#
# As a sub-assertion, the test verifies that a nested file system with
# a mounted file system cannot be destroyed.
#
# STRATEGY:
# 1. Create a file in the new mountpoint
# 2. Unmount the new mountpoint
# 3. Show a nested file system with file systems cannot be destroyed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_018_pos
#
# DESCRIPTION:
#
# Verify that reservation doesn't inherit its value from parent.
#
# STRATEGY:
# 1) Create a filesystem tree 
# 2) Set reservation for parents
# 3) Verify that the 'reservation' for descendent doesnot inherit the value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_014_pos
#
# DESCRIPTION:
#
# A reservation cannot exceed the quota on a dataset
#
# STRATEGY:
# 1) Create a filesystem and volume
# 2) Set a quota on the filesystem
# 3) Attempt to set a reservation larger than the quota. Verify
# that the attempt fails.
# 4) Repeat 2-3 for volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_010_pos
#
# DESCRIPTION:
#
# In pool with a full filesystem and a filesystem with a reservation
# destroying another filesystem should allow more data to be written to
# the full filesystem
#
#
# STRATEGY:
# 1) Create a filesystem as dataset
# 2) Create a filesystem at the same level
# 3) Set a reservation on the dataset filesystem
# 4) Fill up the second filesystem
# 5) Destroy the dataset filesystem
# 6) Verify can write more data to the full filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_001_pos
#
# DESCRIPTION:
#
# ZFS allows reservations to be set on filesystems and volumes, provided
# the reservation is less than the space available in the pool. 
#
# STRATEGY:
# 1) Create a regular and sparse volume 
#   (filesystem already created by default_setup)
# 2) Get the space available in the pool
# 3) Set a reservation on the filesystem less than the space available.
# 4) Verify that the 'reservation' property for the filesystem has 
#    the correct value.
# 5) Reset the reservation to 'none'
# 6) Repeat steps 2-5 for both volume types
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_005_pos
#
# DESCRIPTION:
#
# When a reservation property of a filesystem, regular volume
# or sparse volume is set to 'none' the space previously consumed by the 
# reservation should be released back to the pool
#
# STRATEGY:
# 1) Create a filesystem, regular volume and sparse volume
# 2) Get the space used and available in the pool
# 3) Set a reservation on the filesystem less than the space available.
# 4) Verify that the 'reservation' property for the filesystem has
# the correct value.
# 5) Reset the reservation value back to zero (or 'none')
# 6) Verify that the space used and available totals for the pool have
# changed by the expected amounts (within tolerances).
# 7) Repeat steps 3-6 for a regular volume, sparse volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_009_pos
#
# DESCRIPTION:
#
# In pool with a full filesystem and another filesystem with a reservation
# setting the reservation on the second filesystem to 'none' should allow more
# data to be written to the first filesystem.
#
#
# STRATEGY:
# 1) Create a filesystem as a dataset
# 2) Create a filesystem at the same level
# 3) Set a reservation on the dataset filesystem
# 4) Fill up the filesystem
# 5) Set the reservation on the dataset filesystem to 'none'
# 6) Verify we can write more data to the first filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_008_pos
#
# DESCRIPTION:
#
# Setting a reservation reserves a defined minimum amount of space for
# a dataset, and prevents other datasets using that space. Verify that
# reducing the reservation on a filesystem allows other datasets in
# the pool to use that space.
#
# STRATEGY:
# 1) Create multiple filesystems
# 2) Set reservations on all bar one of the filesystems
# 3) Fill up the one non-reserved filesystem
# 4) Reduce one of the reservations and verify can write more
# data into the non-reserved filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_004_pos
#
# DESCRIPTION:
#
# When a dataset which has a reservation set on it is destroyed,
# the space consumed or reserved by that dataset should be released
# back into the pool.
#
# STRATEGY:
# 1) Create a filesystem, regular and sparse volume
# 2) Get the space used and available in the pool
# 3) Set a reservation on the filesystem less than the space available.
# 4) Verify that the 'reservation' property for the filesystem has
# the correct value.
# 5) Destroy the filesystem without resetting the reservation value.
# 6) Verify that the space used and available totals for the pool have
# changed by the expected amounts (within tolerances).
# 7) Repeat steps 3-6 for a regular volume and sparse volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_015_pos
#
# DESCRIPTION:
#
# In pool with a full filesystem and a regular volume with an implicit 
# reservation, setting the reservation on the volume to 'none' should allow 
# more data to be written to the filesystem.
#
#
# STRATEGY:
# 1) Create a regular non-sparse volume (which implicitly sets the reservation 
#    property to a value equal to the volume size)
# 2) Create a filesystem at the same level
# 3) Fill up the filesystem
# 4) Set the reservation on the volume to 'none'
# 5) Verify can write more data to the filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_011_pos
#
# DESCRIPTION:
#
# ZFS has two mechanisms dealing with space for datasets, namely
# reservations and quotas. Setting one should not affect the other,
# provided the values are legal (i.e. enough space in pool etc).
#
# STRATEGY:
# 1) Create one filesystem
# 2) Get the current quota setting 
# 3) Set a reservation 
# 4) Verify that the quota value remains unchanged
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_002_pos
#
# DESCRIPTION:
#
# Reservation values cannot exceed the amount of space available
# in the pool. Verify that attempting to set a reservation greater
# than this value fails.
#
# STRATEGY:
# 1) Create a filesystem, regular and sparse volume
# 2) Get the space available in the pool
# 3) Attempt to set a reservation greater than the available space
# on the filesystem and verify it fails.
# 4) Verify that the reservation is still set to 'none' (or 0) on
# the filesystem.
# 5) Repeat 3-4 for regular and sparse volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_006_pos
#
# DESCRIPTION:
#
# Reservations (if successfully set) guarantee a minimum amount of space
# for a dataset. Unlike quotas however there should be no restrictions
# on accessing space outside of the limits of the reservation (if the
# space is available in the pool). Verify that in a filesystem with a
# reservation set that its possible to create files both within the
# reserved space and also outside.
#
# STRATEGY:
# 1) Create a filesystem
# 2) Get the space used and available in the pool
# 3) Set a reservation on the filesystem
# 4) Verify can write a file that is bigger than the reserved space
#
# i.e. we start writing within the reserved region and then continue
# for 20MB outside it.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_017_pos
#
# DESCRIPTION:
#
# For a sparse volume changes to the volsize are not reflected in the reservation
#
# STRATEGY:
# 1) Create a regular and sparse volume 
# 2) Get the space available in the pool
# 3) Set reservation with various size on the regular and sparse volume
# 4) Verify that the 'reservation' property for the regular volume has
#    the correct value.
# 5) Verify that the 'reservation' property for the sparse volume is set to 'none'
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_013_pos
#
# DESCRIPTION:
#
# Reservation properties on data objects should be preserved when the
# pool within which they are contained is exported and then re-imported.
#
#
# STRATEGY:
# 1) Create a filesystem as dataset
# 2) Create another filesystem at the same level
# 3) Create a regular volume at the same level
# 4) Create a sparse volume at the same level
# 5) Create a filesystem within the dataset filesystem
# 6) Set reservations on all filesystems
# 7) Export the pool
# 8) Re-import the pool
# 9) Verify that the reservation settings are correct
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_016_pos
#
# DESCRIPTION:
#
# In pool with a full filesystem and a regular volume (with implicit
# reservation) destroying the volume should allow more data to be written 
# to the filesystem
#
#
# STRATEGY:
# 1) Create a regular (non-sparse) volume
# 2) Create a filesystem at the same level
# 3) Fill up the filesystem
# 4) Destroy the volume
# 5) Verify can write more data to the filesystem
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_012_pos
#
# DESCRIPTION:
#
# A reservation guarantees a certain amount of space for a dataset.
# Nothing else which happens in the same pool should affect that
# space, i.e. even if the rest of the pool fills up the reserved
# space should still be accessible.
#
# STRATEGY:
# 1) Create 2 filesystems
# 2) Set a reservation on one filesystem
# 3) Fill up the other filesystem (which does not have a reservation
# set) until all space is consumed
# 4) Verify can still write to the filesystem which has a reservation
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_003_pos
#
# DESCRIPTION:
#
# Verify that it's possible to set a reservation on a filesystem,
# or volume multiple times, without resetting the reservation
# to none.
#
# STRATEGY:
# 1) Create a regular volume and a sparse volume
# 2) Get the space available in the pool
# 3) Set a reservation on the filesystem less than the space available.
# 4) Verify that the 'reservation' property for the filesystem has
# the correct value.
# 5) Repeat 2-4 for different reservation values
# 6) Repeat 3-5 for regular and sparse volume
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_007_pos
#
# DESCRIPTION:
#
# Setting a reservation on dataset should have no effect on any other
# dataset at the same level in the hierarchy beyond using up available
# space in the pool.
#
# STRATEGY:
# 1) Create a filesystem
# 2) Set a reservation on the filesystem
# 3) Create another filesystem at the same level
# 4) Set a reservation on the second filesystem
# 5) Destroy both the filesystems
# 6) Verify space accounted for correctly
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  devices_003_pos
#
# DESCRIPTION:
#
# Writing random data into /dev/zfs should do no harm.
#
# STRATEGY:
# 1. Write some random data into /dev/zfs
# 2. Verify that this should fail.
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: devices_002_neg
# 
# DESCRIPTION:
# When set property devices=off on file system, device files cannot be used
# in this file system. 
#
# STRATEGY:
# 1. Create pool and file system.
# 2. Set devices=off on this file system.
# 3. Separately create block device file and character file.
# 4. Separately read from those two device files.
# 5. Check the return value, and make sure it failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: devices_001_pos
# 
# DESCRIPTION:
# When set property devices=on on file system, devices files can be used in
# this file system. 
#
# STRATEGY:
# 1. Create pool and file system.
# 2. Set devices=on on this file system.
# 3. Separately create block device file and character file.
# 4. Separately read from those two device files.
# 5. Check the return value, and make sure it succeeds.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: exec_002_neg
# 
# DESCRIPTION:
# When set property exec=off on a filesystem, processes can not be executed from
# this filesystem.
#
# STRATEGY:
# 1. Create pool and file system.
# 2. Copy '/usr/bin/ls' to the ZFS file system.
# 3. Setting exec=off on this file system.
# 4. Make sure '/usr/bin/ls' can not work in this ZFS file system.
# 5. Make sure mmap which is using the PROT_EXEC calls failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: exec_001_pos
# 
# DESCRIPTION:
# When set property exec=on on a filesystem, processes can be executed from
# this filesystem.
#
# STRATEGY:
# 1. Create pool and file system.
# 2. Copy '/usr/bin/ls' to the ZFS file system.
# 3. Setting exec=on on this file system.
# 4. Make sure '/usr/bin/ls' can work in this ZFS file system.
# 5. Make sure mmap which is using the PROT_EXEC calls succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mmap_write_001_pos
#
# DESCRIPTION:
# Writing to a file and mmaping that file at the
# same time does not result in a deadlock.
#
# STRATEGY:
# 1. Make sure this test executes on multi-processes system.
# 2. Call mmapwrite binary.
# 3. wait 120s and make sure the test file existed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: read_mmap_001_pos
#
# DESCRIPTION:
# read()s from mmap()'ed file contain correct data.
#
# STRATEGY:
# 1. Create a pool & dataset
# 2. Call readmmap binary
# 3. unmount this file system
# 4. Verify the integrity of this pool & dateset
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_002_neg
#
# DESCRIPTION:
#
# the zfs rootpool can not be destroyed
#
# STRATEGY:
# 1) check if the current system is installed as zfs root 
# 2) get the rootpool
# 3) try to destroy the rootpool, which should fail.
# 4) try to destroy the rootpool filesystem, which should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_007_neg
#
# DESCRIPTION:
#
# the zfs rootfilesystem's compression property can not set to gzip[1-9]
#
# STRATEGY:
# 1) check if the current system is installed as zfs root 
# 2) get the rootfs
# 3) set the rootfs's compression to gzip 1-9 which should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_003_neg
#
# DESCRIPTION:
#
#  system related filesystems can not be renamed or destroyed
#
# STRATEGY:
#
# 1) check if the current system is installed as zfs rootfs 
# 2) get the rootfs
# 3) try to rename the rootfs to some newfs, which should fail.
# 4) try to destroy the rootfs, which should fail.
# 5) try to destroy the rootfs with -f which should fail
# 6) try to destroy the rootfs with -fR which should fail
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_006_pos
#
# DESCRIPTION:
#
# the zfs rootfs's mountpoint must be mounted and must be /
#
# STRATEGY:
# 1) check if the current system is installed as zfs root 
# 2) get the rootfs
# 3) check the rootfs's mount ponit, it must be mounted and must be /
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_004_pos
#
# DESCRIPTION:
#
#  rootfs's canmount property must be noauto
#
# STRATEGY:
#
# 1) check if the current system is installed as zfs rootfs or not. 
# 2) get the rootfs
# 3) get the canmount value of rootfs
# 4) check to see if the upper value equal to noauto or not.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_001_pos
#
# DESCRIPTION:
# 
# rootpool's bootfs property must be equal to <rootfs>
#
# STRATEGY:
# 1) check if the system is zfsroot or not.
# 2) get the rootpool and rootfs if it's zfs root
# 3) check the rootpool's bootfs value
# 4) chek if the boofs equal to rootfs
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-01-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  rootpool_005_pos
#
# DESCRIPTION:
#
#  rootpool/ROOT's mountpoint property should be legacy
#
# STRATEGY:
#
# 1) check if the current system is installed as zfs rootfs or not. 
# 2) get the rootpool's name
# 3) get the mountpoint value of rootpool/ROOT
# 4) check to see if the upper value equal to legacy or not.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-02-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_001_pos
#
# DESCRIPTION:
#       'zfs rename' should successfully rename valid datasets.
#       As a sub-assertion we check to ensure the datasets that can
#       be mounted are mounted.
#
# STRATEGY:
#       1. Given a file system, snapshot and volume.
#       2. Rename each dataset object to a new name.
#       3. Verify that only the new name is displayed by zfs list.
#       4. Verify mountable datasets are mounted.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_004_neg
#
# DESCRIPTION:
#       'zfs rename' should fail when this dataset was changed to an existed 
#	dataset name or datasets are of different types.
#       For example, a filesystem cannot be renamed as a volume.
#
# STRATEGY:
#       1. Given a file system, snapshot and volume.
#       2. Rename each dataset object to a different type.
#       3. Verify that only the original name is displayed by zfs list.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_010_neg
#
# DESCRIPTION:
#	The recursive flag -r can only be used for snapshots and not for
#	volumes/filesystems.
#
# STRATEGY:
#	1. Loop pool, fs, container and volume.
#	2. Verify none of them can be rename by rename -r.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_011_pos
#
# DESCRIPTION
#       'zfs rename -p' should work as expected
#
# STRATEGY:
#	1. Make sure the upper level of $newdataset does not exist
#       2. Make sure without -p option, 'zfs rename' will fail
#       3. With -p option, rename works
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_008_pos
#
# DESCRIPTION:
#	zfs rename -r can rename snapshot recursively.
#
# STRATEGY:
#	1. Create snapshot recursively.
#	2. Rename snapshot recursively.
#	3. Verify rename -r snapshot correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_005_neg
#
# DESCRIPTION:
#       'zfs rename' should fail when the dataset are not within the same pool 
#
# STRATEGY:
#       1. Given a file system, snapshot and volume.
#       2. Rename each dataset object to a different pool.
#       3. Verify the operation fails, and only the original name 
#	   is displayed by zfs list.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_009_neg
#
# DESCRIPTION:
#	A snapshot already exists with the new name, then none of the
#	snapshots is renamed.
#
# STRATEGY:
#	1. Create snapshot for a set of datasets.
#	2. Create a new snapshot for one of datasets.
#	3. Using rename -r command with exists snapshot name.
#	4. Verify none of the snapshots is renamed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_012_neg
#
# DESCRIPTION:
#	'zfs rename' should be failed with bad option, null target dataset,  
#	too many datasets and long target dataset name.
#
# STRATEGY:
#	1. Create a set of ZFS datasets;
#	2. Try 'zfs rename' with various illegal scenarios;
#	3. Verify 'zfs rename' command should be failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_013_pos
#
# DESCRIPTION:
#	zfs rename -r can rename snapshot when child datasets
#	don't have a snapshot of the given name.
#
# STRATEGY:
#	1. Create snapshot.
#	2. Rename snapshot recursively.
#	3. Verify rename -r snapshot correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_006_pos
#
# DESCRIPTION: 
#       'zfs rename' can successfully rename a volume snapshot.
#
# STRATEGY:
#       1. Create a snapshot of volume.
#       2. Rename volume snapshot to a new one.
#	3. Rename volume to a new one.
#       5. Verify that the rename operations are successful and zfs list can
#	   list them.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_002_pos
#
# DESCRIPTION: 
#       'zfs rename' should successfully be capable of renaming
#       valid datasets back and forth multiple times.
#
# STRATEGY:
#       1. Given a file system, snapshot and volume.
#       2. Rename each dataset object to a new name.
#       3. Rename each dataset back to its original name.
#       4. Repeat steps 2 and 3 multiple times.
#       5. Verify that the correct name is displayed by zfs list.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_007_pos
#
# DESCRIPTION:
#	Rename dataset, verify that the data haven't changed.
#
# STRATEGY:
#	1. Create random data and copy to dataset.
#	2. Perform renaming commands.
#	3. Verify that the data haven't changed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rename_003_pos
#
# DESCRIPTION:
#	'zfs rename' can address the abbreviated snapshot name. 
#
# STRATEGY:
#	1. Create pool, fs and snap.
#	2. Verify 'zfs rename' support the abbreviated snapshot name.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_set_001_pos
#
# DESCRIPTION:
#
# Zpool set usage message is displayed when called with no arguments
#
# STRATEGY:
#	1. Run zpool set
#	2. Check that exit status is set to 2
#	3. Check usage message contains text "usage"
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_set_002_neg
#
# DESCRIPTION:
#
# Malformed zpool set commands are rejected
#
# STRATEGY:
#	1. Create an array of many different malformed zfs set arguments
#	2. Run zpool set for each arg checking each will exit with status code 1
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_set_003_neg
#
# DESCRIPTION:
#
# zpool set cannot set a readonly property
#
# STRATEGY:
# 1. Create a pool
# 2. Verify that we can't set readonly properties on that pool
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_export_002_pos
#
# DESCRIPTION:
# The 'zpool export' command must fail when a pool is
# busy i.e. mounted.
#
# STRATEGY:
# 1. Try and export the default pool when mounted and busy.
# 2. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_export_003_neg
#
# DESCRIPTION: 
#       'zpool export' should return an error with badly formed parameters,
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute 'zpool export'
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_export_004_pos 
#
# DESCRIPTION:
#	Verify zpool export succeed or fail with spare.
#
# STRATEGY:
#	1. Create two mirror pools with same spare.
#	2. Verify zpool export one pool succeed.
#	3. Import the pool.
#	4. Replace one device with the spare and detach it in one pool.
#	5. Verify zpool export the pool succeed.
#	6. Import the pool.
#	7. Replace one device with the spare in one pool.
#	8. Verify zpool export the pool fail.
#	9. Verify zpool export the pool with "-f" succeed.
#	10. Import the pool.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-03-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_export_001_pos
#
# DESCRIPTION:
# Exported pools should no longer be visible from 'zpool list'.
# Therefore, we export an existing pool and verify it cannot
# be accessed.
#
# STRATEGY:
# 1. Unmount the test directory.
# 2. Export the pool.
# 3. Verify the pool is no longer present in the list output.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_reservation_002_pos
#
# DESCRIPTION:
# A reservation of 'none' (which is an alias for 0) should be allowed. This
# test verifies that is true.
#
# STRATEGY:
# 1. Create a new file system in the test pool.
# 2. Set the reservation to 'none'.
# 3. Verify the associated reservation is indeed 0.
# 4. Repeat with reservation set to 0.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_reservation_001_pos
#
# DESCRIPTION:
# Exceed the maximum limit for a reservation and ensure it fails.
#
# STRATEGY:
# 1. Create a reservation file system.
# 2. Set the reservation to an absurd value.
# 3. Verify the return code is an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_all_001_pos
#
# DESCRIPTION:
#       Verify that 'zfs unmount -a[f]' succeeds as root.
#
# STRATEGY:
#       1. Create a group of pools with specified vdev.
#       2. Create zfs filesystems within the given pools.
#       3. Mount all the filesystems.
#       4. Verify that 'zfs unmount -a[f]' command succeed,
#	   and all available ZFS filesystems are unmounted.
#	5. Verify that 'zfs mount' is identical with 'df -F zfs'
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_003_pos
#
# DESCRIPTION:
# If invoke "zfs unmount [-f]" with a filesystem|mountpoint
# whose mountpoint property is 'legacy' or 'none',
# it will fail with a return code of 1
# and issue an error message.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is mounted.
# 2. Apply 'zfs set mountpoint=legacy|none <filesystem>'.
# 3. Unmount the file system using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 4. Unmount the mountpoint using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 5. Verify the above expected results of the filesystem|mountpoint.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_007_neg
#
# DESCRIPTION:
#	Try each 'zfs unmount' with inapplicable scenarios to make sure
#	it returns an error. include:
#		* Multiple filesystem|mountpoint specified	
#		* '-a', but also with a specific filesystem|mountpoint.
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute the sub-command
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_002_pos
#
# DESCRIPTION:
# If invoke "zfs unmount [-f]" with a filesystem|mountpoint
# whose name is not in "zfs list",
# it will fail with a return code of 1
# and issue an error message.
#
# STRATEGY:
# 1. Make sure that the non-existent ZFS filesystem|mountpoint
# not in 'zfs list'.
# 2. Unmount the file system using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 3. Unmount the mountpoint using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 4. Verify the above expected results of the filesystem|mountpoint.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_006_pos
#
# DESCRIPTION:
#	Re-creating zfs files, 'zfs unmount' still succeed. 
#
# STRATEGY:
#	1. Create pool and filesystem.
#	2. Recreating the same file in this fs for a while, then breaking out.
#	3. Verify the filesystem can be unmount successfully.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_004_pos
#
# DESCRIPTION:
# If invoke "zfs unmount [-f]" with a specific filesystem|mountpoint,
# which is not currently mounted,
# it will fail with a return code of 1
# and issue an error message.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is mounted.
# 2. Invoke 'zfs unmount <filesystem>'.
# 3. Verify that the filesystem is unmounted.
# 4. Unmount the file system using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 5. Unmount the mountpoint using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (FAILED)
# 6. Verify the above expected results of the filesystem|mountpoint.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_008_neg
#
# DESCRIPTION:
# Verify that zfs unmount should fail with bad parameters or scenarios:
#	1. bad option;
#	2. too many arguments;
#	3. null arguments;
#	4. invalid datasets;
#	5. invalid mountpoint;
#	6. already unmounted zfs filesystem;
#	7. legacy mounted zfs filesystem
#
# STRATEGY:
# 1. Make an array of bad parameters
# 2. Use zfs unmount to unmount the filesystem
# 3. Verify that zfs unmount returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_001_pos
#
# DESCRIPTION:
# Creates a file system and verifies that it can be unmounted
# using each of the various unmount options and sub-command
# variants.
#
# STRATEGY:
# 1. Create and mount a file system as necessary.
# 2. Umount the file system using the various combinations.
# 	- With force option.
# 	- Without force option.
# 	- Using the unmount sub-command.
# 	- Using the umount sub-command.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_005_pos
#
# DESCRIPTION:
# If invoke "zfs unmount" with a specific filesystem|mountpoint
# that have been mounted, but it's currently in use,
# it will fail with a return code of 1
# and issue an error message.
# But unmount forcefully will bypass this restriction and
# unmount that given filesystem successfully.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is mounted.
# 2. Change directory to that given mountpoint.
# 3. Unmount the file system using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (PASS)
# 4. Unmount the mountpoint using the various combinations.
# 	- Without force option. (FAILED)
# 	- With force option. (PASS)
# 5. Verify the above expected results of the filesystem|mountpoint.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unmount_009_pos
#
# DESCRIPTION:
# Verify that zfs unmount and destroy in a snapshot directory will not cause error.
#
# STRATEGY:
# 1. Create a file in a zfs filesystem, snapshot it and change directory to snapshot directory
# 2. Verify that 'zfs unmount -a'  will fail and 'zfs unmount -fa' will succeed
# 3. Verify 'ls' and 'cd /' will succeed
# 4. 'zfs mount -a' and change directory to snapshot directory again
# 5. Verify that zfs destroy snapshot will succeed
# 6. Verify 'ls' and 'cd /' will succeed
# 7. Create zfs filesystem, create a file, snapshot it and change to snapshot directory
# 8. Verify that zpool destroy the pool will succeed
# 9. Verify 'ls' 'cd /' 'zpool list' and etc will succeed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-07-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_009_neg
#
# DESCRIPTION:
#	Verify 'zfs receive' fails with bad options, missing argument or too many 
#	arguments.
#
# STRATEGY:
#	1. Set a array of illegal arguments
#	2. Execute 'zfs receive' with illegal arguments
#	3. Verify the command should be failed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_005_neg
#
# DESCRIPTION:
#	Verify 'zfs receive' fails with unsupported scenarios.
#	including:
#	(1) Invalid send streams;
#	(2) The received incremental send doesn't match the filesystem
#	    latest status.
#
# STRATEGY:
#	1. Preparation for unsupported scenarios 
#	2. Execute 'zfs receive' 
#	3. Verify the results are failed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_008_pos
#
# DESCRIPTION:
#	Verifying 'zfs receive -vn [<filesystem|snapshot>] 
#		   and zfs receive -vn -d <filesystem>'
#
# STRATEGY:
#	1. Fill in fs with some data
#	2. Create full and incremental send stream
#	3. run zfs receive with -v option
#	3. Dryrun zfs receive with -vn option
#	3. Dryrun zfs receive with -vn -d option
#	4. Verify receive output and result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_004_neg
#
# DESCRIPTION:
#	Verify 'zfs receive' fails with malformed parameters.
#
# STRATEGY:
#	1. Denfine malformed parameters array
#	2. Feed the malformed parameters to 'zfs receive' 
#	3. Verify the command should be failed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_001_pos
#
# DESCRIPTION:
#	Verifying 'zfs receive [<filesystem|snapshot>] -d <filesystem>' works.
#
# STRATEGY:
#	1. Fill in fs with some data
#	2. Create full and incremental send stream 
#	3. Receive the send stream
#	4. Verify the restoring results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_003_pos
#
# DESCRIPTION:
#	'zfs recv -F' to force rollback. 
#
# STRATEGY:
#	1. Create pool and fs.
#	2. Create some files in fs and take a snapshot1.
#	3. Create another files in fs and take snapshot2.
#	4. Create incremental stream from snapshot1 to snapshot2.
#	5. fs rollback to snapshot1 and modify fs.
#	6. Verify 'zfs recv -F' can force rollback.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_007_neg
#
# DESCRIPTION:
#	'zfs recv -F' should fail if the incremental stream does not match
#
# STRATEGY:
#	1. Create pool and fs.
#	2. Create some files in fs and take snapshots.
#	3. Keep the incremental stream and restore the stream to the pool
#	4. Verify receiving the stream fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_006_pos
#
# DESCRIPTION:
#	'zfs recv -d <fs>' should create ancestor filesystem if it does not
#   exist and it should not fail if it exists
#
# STRATEGY:
#	1. Create pool and fs.
#	2. Create some files in fs and take snapshots.
#	3. Keep the stream and restore the stream to the pool
#	4. Verify receiving the stream succeeds, and the ancestor filesystem 
#	   is created if it did not exist
#	5. Verify receiving the stream still succeeds when ancestor filesystem
#	   exists
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_receive_002_pos
#
# DESCRIPTION:
#	Verifying 'zfs receive <volume>' works.
#
# STRATEGY:
#	1. Fill in volume with some data
#	2. Create full and incremental send stream
#	3. Restore the send stream  
#	4. Verify the restoring results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_get_001_pos
#
# DESCRIPTION:
#
# Zpool get usage message is displayed when called with no arguments
#
# STRATEGY:
#	1. Run zpool get
#	2. Check that exit status is set to 2
#	3. Check usage message contains text "usage"
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_get_004_neg
#
# DESCRIPTION:
#
# Malformed zpool get commands are rejected
#
# STRATEGY:
#
# 1. Run several different "zpool get" commands that should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_get_002_pos
#
# DESCRIPTION:
#
# zpool get all works as expected
#
# STRATEGY:
#
# 1. Using zpool get, retrieve all default values
# 2. Verify that the header is printed
# 3. Verify that we can see all the properties we expect to see
# 4. Verify that the total output contains just those properties + header.
#
# Test for those properties are expected to check whether their
# default values are sane, or whether they can be changed with zpool set.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID:  zpool_get_003_pos
#
# DESCRIPTION:
#
# Zpool get returns values for all known properties
#
# STRATEGY:
# 1. For all properties, verify zpool get retrieves a value
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_002_pos
#
# DESCRIPTION:
# import pools of all versions - zpool upgrade on each pools works
#
# STRATEGY:
# 1. Execute the command with several invalid options
# 2. Verify a 0 exit status for each
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_006_neg
#
# DESCRIPTION:
# Attempting to upgrade a non-existent pool will return an error
#
# STRATEGY:
# 1. Verify a pool doesn't exist, then try to upgrade it
# 2. Verify a 0 exit status
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_007_pos
#
# DESCRIPTION:
# import pools of all versions - verify the following operation not break.
#	* zfs create -o version=<vers> <filesystem>
#	* zfs upgrade [-V vers] <filesystem>
#	* zfs set version=<vers> <filesystem>
#
# STRATEGY:
# 1. Import pools of all versions
# 2. Setup a test enviorment over the old pools.
# 3. Verify the commands related to 'zfs upgrade' succeed as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_003_pos
#
# DESCRIPTION:
# Upgrading a pool that has already been upgraded succeeds.
#
# STRATEGY:
# 1. Upgrade a pool, then try to upgrade it again
# 2. Verify a 0 exit status
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_001_pos
#
# DESCRIPTION:
# Executing 'zpool upgrade -v' command succeeds, and also prints a description
# of at least the current ZFS version.
#
# STRATEGY:
# 1. Execute the command
# 2. Verify a 0 exit status
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_009_neg
#
# DESCRIPTION:
#
# Zpool upgrade -V shouldn't be able to upgrade a pool to an unknown version
#
# STRATEGY:
# 1. Take an existing pool
# 2. Attempt to upgrade it to an unknown version
# 3. Verify that the upgrade failed, and the pool version was still the original
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-09-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_005_neg
#
# DESCRIPTION:
# Variations of upgrade -v print usage message, return with non-zero status
#
# STRATEGY:
# 1. Execute the command with several invalid options
# 2. Verify a 0 exit status for each
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_008_pos
#
# DESCRIPTION:
#
# Zpool upgrade should be able to upgrade pools to a given version using -V
#
# STRATEGY:
# 1. For all versions pools that can be upgraded on a given OS version
#    (latest pool version - 1)
# 2. Pick a version that's a random number, greater than the version
#    we're running.
# 3. Attempt to upgrade that pool to the given version
# 4. Check the pool was upgraded correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-09-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_upgrade_004_pos
#
# DESCRIPTION:
# zpool upgrade -a works
#
# STRATEGY:
# 1. Create all upgradable pools for this system, then upgrade -a
# 2. Verify a 0 exit status
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_detach_001_neg
#
# DESCRIPTION:
# Executing 'zpool detach' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool detach' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_008_neg
#
# DESCRIPTION:
# 'zfs create' should return an error with badly formed parameters.
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute 'zfs create'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_005_pos
#
# DESCRIPTION:
# 'zfs create -o property=value filesystem' can successfully create a ZFS
# filesystem with multiple properties set. 
#
# STRATEGY:
# 1. Create a ZFS filesystem in the storage pool with multiple -o options
# 2. Verify the filesystem created successfully
# 3. Verify the properties are correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_001_pos
#
# DESCRIPTION:
# 'zfs create <filesystem>' can create a ZFS filesystem in the namespace.
#
# STRATEGY:
# 1. Create a ZFS filesystem in the storage pool
# 2. Verify the filesystem created successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_011_pos
#
# DESCRIPTION:
# 'zfs create -p'  should work as expecteed
#
# STRATEGY:
# 1. To create $newdataset with -p option, first make sure the upper level
#    of $newdataset does not exist
# 2. Make sure without -p option, 'zfs create' will fail
# 3. Create $newdataset with -p option, verify it is created
# 4. Run 'zfs create -p $newdataset' again, the exit code should be zero
#    even $newdataset exists
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_010_neg
#
# DESCRIPTION:
# 'zfs create [-b <blocksize> ] -V <size> <volume>' fails with badly formed 
# <size> or <volume> arguments,including:
#	*Invalid volume size and volume name
#	*Invalid blocksize
#	*Incomplete component in the dataset tree
#	*The volume already exists
#	*The volume name beyond the maximal name length - 256.
#       *Same property set multiple times via '-o property=value' 
#       *Filesystems's property set on volume
#
# STRATEGY:
# 1. Create an array of badly formed arguments
# 2. For each argument, execute 'zfs create -V <size> <volume>'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_009_neg
#
# DESCRIPTION:
# 'zfs create <filesystem>' fails with bad <filesystem> arguments, including:
# 	*Invalid character against the ZFS namespace
#	*Incomplete component
#	*Too many arguments
#	*Filesystem already exists 
#	*Beyond maximal name length.
#	*Same property set multiple times via '-o property=value' 
#	*Volume's property set on filesystem
#
# STRATEGY:
# 1. Create an array of <filesystem> arguments
# 2. Execute 'zfs create <filesystem>' with each argument
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_004_pos
#
# DESCRIPTION:
# 'zfs create -o property=value filesystem' can successfully create a ZFS
# filesystem with correct property set. 
#
# STRATEGY:
# 1. Create a ZFS filesystem in the storage pool with -o option
# 2. Verify the filesystem created successfully
# 3. Verify the property is correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_013_pos
#
# DESCRIPTION:
# 'zfs create -s -V <size> <volume>' can create various-size sparse volume
#  with long fs name
#
# STRATEGY:
# 1. Create a volume in the storage pool.
# 2. Verify the volume is created correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_006_pos
#
# DESCRIPTION:
# 'zfs create -o property=value -V size volume' can successfully create a ZFS
# volume with correct property set. 
#
# STRATEGY:
# 1. Create a ZFS volume in the storage pool with -o option
# 2. Verify the volume created successfully
# 3. Verify the property is correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_002_pos
#
# DESCRIPTION:
# 'zfs create -s -V <size> <volume>' can create various-size sparse volume.
#
# STRATEGY:
# 1. Create a volume in the storage pool.
# 2. Verify the volume is created correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_007_pos
#
# DESCRIPTION:
# 'zfs create -o property=value -V size volume' can successfully create a ZFS
# volume with multiple properties set. 
#
# STRATEGY:
# 1. Create a ZFS volume in the storage pool with -o option
# 2. Verify the volume created successfully
# 3. Verify the properties are correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_003_pos
#
# DESCRIPTION:
# 'zfs create [-b <blocksize>] -V <size> <volume>' can create a volume 
# with specified blocksize, which is power of 2 between 512 - 128k.
#
# STRATEGY:
# 1. Create a volume with blocksize in the storage pool
# 2. Verify the volume created successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-23)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_create_012_pos
#
# DESCRIPTION:
# 'zfs create -p -o version=1' should only cause the leaf filesystem to be version=1
#
# STRATEGY:
# 1. Create $newdataset with -p option, verify it is created
# 2. Verify only the leaf filesystem to be version=1, others use the current version
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_destroy_003_neg
#
# DESCRIPTION: 
#       'zpool destroy' should return an error with badly formed parameters,
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute 'zpool destroy'
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_destroy_002_pos
#
# DESCRIPTION: 
#	'zpool destroy -f <pool>' can forcely destroy the specified pool.
#
# STRATEGY:
#	1. Create a storage pool
#	2. Create some datasets within the pool
#	3. Change directory to any mountpoint of these datasets, 
#	   Verify 'zpool destroy' without '-f' will fail.
#	4. 'zpool destroy -f' the pool
#	5. Verify the pool is destroyed successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_destroy_001_pos
#
# DESCRIPTION: 
#	'zpool destroy <pool>' can successfully destroy the specified pool.
#
# STRATEGY:
#	1. Create a storage pool
#	2. Destroy the pool
#	3. Verify the is destroyed successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rollback_001_pos
#
# DESCRIPTION:
# 	'zfs rollback -r|-rf|-R|-Rf' will recursively destroy any snapshots 
#	more recent than the one specified. 
#
# STRATEGY:
#	1. Create pool, fs & volume.
#	2. Separately create three snapshots or clones for fs & volume
#	3. Roll back to the second snapshot and check the results.
#	4. Create the third snapshot or clones for fs & volume again.
#	5. Roll back to the first snapshot and check the results.
#	6. Separately create two snapshots for fs & volume.
#	7. Roll back to the first snapshot and check the results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rollback_004_neg
#
# DESCRIPTION:
#	'zfs rollback' should fail when passing invalid options, too many
#	arguments,non-snapshot datasets or missing datasets	
#
# STRATEGY:
#	1. Create an array of invalid options 
#	2. Execute 'zfs rollback' with invalid options, too many arguments 
#	   or missing datasets
#	3. Verify 'zfs rollback' return with errors
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rollback_002_pos
#
# DESCRIPTION:
#	'zfs rollback -f' will force unmount any filesystems. 
#
# STRATEGY:
#	1. Create pool & fs.
#	2. Create the snapshot of this file system.
#	3. Write the mountpoint directory of this file system.
#	4. Make sure 'zfs rollback -f' succeeds.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_rollback_003_neg
#
# DESCRIPTION:
#	Seperately verify 'zfs rollback ''|-f|-r|-rf|-R|-rR will fail in 
#	different conditions.
#
# STRATEGY:
#	1. Create pool and file system
#	2. Create 'snap' and 'snap1' of this file system.
#	3. Run 'zfs rollback ""|-f <snap>' and it should fail.
#	4. Create 'clone1' based on 'snap1'.
#	5. Run 'zfs rollback -r|-rf <snap>' and it should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_007_pos
#
# DESCRIPTION:
#	For raidz, one destroyed pools devices was removed or used by other
#	pool, it still can be imported correctly.
#
# STRATEGY:
#	1. Create a raidz pool A with N disks.
#	2. Destroy this pool A.
#	3. Create another pool B with 1 disk which was used by pool A.
#	4. Verify import this raidz pool can succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_003_pos
#
# DESCRIPTION:
#	Destroyed pools are not listed unless with -D option is specified.
#
# STRATEGY:
#	1. Create test pool A.
#	2. Destroy pool A.
#	3. Verify only 'import -D' can list pool A.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_012_pos
#
# DESCRIPTION:
# Once a pool has been exported, it should be recreated after a
# successful import, all the sub-filesystems within it should all be restored,
# include mount & share status. Verify that is true.
#
# STRATEGY:
#	1. Create the test pool and hierarchical filesystems.
#	2. Export the test pool, or destroy the test pool,
#		depend on testing import [-Df].
#	3. Import it using the various combinations.
#		- Regular import
#		- Alternate Root Specified
#	4. Verify the mount & share status is restored. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-11-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_013_neg
#
# DESCRIPTION:
#	For pool may be in use from other system, 
#	'zpool import' will prompt the warning and fails.
#
# STRATEGY:
#	1. Prepare rawfile that are created from other system.
#	2. Verify 'zpool import' will fail.
#	3. Verify 'zpool import -f' succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_all_001_pos
#
# DESCRIPTION:
# Verify that 'zpool import -a' succeeds as root.
#
# STRATEGY:
# 1. Create a group of pools with specified vdev.
# 2. Create zfs filesystems within the given pools.
# 3. Export the pools.
# 4. Verify that import command succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_missing_003_pos
#
# DESCRIPTION:
# 	Once a pool has been exported, but one or more devices are 
#	overlapped with other exported pool, import should handle
#	this kind of situation properly.
#
# STRATEGY:
#	1. Repeat 1-3, create two test pools upon device files separately.
#	   These two pools should have one or more devices are overlapped.
#	   using the various combinations.
#		- Regular pool
#		- Mirror
#		- Raidz
#	2. Create necessary filesystem and test files.
#	3. Export the test pool.
#	4. Verify 'zpool import -d' with these two pools will have results
#	   as described:
#		- Regular, report error while any number of devices failing.
#		- Mirror could withstand (N-1) devices failing 
#		  before data integrity is compromised 
#		- Raidz could withstand one devices failing 
#		  before data integrity is compromised 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_missing_002_pos
#
# DESCRIPTION:
# 	Once a pool has been exported, and one or more devices are 
#	move to other place, import should handle this kind of situation
#	as described:
#		- Regular, report error while any number of devices failing.
#		- Mirror could withstand (N-1) devices failing 
#		  before data integrity is compromised 
#		- Raidz could withstand one devices failing 
#		  before data integrity is compromised 
# 	Verify that is true.
#
# STRATEGY:
#	1. Create test pool upon device files using the various combinations.
#		- Regular pool
#		- Mirror
#		- Raidz
#	2. Create necessary filesystem and test files.
#	3. Export the test pool.
#	4. Move one or more device files to other directory 
#	5. Verify 'zpool import -d' with the new directory 
#	   will handle moved files successfullly.
#	   Using the various combinations.
#		- Regular import
#		- Alternate Root Specified
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_006_pos
#
# DESCRIPTION:
#	For mirror, N-1 destroyed pools devices was removed or used by other
#	pool, it still can be imported correctly.
#
# STRATEGY:
#	1. Create mirror with N disks.
#	2. Destroy this mirror.
#	3. Create another pool with N-1 disks which was used by this mirror.
#	4. Verify import mirror can succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_002_pos
#
# DESCRIPTION:
# Verify that an exported pool cannot be imported
# more than once.
#
# STRATEGY:
#	1. Populate the default test directory and unmount it.
#	2. Export the default test pool.
#	3. Import it using the various combinations.
#		- Regular import
#		- Alternate Root Specified
#	4. Verify it shows up under 'zpool list'.
#	5. Verify it contains a file.
#	6. Attempt to import it for a second time. Verify this fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_rename_001_pos
#
# DESCRIPTION:
# An exported pool can be imported under a different name. Hence
# we test that a previously exported pool can be renamed.
#
# STRATEGY:
#	1. Copy a file into the default test directory.
#	2. Umount the default directory.
#	3. Export the pool.
#	4. Import the pool using the name ${TESTPOOL}-new,
#	   and using the various combinations.
#               - Regular import
#               - Alternate Root Specified
#	5. Verify it exists in the 'zpool list' output.
#	6. Verify the default file system is mounted and that the file
#	   from step (1) is present.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_009_neg
#
# DESCRIPTION:
#	Try each 'zpool import' with inapplicable scenarios to make sure
#	it returns an error. include:
#		* A non-existent pool name is given
#		* '-d', but no device directory specified
#		* '-R', but no alter root directory specified
#		* '-a', but a pool name specified either
#		* more than 2 pool names is given
#		* The new pool name specified already exists
#		* Contain invalid characters not allowed in the ZFS namespace
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute the sub-command
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_004_pos
#
# DESCRIPTION:
#	Destroyed pools devices was moved to another directory, it still can be
#	imported correctly.
#
# STRATEGY:
#	1. Create test pool A with several devices.
#	2. Destroy pool A.
#	3. Move devices to another directory.
#	4. Verify 'zpool import -D' succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_008_pos
#
# DESCRIPTION:
#	For raidz2, two destroyed pool's devices were removed or used by other
#	pool, it still can be imported correctly.
#
# STRATEGY:
#	1. Create a raidz2 pool A with N disks.
#	2. Destroy this pool A.
#	3. Create another pool B with two disks which were used by pool A.
#	4. Verify import this raidz2 pool can succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_005_pos
#
# DESCRIPTION:
#	Destroyed pools devices was renamed, it still can be imported correctly.
#
# STRATEGY:
#	1. Create test pool A with several devices.
#	2. Destroy pool A and rename devices name.
#	3. Verify 'zpool import -D' succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_001_pos
#
# DESCRIPTION:
# Once a pool has been exported, it should be recreated after a
# successful import. Verify that is true.
#
# STRATEGY:
#	1. Populate the default test directory and unmount it.
#	2. Export the default test pool.
#	3. Import it using the various combinations.
#		- Regular import
#		- Alternate Root Specified
#	   Try to import it by name or guid randomly.
#	4. Verify it shows up under 'zpool list'.
#	5. Verify it can be mounted again and contains a file.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_010_pos
#
# DESCRIPTION:
#	'zpool -D -a' can import all the specified directories destroyed pools.
#
# STRATEGY:
#	1. Create a 5 ways mirror pool A with dev0/1/2/3/4, then destroy it.
#	2. Create a stripe pool B with dev1. Then destroy it.
#	3. Create a raidz2 pool C with dev2/3/4. Then destroy it.
#	4. Create a raidz pool D with dev3/4. Then destroy it.
#	5. Create a stripe pool E with dev4. Then destroy it.
#	6. Verify 'zpool import -D -a' recover all the pools.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_missing_001_pos
#
# DESCRIPTION:
# 	Once a pool has been exported, and one or more devices are 
#	damaged or missing (d/m), import should handle this kind of situation
#	as described:
#		- Regular, report error while any number of devices failing.
#		- Mirror could withstand (N-1) devices failing 
#		  before data integrity is compromised 
#		- Raidz could withstand one devices failing 
#		  before data integrity is compromised 
# 	Verify those are true.
#
# STRATEGY:
#	1. Create test pool upon device files using the various combinations.
#		- Regular pool
#		- Mirror
#		- Raidz
#	2. Create necessary filesystem and test files.
#	3. Export the test pool.
#	4. Remove one or more devices
#	5. Verify 'zpool import' will handle d/m device successfully.
#	   Using the various combinations.
#		- Regular import
#		- Alternate Root Specified
#	   It should be succeed with single d/m device upon 'raidz' & 'mirror',
#	   but failed against 'regular' or more d/m devices.
#	6. If import succeed, verify following is true:
#		- The pool shows up under 'zpool list'.
#		- The pool's health should be DEGRADED.
#		- It contains the correct test file
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_import_011_neg
#
# DESCRIPTION:
#	For strip pool, any destroyed pool devices was demaged, zpool import -D
#	will failed.
#
# STRATEGY:
#	1. Create strip pool A with three devices.
#	2. Destroy this pool B.
#	3. Create pool B with one of devices in step 1.
#	4. Verify 'import -D' pool A will failed whenever pool B was destroyed 
#	   or not.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_007_neg
#
# DESCRIPTION:
# 'zpool create' should return an error with badly formed parameters.
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute 'zpool create'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_002_pos
#
# DESCRIPTION:
# 'zpool create -f <pool> <vspec> ...' can successfully create a
# new pool in some cases.
#
# STRATEGY:
# 1. Prepare the scenarios for '-f' option
# 2. Use -f to override the devices to create new pools
# 3. Verify the pool created successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_006_pos
#
# DESCRIPTION:
#	Verify zpool create succeed with multiple keywords combination.
#
# STRATEGY:
#	1. Create base filesystem to hold virtual disk files.
#	2. Create several files >= 64M.
#	3. Verify 'zpool create' succeed with valid keywords combination.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_021_pos
#
# DESCRIPTION:
# 'zpool create -O property=value pool' can successfully create a pool 
# with correct filesystem property set. 
#
# STRATEGY:
# 1. Create a storage pool with -O option
# 2. Verify the pool created successfully
# 3. Verify the filesystem property is correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_012_neg
#
#
# DESCRIPTION:
# 'zpool create' will fail with formal disk slice in swap
#
#
# STRATEGY:
# 1. Get all the disk devices in swap
# 2. For each device, try to create a new pool with this device
# 3. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_016_pos
#
#
# DESCRIPTION:
# 'zpool create' will success with no device in swap
#
#
# STRATEGY:
# 1. delete all devices in the swap
# 2. create a zpool
# 3. Verify the creation is successed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_013_neg
#
#
# DESCRIPTION:
# 'zpool create' will fail with metadevice in swap
#
# STRATEGY:
# 1. Create a one way strip metadevice
# 2. Try to create a new pool with metadevice in swap
# 3. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_017_neg
#
#
# DESCRIPTION:
# 'zpool create' will fail with mountpoint exists and is not empty.
#
#
# STRATEGY:
# 1. Prepare the mountpoint put some stuff into it.
# 2. Verify 'zpool create' over that mountpoint fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_003_pos
#
# DESCRIPTION:
# 'zpool create -n <pool> <vspec> ...' can display the configuration without
# actually creating the pool.
#
# STRATEGY:
# 1. Create storage pool with -n option
# 2. Verify the pool has not been actually created
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_020_pos
#
# DESCRIPTION:
#
# zpool create -R works as expected
#
# STRATEGY:
# 1. Create a -R altroot pool
# 2. Verify the pool is mounted at the correct location
# 3. Verify that cachefile=none for the pool
# 4. Verify that root=<mountpoint> for the pool
# 5. Verify that no reference to the pool is found in /etc/zfs/zpool.cache

#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_018_pos
#
# DESCRIPTION:
#
# zpool create can create pools with specified properties
#
# STRATEGY:
# 1. Create a pool with all editable properties
# 2. Verify those properties are set
# 3. Create a pool with two properties set
# 4. Verify both properties are set correctly
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_011_neg
#
# DESCRIPTION:
# 'zpool create' will fail in the following cases:
# existent pool; device is part of an active pool; nested virtual devices;
# differently sized devices without -f option; device being currently
# mounted; devices in /etc/vfstab; specified as the dedicated dump device.
#
# STRATEGY:
# 1. Create case scenarios
# 2. For each scenario, try to create a new pool with the virtual devices
# 3. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_015_neg
#
#
# DESCRIPTION:
# 'zpool create' will fail with zfs vol device in swap
#
#
# STRATEGY:
# 1. Create a zpool
# 2. Create a zfs vol on zpool
# 3. Add this zfs vol device to swap
# 4. Try to create a new pool with devices in swap
# 5. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_023_neg
#
# DESCRIPTION:
# 'zpool create -O' should return an error with badly formed parameters.
#
# STRATEGY:
# 1. Create an array of parameters with '-O'
# 2. For each parameter in the array, execute 'zpool create -O'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_022_pos
#
# DESCRIPTION:
# 'zpool create -O property=value pool' can successfully create a pool
# with multiple filesystem properties set. 
#
# STRATEGY:
# 1. Create a storage pool with multiple -O options
# 2. Verify the pool created successfully
# 3. Verify the properties are correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_001_pos
#
# DESCRIPTION:
# 'zpool create <pool> <vspec> ...' can successfully create a
# new pool with a name in ZFS namespace.
#
# STRATEGY:
# 1. Create storage pools with a name in ZFS namespace with different
# vdev specs.
# 2. Verify the pool created successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_005_pos
#
# DESCRIPTION:
# 'zpool create [-R root][-m mountpoint] <pool> <vdev> ...' can create an
#  alternate root pool or a new pool mounted at the specified mountpoint.
#
# STRATEGY:
# 1. Create a pool with '-m' option 
# 2. Verify the pool is mounted at the specified mountpoint
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_009_neg
#
# DESCRIPTION:
#	Create a pool with same devices twice or create two pools with same
#	devices, 'zpool create' should failed.
#
# STRATEGY:
#	1. Loop to create the following three kinds of pools.
#		- Regular pool
#		- Mirror
#		- Raidz
#	2. Create two pools but using the same disks, expect failed. 
#	3. Create one pool but using the same disks twice, expect failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-15)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_008_pos
#
# DESCRIPTION:
# 'zpool create' have to use '-f' scenarios
#
# STRATEGY:
# 1. Prepare the scenarios
# 2. Create pool without '-f' and verify it fails
# 3. Create pool with '-f' and verify it succeeds
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_004_pos
#
# DESCRIPTION:
# 'zpool create [-f]' can create a storage pool with large number of 
# file-in-zfs-filesystem-based vdevs without any errors.
#
# STRATEGY:
# 1. Create assigned number of files in ZFS filesystem as vdevs 
# 2. Creating a new pool based on the vdevs should get success
# 3. Fill in the filesystem and create a partially writen file as vdev
# 4. Add the new file into vdevs list and create a pool 
# 5. Creating a storage pool with the new vdevs list should be failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_019_pos
#
# DESCRIPTION:
#
# zpool create cannot create pools specifying readonly properties
#
# STRATEGY:
# 1. Attempt to create a pool, specifying each readonly property in turn
# 2. Verify the pool was not created
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_010_neg
#
# DESCRIPTION:
# 'zpool create' should return an error with VDEVsof size  <64mb
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute 'zpool create'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_create_014_neg
#
#
# DESCRIPTION:
# 'zpool create' will fail with ordinary file in swap
#
# STRATEGY:
# 1. Create a regular file on top of UFS-zvol filesystem
# 2. Try to create a new pool with regular file in swap
# 3. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-04-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_007_pos
#
# DESCRIPTION:
# The following options can be set on a temporary basis using the -o option
# without affecting the on-disk property. The original on-disk value will be
# restored when the file system is unmounted and mounted.
#
#         PROPERTY		MOUNT OPTION
#	  atime			atime/noatime
#	  devices		devices/nodevices
#	  exec			exec/noexec
#	  readonly		ro/rw
#	  setuid		setuid/nosetuid
#
# STRATEGY:
#	1. Create filesystem and get origianl property value.
#	2. Using 'zfs mount -o' to set filesystem property.
#	3. Verify the property was set temporarily.
#	4. Verify it will not affect the property that is stored on disk.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_003_pos
#
# DESCRIPTION:
# Invoke "zfs mount <filesystem>" with a filesystem whose mountpoint property
# is 'legacy' or 'none',
# it will fail with a return code of 1 and issue an error message.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is unmounted.
# 2. Mount the filesystem using the various combinations
# 	- zfs set mountpoint=legacy <filesystem>
#	- zfs set mountpoint=none <filesystem>
# 3. Verify that mount failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_006_pos
#
# DESCRIPTION:
#	Invoke "zfs mount <filesystem>" with a filesystem
#	which mountpoint be the identical or the top of an existing one,	
#	it will fail with a return code of 1
#
# STRATEGY:
#	1. Prepare an existing mounted filesystem.
#	2. Setup a new filesystem and make sure that it is unmounted.
#       3. Mount the new filesystem using the various combinations
#		- zfs set mountpoint=<identical path> <filesystem>
#		- zfs set mountpoint=<top path> <filesystem>
#       4. Verify that mount failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_002_pos
#
# DESCRIPTION:
# Invoking "zfs mount <filesystem>" with a filesystem whose name is not in 
# "zfs list", will fail with a return code of 1.
#
# STRATEGY:
# 1. Make sure the NONEXISTFSNAME ZFS filesystem is not in 'zfs list'.
# 2. Invoke 'zfs mount <filesystem>'.
# 3. Verify that mount failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_010_neg
#
# DESCRIPTION:
# Verify that zfs mount should fail when mounting a mounted zfs filesystem or
# the mountpoint is busy
#
# STRATEGY:
# 1. Make a zfs filesystem mounted or mountpoint busy
# 2. Use zfs mount to mount the filesystem
# 3. Verify that zfs mount returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_all_001_pos
#
# DESCRIPTION:
#       Verify that 'zfs mount -a' succeeds as root.
#
# STRATEGY:
#       1. Create a group of pools with specified vdev.
#       2. Create zfs filesystems within the given pools.
#       3. Unmount all the filesystems.
#       4. Verify that 'zfs mount -a' command succeed,
#	   and all available ZFS filesystems are mounted.
#	5. Verify that 'zfs mount' is identical with 'df -F zfs'	
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_004_pos
#
# DESCRIPTION:
# Invoke "zfs mount <filesystem>" with a filesystem
# which has been already mounted,
# it will fail with a return code of 1
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is unmounted.
# 2. Invoke 'zfs mount <filesystem>'.
# 3. Verify that the filesystem is mounted.
# 4. Invoke 'zfs mount <filesystem>' the second times.
# 5. Verify the last mount operation failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_008_pos
#
# DESCRIPTION:
#	'zfs mount -O' allow the file system to be mounted over an existing
#	mount point, making the underlying file system inaccessible.
#
# STRATEGY:
#	1. Create two filesystem fs & fs1, and create two test files for them.
#	2. Unmount fs1 and set mountpoint property is identical to fs.
#	3. Verify 'zfs mount -O' will make the underlying filesystem fs
#	   inaccessible.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_009_neg
#
# DESCRIPTION: 
#	Try each 'zfs mount' with inapplicable scenarios to make sure
#	it returns an error. include:
#		* Multiple filesystems specified	
#		* '-a', but also with a specific filesystem.
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute the sub-command
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_005_pos
#
# DESCRIPTION:
# Invoke "zfs mount <filesystem>" with a filesystem
# but its mountpoint is currently in use,
# it will fail with a return code of 1
# and issue an error message.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is unmounted.
# 2. Apply 'zfs set mountpoint=path <filesystem>'.
# 3. Change directory to that given mountpoint.
# 3. Invoke 'zfs mount <filesystem>'.
# 4. Verify that mount failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_001_pos
#
# DESCRIPTION:
# Invoke "zfs mount <filesystem>" with a regular name of filesystem,
# will mount that filesystem successfully.
#
# STRATEGY:
# 1. Make sure that the ZFS filesystem is unmounted.
# 2. Invoke 'zfs mount <filesystem>'.
# 3. Verify that the filesystem is mounted.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_mount_011_neg
#
# DESCRIPTION:
# Verify that zfs mount should fail with bad parameters
#
# STRATEGY:
# 1. Make an array of bad parameters
# 2. Use zfs mount to mount the filesystem
# 3. Verify that zfs mount returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_004_pos
#
# DESCRIPTION:
#	Resilver prevent scrub from starting until the resilver completes
#
# STRATEGY:
#	1. Setup a mirror pool and filled with data.
#	2. Detach one of devices
#	3. Verify scrub failed until the resilver completed 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_001_neg
#
# DESCRIPTION:
# A badly formed parameter passed to 'zpool scrub' should
# return an error.
#
# STRATEGY:
# 1. Create an array containing bad 'zpool scrub' parameters.
# 2. For each element, execute the sub-command.
# 3. Verify it returns an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_005_pos
#
# DESCRIPTION:
#	When scrubbing, detach device should not break system.
#
# STRATEGY:
#	1. Setup filesys with data.
#	2. Detaching and attaching the device when scrubbing.
#	3. Try it twice, verify both of them work fine.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_003_pos
#
# DESCRIPTION:
#	scrub command terminates the existing scrub process and starts
#	a new scrub.
#
# STRATEGY:
#	1. Setup a pool and fill with data
#	2. Kick off a scrub
#	3. Check the completed percent and invoke another scrub
#	4. Check the percent again, verify a new scrub started.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_scrub_002_pos
#
# DESCRIPTION:
#	Verify scrub -s works correctly.
#
# STRATEGY:
#	1. Create pool and fill with hundreds data.
#	2. zpool scrub the pool
#	3. Verify zpool scrub -s succeed when the system is scrubbing.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_set_property_001_pos
#
# DESCRIPTION:
# For each property verify that it accepts on/off/inherit.
#
# STRATEGY:
# 1. Create an array of properties.
# 2. Create an array of possible values.
# 3. For each property set to every possible value.
# 4. Verify success is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_inherit_001_neg
#
# DESCRIPTION:
# 'zfs inherit' should return an error when attempting to inherit
# properties which are not inheritable.
#
# STRATEGY:
# 1. Create an array of properties which cannot be inherited
# 2. For each property in the array, execute 'zfs inherit'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_inherit_003_pos
#
# DESCRIPTION:
# 'zfs inherit' should return an error with bad parameters in one command. 
#
# STRATEGY:
# 1. Set an array of bad options and invlid properties to 'zfs inherit'
# 2. Execute 'zfs inherit' with bad options and passing invlid properties
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_inherit_002_neg
#
# DESCRIPTION:
# 'zfs inherit' should return an error with bad parameters in one command. 
#
# STRATEGY:
# 1. Set an array of bad options and invlid properties to 'zfs inherit'
# 2. Execute 'zfs inherit' with bad options and passing invlid properties
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_online_002_neg
#
# DESCRIPTION:
# Executing 'zpool online' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool online' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_online_001
#
# DESCRIPTION:
# Executing 'zpool online' with valid parameters succeeds.
#
# STRATEGY:
# 1. Create an array of correctly formed 'zpool online' options
# 2. Execute each element of the array.
# 3. Verify use of each option is successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_007_pos
#
# DESCRIPTION:
# 'zfs clone -o version=' could upgrade version, but downgrade is denied.
#
# STRATEGY:
# 1. Create clone with "-o version=" specified
# 2. Verify it succeed while upgrade, but fails while the version downgraded.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_003_pos
#
# DESCRIPTION:
# 'zfs clone -o property=value filesystem' can successfully create a ZFS
# clone filesystem with correct property set. 
#
# STRATEGY:
# 1. Create a ZFS clone filesystem in the storage pool with -o option
# 2. Verify the filesystem created successfully
# 3. Verify the property is correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_006_pos
#
# DESCRIPTION:
# 'zfs clone -o property=value volume' can successfully create a ZFS
# clone volume with multiple properties set. 
#
# STRATEGY:
# 1. Create a ZFS clone volume in the storage pool with -o option
# 2. Verify the volume created successfully
# 3. Verify the properties are correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_002_pos
#
# DESCRIPTION: 
#	'zfs clone -p' should work as expected
#
# STRATEGY:
#	1. prepare snapshots
#	2. make sure without -p option, 'zfs clone' will fail
#	3. with -p option, the clone can be created
#	4. run 'zfs clone -p' again, the exit code should be zero
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_004_pos
#
# DESCRIPTION:
# 'zfs clone -o property=value filesystem' can successfully create a ZFS
# clone filesystem with multiple properties set. 
#
# STRATEGY:
# 1. Create a ZFS clone filesystem in the storage pool with multiple -o options
# 2. Verify the filesystem created successfully
# 3. Verify the properties are correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_009_neg
#
# DESCRIPTION:
# 'zfs clone -o <volume>' fails with badly formed arguments,including:
#       *Same property set multiple times via '-o property=value' 
#       *Filesystems's property set on volume
#
# STRATEGY:
# 1. Create an array of badly formed arguments
# 2. For each argument, execute 'zfs clone -o <volume>'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_001_neg
#
# DESCRIPTION: 
#	'zfs clone' should fail with inapplicable scenarios, including:
#		* Null arguments
#		* non-existant snapshots.
#		* invalid characters in ZFS namesapec
#		* Leading slash in the target clone name
#		* The argument contains an empty component.
#		* The pool specified in the target doesn't exist.
#		* The parent dataset of the target doesn't exist.
#		* The argument refer to a pool, not dataset.
#		* The target clone already exists.
#		* Null target clone argument.
#		* Too many arguments. 
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute the sub-command
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_005_pos
#
# DESCRIPTION:
# 'zfs clone -o property=value -V size volume' can successfully create a ZFS
# clone volume with correct property set. 
#
# STRATEGY:
# 1. Create a ZFS clone volume in the storage pool with -o option
# 2. Verify the volume created successfully
# 3. Verify the property is correctly set
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_clone_008_neg
#
# DESCRIPTION:
# 'zfs clone -o <filesystem>' fails with bad <filesystem> arguments, including:
#	*Same property set multiple times via '-o property=value' 
#	*Volume's property set on filesystem
#
# STRATEGY:
# 1. Create an array of <filesystem> arguments
# 2. Execute 'zfs clone -o <filesystem>' with each argument
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_expand_002_pos
#
# DESCRIPTION:
# After zpool online -e poolname zvol vdevs, zpool can autoexpand by 
# Dynamic LUN Expansion
# 
#
# STRATEGY:
# 1) Create a pool
# 2) Create volume on top of the pool
# 3) Create pool by using the zvols
# 4) Expand the vol size by zfs set volsize
# 5  Use zpool online -e to online the zvol vdevs
# 6) Check that the pool size was expaned
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_expand_003_neg
#
# Description:
# Once set zpool autoexpand=off, zpool can *NOT* autoexpand by 
# Dynamic LUN Expansion
# 
#
# STRATEGY:
# 1) Create a pool
# 2) Create volumes on top of the pool
# 3) Create pool by using the zvols and set autoexpand=off
# 4) Expand the vol size by zfs set volsize
# 5) Check that the pool size is not changed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_expand_001_pos
#
# DESCRIPTION:
# Once zpool set autoexpand=on poolname, zpool can autoexpand by 
# Dynamic LUN Expansion
# 
#
# STRATEGY:
# 1) Create a pool
# 2) Create volume on top of the pool
# 3) Create pool by using the zvols and set autoexpand=on
# 4) Expand the vol size by 'zfs set volsize'
# 5) Check that the pool size was expanded
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-06-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_remove_001_neg
#
# DESCRIPTION:
# Verify that 'zpool can not remove device except inactive hot spares from pool'
#
# STRATEGY:
# 1. Create all kinds of pool (strip, mirror, raidz, hotspare)
# 2. Try to remove device from the pool
# 3. Verify that the remove failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_remove_002_pos
#
# DESCRIPTION:
# Verify that 'zpool can only remove inactive hot spare devices from pool'
#
# STRATEGY:
# 1. Create a hotspare pool
# 2. Try to remove the inactive hotspare device from the pool
# 3. Verify that the remove succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_remove_003_pos
#
# DESCRIPTION:
# Verify that 'zpool can remove hotspare devices from pool when it state
#              switch from active to inactive'
#
# STRATEGY:
# 1. Create a hotspare pool
# 2. Try to replace the inactive hotspare device to active device in the pool
# 3. Try to detach active (spare) device to make it inactive
# 3. Verify that the zpool remove succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_send_001_pos
#
# DESCRIPTION:
#	Verify 'zfs send' can create valid send streams as expected. 
#
# STRATEGY:
#	1. Fill in fs with some data
#	2. Create a full send streams with the fs
#	3. Receive the send stream and verify the data integrity
#	4. Fill in fs with some new data
#	5. Create an incremental send stream with the fs
#	6. Receive the incremental send stream and verify the data integrity.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_send_004_neg
#
# DESCRIPTION:
#	Verify 'zfs send' fails with malformed parameters.
#
# STRATEGY:
#	1. Define malformed parameters in array
#	2. Feed the parameters to 'zfs send' 
#	3. Verify the result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_send_002_pos
#
# DESCRIPTION:
#	Verify 'zfs send' can generate valid streams with a property setup. 
#
# STRATEGY:
#	1. Setup property for filesystem
#	2. Fill in some data into filesystem 
#	3. Create a full send streams 
#	4. Receive the send stream
#	5. Verify the receive result
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_send_003_pos
#
# DESCRIPTION:
#	'zfs send -i' can deal with abbreviated snapshot name. 
#
# STRATEGY:
#	1. Create pool, fs and two snapshots.
#	2. Make sure 'zfs send -i' support abbreviated snapshot name.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_006_pos
#
# DESCRIPTION:
# 'zpool add [-f]' can add large numbers of file-in-zfs-filesystem-based vdevs 
# to the specified pool without any errors.
#
# STRATEGY:
# 1. Create assigned number of files in ZFS filesystem as vdevs and use the first
# file to create a pool
# 2. Add other vdevs to the pool should get success
# 3  Fill in the filesystem and create a partially written file 
# as vdev
# 4. Add the new file into the pool should be failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-09)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_002_pos
#
# DESCRIPTION:
# 	'zpool add -f <pool> <vdev> ...' can successfully add the specified
# devices to given pool in some cases.
#
# STRATEGY:
# 	1. Create a mirrored pool
#	2. Without -f option to add 1-way device the mirrored pool will fail
# 	3. Use -f to override the errors to add 1-way device to the mirrored 
# 	pool 
# 	4. Verify the device is added successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_007_neg
#
# DESCRIPTION: 
#       'zpool add' should return an error with badly-formed parameters,
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute 'zpool add'
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_003_pos
#
# DESCRIPTION:
# 	'zpool add -n <pool> <vdev> ...' can display the configuration without
# adding the specified devices to given pool
#
# STRATEGY:
# 	1. Create a storage pool
# 	2. Use -n to add a device to the pool
# 	3. Verify the device is not added actually
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_005_pos
#
# DESCRIPTION: 
#       'zpool add' should return fail if 
#	1. vdev is part of an active pool
# 	2. vdev is currently mounted
# 	3. vdev is in /etc/vfstab
#	3. vdev is specified as the dedicated dump device
#
# STRATEGY:
#	1. Create case scenarios
#	2. For each scenario, try to add the device to the pool
#	3. Verify the add operation get failed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_001_pos
#
# DESCRIPTION: 
# 	'zpool add <pool> <vdev> ...' can successfully add the specified 
# devices to the given pool
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add spare devices to the pool
#	3. Verify the devices are added to the pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2005-09-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_008_neg
#
# DESCRIPTION: 
#       'zpool add' should return an error with nonexistent pools or vdevs
#
# STRATEGY:
#	1. Create an array of parameters which contains nonexistent pools/vdevs
#	2. For each parameter in the array, execute 'zpool add'
#	3. Verify an error is returned
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_004_pos
#
# DESCRIPTION: 
# 	'zpool add <pool> <vdev> ...' can successfully add a zfs volume 
# to the given pool
#
# STRATEGY:
#	1. Create a storage pool and a zfs volume
#	2. Add the volume to the pool
#	3. Verify the devices are added to the pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_add_009_neg
#
# DESCRIPTION: 
#       'zpool add' should return fail if vdevs are the same or vdev is 
# contained in the given pool
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add the two same devices to pool A
#	3. Add the device in pool A to pool A again
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_status_001_pos
#
# DESCRIPTION:
# Executing 'zpool status' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool status' options
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_status_002_pos
#
# DESCRIPTION:
# Executing 'zpool status' with correct options succeeds
#
# STRATEGY:
# 1. Create an array of correctly formed 'zpool status' options
# 2. Execute each element of the array.
# 3. Verify use of each option is successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_replace_001_neg
#
# DESCRIPTION:
# Executing 'zpool replace' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool replace' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_offline_001_pos
#
# DESCRIPTION:
# Executing 'zpool offline' with valid parameters succeeds.
#
# STRATEGY:
# 1. Create an array of correctly formed 'zpool offline' options
# 2. Execute each element of the array.
# 3. Verify use of each option is successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_offline_002_neg
#
# DESCRIPTION:
# Executing 'zpool offline' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool offline' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_002_pos
#
# DESCRIPTION:
# With ZFS_ABORT set, all zfs commands should be able to abort and generate a core file.
#
# STRATEGY:
# 1. Create an array of zfs command
# 2. Execute each command in the array
# 3. Verify the command aborts and generate a core file 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_003_neg
#
# DESCRIPTION:
# zfs command will failed with unexpected scenarios:
# (1) ZFS_DEV cannot be opened
# (2) MNTTAB cannot be opened
#
# STRATEGY:
# 1. Create an array of zfs command
# 2. Execute each command in the array
# 3. Verify the command aborts and generate a core file 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_001_neg
#
# DESCRIPTION:
# Try each zfs(1) sub-command without parameters to make sure
# it returns an error.
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute the sub-command
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_attach_001_neg
#
# DESCRIPTION:
# Executing 'zpool attach' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool attach' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_clear_001_pos
#
# DESCRIPTION:
# Verify 'zpool clear' can clear pool errors. 
#
# STRATEGY:
# 1. Create various configuration pools
# 2. Make errors to pool
# 3. Use zpool clear to clear errors
# 4. Verify the errors has been cleared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_clear_003_neg
#
# DESCRIPTION:
# Verify 'zpool clear' cannot used to spare device. 
#
# STRATEGY:
# 1. Create a spare pool.
# 2. Try to clear the spare device
# 3. Verify it returns an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_clear_002_neg
#
# DESCRIPTION:
# A badly formed parameter passed to 'zpool clear' should
# return an error.
#
# STRATEGY:
# 1. Create an array containing bad 'zpool clear' parameters.
# 2. For each element, execute the sub-command.
# 3. Verify it returns an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_009_pos
#
# DESCRIPTION:
# Verify that umount/rollback/destroy fails does not unshare the shared 
# file system
#
# STRATEGY:
# 1. Share the filesystem via 'zfs set sharenfs'.
# 2. Try umount failure, and verify that the file system is still shared.
# 3. Try rollback failure, and verify that the file system is still shared.
# 4. Try destroy failure, and verify that the file system is still shared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_001_pos
#
# DESCRIPTION:
# Verify that 'zfs set sharenfs' and 'zfs share' shares a given dataset.
#
# STRATEGY:
# 1. Invoke 'zfs set sharenfs'.
# 2. Verify that the file system is shared.
# 3. Invoke 'zfs share'.
# 4. Verify that the file system is shared.
# 5. Verify that a shared filesystem cannot be shared again.
# 6. Verify that share -a succeeds.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_005_pos
#
# DESCRIPTION:
# Verify that NFS share options are propagated correctly.
#
# STRATEGY:
# 1. Create a ZFS file system.
# 2. For each option in the list, set the sharenfs property.
# 3. Verify through the share command that the options are propagated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_008_neg
#
# DESCRIPTION:
# Verify that sharing a dataset other than filesystem fails.
#
# STRATEGY:
# 1. Create a ZFS file system.
# 2. For each dataset in the list, set the sharenfs property.
# 3. Verify that the invalid datasets are not shared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_004_pos
#
# DESCRIPTION:
# Verify that a file system and its snapshot are shared.
#
# STRATEGY:
# 1. Create a file system
# 2. Set the sharenfs property on the file system
# 3. Create a snapshot
# 4. Verify that both are shared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_009_neg
#
# DESCRIPTION:
# Verify that zfs share should fail when sharing a shared zfs filesystem 
#
# STRATEGY:
# 1. Make a zfs filesystem shared
# 2. Use zfs share to share the filesystem
# 3. Verify that zfs share returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_010_neg
#
# DESCRIPTION:
# Verify that zfs share should fail with bad parameters
#
# STRATEGY:
# 1. Make an array of bad parameters
# 2. Use zfs share to share the filesystem
# 3. Verify that zfs share returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_002_pos
#
# DESCRIPTION:
# Verify that "zfs share" with a non-existent file system fails.
#
# STRATEGY:
# 1. Make sure the NONEXISTFSNAME ZFS file system is not in 'zfs list'.
# 2. Invoke 'zfs share <file system>'.
# 3. Verify that share fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_006_pos
#
# DESCRIPTION:
# Verify that a dataset could not be shared but filesystems are shared.
#
# STRATEGY:
# 1. Create a dataset and file system
# 2. Set the sharenfs property on the dataset
# 3. Verify that the dataset is unable be shared.
# 4. Add a new file system to the dataset.
# 5. Verify that the newly added file system be shared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_007_neg
#
# DESCRIPTION:
# Verify that invalid share parameters and options are caught.
#
# STRATEGY:
# 1. Create a ZFS file system.
# 2. For each option in the list, set the sharenfs property.
# 3. Verify that the error code and sharenfs property.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_share_003_pos
#
# DESCRIPTION:
# Invoking "zfs share <file system>" with a file system
# whose sharenfs property is 'off' , will fail with a
# return code of 1 and issue an error message.
#
# STRATEGY:
# 1. Make sure that the ZFS file system is unshared.
# 2. Mount the file system using the various combinations
# - zfs set sharenfs=off <file system>
# - zfs set sharenfs=none <file system>
# 3. Verify that share failed with return code of 1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_001_neg
#
# DESCRIPTION:
# A badly formed sub-command passed to zpool(1) should
# return an error.
#
# STRATEGY:
# 1. Create an array containg each zpool sub-command name.
# 2. For each element, execute the sub-command.
# 3. Verify it returns an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_003_pos
#
# DESCRIPTION:
#	Verify debugging features of zpool such as ABORT and freeze/unfreeze
#	should run successfully.
#
# STRATEGY:
# 1. Create an array containg each zpool options.
# 2. For each element, execute the zpool command.
# 3. Verify it run successfully.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_002_pos
#
# DESCRIPTION:
# With ZFS_ABORT set, all zpool commands should be able to abort and generate a core file.
#
# STRATEGY:
# 1. Create an array of zpool command
# 2. Execute each command in the array
# 3. Verify the command aborts and generate a core file 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_006_pos
#
# DESCRIPTION:
# 	Verify that the volume space used by multiple copies is charged correctly
#
# STRATEGY:
#	1. Create volume
#	2. Create UFS filesystem based on the volume
#	3. Set the copies property of volume to 1,2 or 3
#	4. Copy specified size data into each filesystem 
#	5. Verify that the volume space is charged as expected
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-06)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_002_pos
#
# DESCRIPTION:
# 	Verify that the space used by multiple copies is charged correctly
#
# STRATEGY:
#	1. Create filesystems with copies set as 2,3 respectively;
#	2. Copy specified size data into each filesystem; 
#	3. Verify that the space is charged as expected with zfs list, ls -s, df(1m),
#	   du(1) commands;
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_003_pos
#
# DESCRIPTION:
# 	Verify that the volume space used by multiple copies is charged correctly
#
# STRATEGY:
#	1. Create volume;
#	2. Create ZFS filesystem based on the volume;
#	3. Set the copies property of volume to 1,2 or 3; 
#	4. Copy specified size data into each filesystem; 
#	5. Verify that the volume space is charged as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_004_neg
#
# DESCRIPTION:
# 	Verify that copies cannot be set to other value except for 1, 2 or 3
#
# STRATEGY:
#	1. Create filesystems with copies set as any value other than 1, 2 or 3
#	2. Verify that the create operations fail
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_001_pos
#
# DESCRIPTION:
# 	Verify "copies" property can be correctly set as 1,2 and 3 and different
#	filesystem can have different value of "copies" property within the same pool.
#
# STRATEGY:
#	1. Create different filesystems with copies set as 1,2,3;
#	2. Verify that the "copies" property has been set correctly
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_copies_005_neg
#
# DESCRIPTION:
# 	Verify that copies cannot be set with pool version 1
#
# STRATEGY:
#	1. Create filesystems with copies set in a pool with version 1
#	2. Verify that the create operations fail
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zdb_001_neg
#
# DESCRIPTION:
# A badly formed parameter passed to zdb(1) should
# return an error.
#
# STRATEGY:
# 1. Create an array containg bad zdb parameters.
# 2. For each element, execute the sub-command.
# 3. Verify it returns an error.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_003_pos
#
# DESCRIPTION:
# 	Executing 'zfs upgrade [-V version] filesystem' command succeeds,
#	it could upgrade a filesystem to specific version or current version.
#
# STRATEGY:
# 1. Prepare a set of datasets which contain old-version and current version.
# 2. Execute 'zfs upgrade [-V version] filesystem', verify return 0, 
# 3. Verify the filesystem be updated as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_006_neg
#
# DESCRIPTION:
# Verify that invalid upgrade parameters and options are caught.
#
# STRATEGY:
# 1. Create a ZFS file system.
# 2. For each option in the list, try 'zfs upgrade'.
# 3. Verify that the operation fails as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_002_pos
#
# DESCRIPTION:
# 	Executing 'zfs upgrade -v ' command succeeds, it should 
#	show the info of available versions.
#
# STRATEGY:
# 1. Execute 'zfs upgrade -v', verify return value is 0.
# 2, Verify all the available versions info are printed out.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_007_neg
#
# DESCRIPTION:
# Verify that version should only by '1' '2' or current version, 
# non-digit input are invalid.
#
# STRATEGY:
# 1. For each invalid value of version in the list, try 'zfs upgrade -V version'.
# 2. Verify that the operation fails as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_004_pos
#
# DESCRIPTION:
# 	Executing 'zfs upgrade -r [-V version] filesystem' command succeeds,
#	it upgrade filesystem recursively to specific or current version.
#
# STRATEGY:
# 1. Prepare a set of datasets which contain old-version and current version.
# 2. Execute 'zfs upgrade -r [-V version] filesystem', verify return 0, 
# 3. Verify the filesystem be updated recursively as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_005_pos
#
# DESCRIPTION:
# 	Executing 'zfs upgrade [-V version] -a' command succeeds,
#	it upgrade all filesystems to specific or current version.
#
# STRATEGY:
# 1. Prepare a set of datasets which contain old-version and current version.
# 2. Execute 'zfs upgrade [-V version] -a', verify return 0, 
# 3. Verify all the filesystems be updated as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_upgrade_001_pos
#
# DESCRIPTION:
# 	Executing 'zfs upgrade' command succeeds, it should report 
#	the current system version and list all old-version filesystems.
#	If no old-version filesystems be founded, it prints out
#	"All filesystems are formatted with the current version."
#
# STRATEGY:
# 1. Prepare a set of datasets which contain old-version and current version.
# 2. Execute 'zfs upgrade', verify return 0, and it prints out
#	the current system version and list all old-version filesystems.
# 3. Remove all old-version filesystems, then execute 'zfs upgrade' again,
#	verify return 0, and get the expected message.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_001_pos
#
# DESCRIPTION: 
#	'zfs promote' can promote a clone filesystem to no longer be dependent
#	on its "origin" snapshot.
#
# STRATEGY:
#	1. Create a snapshot and a clone of the snapshot
#	2. Promote the clone filesystem
#	3. Verify the promoted filesystem become independent
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_005_pos
#
# DESCRIPTION:
#	The original fs was unmounted, 'zfs promote' still should succeed.
#
# STRATEGY:
#	1. Create pool, fs and snapshot.
#	2. Create clone of fs.
#	3. Unmount fs, then verify 'zfs promote' clone still succeed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_004_pos
#
# DESCRIPTION: 
#	'zfs promote' can deal with multi-level clones.
#
# STRATEGY:
#	1. Create multiple snapshots and multi-level clones
#	2. Promote a clone filesystem
#	3. Verify the dataset dependency relationships are correct after promotion. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_008_pos
#
# DESCRIPTION: 
#	'zfs promote' can successfully promote a volume clone.
#
# STRATEGY:
#	1. Create a volume clone
#	2. Promote the volume clone
#	3. Verify the dependency changed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_002_pos
#
# DESCRIPTION: 
#	'zfs promote' can deal with multiple snapshots in the origin filesystem.
#
# STRATEGY:
#	1. Create multiple snapshots and a clone of the last snapshot
#	2. Promote the clone filesystem
#	3. Verify the promoted filesystem included all snapshots
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_007_neg
#
# DESCRIPTION: 
#	'zfs promote' can deal with conflicts in the namespaces.
#
# STRATEGY:
#	1. Create a snapshot and a clone of the snapshot
#	2. Create the same name snapshot for the clone
#	3. Promote the clone filesystem
#	4. Verify the promote operation fail due to the name conflicts.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_003_pos
#
# DESCRIPTION: 
#	'zfs promote' can deal with multi-point snapshots.
#
# STRATEGY:
#	1. Create multiple snapshots and a clone to a middle point snapshot
#	2. Promote the clone filesystem
#	3. Verify the origin filesystem and promoted filesystem include 
#	   correct datasets seperated by the clone point.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_promote_006_neg
#
# DESCRIPTION: 
#	'zfs promote' will fail with invalid arguments:
#	(1) NULL arguments
#	(2) non-existent clone
#	(3) non-clone datasets:
#		pool, fs, snapshot,volume
#	(4) too many arguments.
#	(5) invalid options
#
# STRATEGY:
#	1. Create an array of invalid arguments
#	2. For each invalid argument in the array, 'zfs promote' should fail
#	3. Verify the return code from zfs promote
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-05-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_008_pos
#
# DESCRIPTION:
# Verify "-d <n>" can work with other options
#
# STRATEGY:
# 1. Create pool, filesystem, dataset, volume and snapshot.
# 2. Getting an -d option, other options and properties random combination.
# 3. Using the combination as the parameters of 'zfs get' to check the
# command line return value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-22)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_004_pos
#
# DESCRIPTION:
# Verify 'zfs get all' can get all properties for all datasets in the system
#
# STRATEGY:
#	1. Create datasets for testing 
#	2. Issue 'zfs get all' command
#	3. Verify the command gets all available properties of all datasets 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_005_neg
#
# DESCRIPTION:
# Setting the invalid option and properties, 'zfs get' should failed.
#
# STRATEGY:
# 1. Create pool, filesystem, volume and snapshot.
# 2. Getting incorrect combination by invalid parameters
# 3. Using the combination as the parameters of 'zfs get' to check the
# command line return value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_010_neg
#
# DESCRIPTION:
# A negative depth or a non numeric depth should fail in 'zfs get -d <n>'
#
# STRATEGY:
# 1. Run zfs get -d with negative depth or non numeric depth
# 2. Verify that zfs get returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-22)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_001_pos
#
# DESCRIPTION:
# Setting the valid option and properties, 'zfs get' should return the
# correct property value.
#
# STRATEGY:
# 1. Create pool, filesystem, volume and snapshot.
# 2. Setting valid parameter, 'zfs get' should succeed.
# 3. Compare the output property name with the original input property.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_009_pos
#
# DESCRIPTION:
#	'zfs get -d <n>' should get expected output.
#
# STRATEGY:
#	1. Create a multiple depth filesystem.
#	2. 'zfs get -d <n>' to get the output.
#	3. 'zfs get -r|egrep' to get the expected output.
#	4. Compare the two outputs, they shoud be same.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_003_pos
#
# DESCRIPTION:
#	'zfs get' should get consistent report with different options. 
#
# STRATEGY:
#	1. Create pool and filesystem.
#	2. 'zfs mount -o remount,noatime <fs>.'
#	3. Verify the value of 'zfs get atime' and 'zfs get all | grep atime'
#	   are identical.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_006_neg
#
# DESCRIPTION:
# Verify 'zfs get all' can deal with invalid scenarios
#
# STRATEGY:
# 1. Define invalid scenarios for 'zfs get all' 
# 2. Run zfs get with those invalid scenarios
# 3. Verify that zfs get fails with invalid scenarios
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_002_pos
#
# DESCRIPTION:
# Setting the valid option and properties 'zfs get' return correct value.
# It should be successful.
#
# STRATEGY:
# 1. Create pool, filesystem, dataset, volume and snapshot.
# 2. Getting the options and properties random combination.
# 3. Using the combination as the parameters of 'zfs get' to check the
# command line return value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_get_007_neg
#
# DESCRIPTION:
# 'zfs get -o' should fail with invalid column names 
#
# STRATEGY:
# 1. Run zfs get -o with invalid column name combinations
# 2. Verify that zfs get returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_history_001_neg
#
# DESCRIPTION:
#	Verify 'zpool history' can deal with non-existent pools and garbage 
#	to the command. 
#
# STRATEGY:
#	1. Create pool, volume & snap
#	2. Verify 'zpool history' can cope with incorret arguments.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zpool_history_002_pos
#
# DESCRIPTION:
#	Verify zpool history can handle options [-il] correctly.
#
# STRATEGY:
#	1. Create varied combinations of option -i & -l.
#	2. Verify 'zpool history' can cope with these combination correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-11-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_001_pos
#
# DESCRIPTION:
#	'zfs destroy -r|-rf|-R|-Rf <fs|ctr|vol|snap>' should recursively destroy
#	all children and clones based on options.
#
# STRATEGY:
#	1. Create test environment according to options. There are three test
#	models can be created. Only ctr, fs & vol; with snap; with clone.
#	2. According to option, make the dataset busy or not.
#	3. Run 'zfs destroy [-rRf] <dataset>'
#	4. According to dataset and option, check if get the expected results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-22)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_005_neg
#
# DESCRIPTION:
#	Seperately verify 'zfs destroy -f|-r|-rf|-R|-rR <dataset>' will fail in 
#       different conditions.
#
# STRATEGY:
#	1. Create pool, fs & vol.
#	2. Create snapshot for fs & vol.
#	3. Invoke 'zfs destroy ''|-f <dataset>', it should fail.
#	4. Create clone for fs & vol.
#	5. Invoke 'zfs destroy -r|-rf <dataset>', it should fail.
#	6. Write file to filesystem or enter snapshot mountpoint.
#	7. Invoke 'zfs destroy -R|-rR <dataset>', it should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-03)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_004_pos
#
# DESCRIPTION: 
#	Verify 'zfs destroy -f' succeeds as root.
#
# STRATEGY:
#	1. Create filesystem in the storage pool
#	2. Set mountpoint for the filesystem and make it busy
#	3. Verify that 'zfs destroy' fails to destroy the filesystem
#	4. Verify 'zfs destroy -f' succeeds to destroy the filesystem. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_007_neg
#
# DESCRIPTION:
#	'zpool destroy' failed if this filesystem is namespace-parent
#	of origin. 
#
# STRATEGY:
#	1. Create pool, fs and snapshot.
#	2. Create a namespace-parent of origin clone.
#	3. Promote this clone
#	4. Verify the original fs can not be destroyed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_002_pos
#
# DESCRIPTION: 
#	'zfs destroy <filesystem|volume|snapshot>' can successfully destroy 
#	the specified dataset which has no active dependents.
#
# STRATEGY:
#	1. Create a filesystem,volume and snapshot in the storage pool
#	2. Destroy the filesystem,volume and snapshot
#	3. Verify the datasets are destroyed successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_destroy_006_neg
#
# DESCRIPTION:
# 'zfs destroy' should return an error with badly formed parameters,
# including null destroyed object parameter, invalid options excluding
# '-r' and '-f', non-existent datasets.
#
# STRATEGY:
# 1. Create an array of parameters
# 2. For each parameter in the array, execute 'zfs destroy'
# 3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_005_neg
#
# DESCRIPTION:
#	Long name filesystem with snapshot should not break ZFS.
#
# STRATEGY:
#	1. Create filesystem and snapshot.
#	2. When the snapshot length is 256, rename the filesystem.
#	3. Verify it does not break ZFS
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-09)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_001_neg
#
# DESCRIPTION: 
#	Try each 'zfs snapshot' with inapplicable scenarios to make sure
#	it returns an error. include:
#		* No arguments given.
#		* The argument contains invalid characters for the ZFS namesapec
#		* Leading slash in snapshot name
#		* The argument contains an empty component.
#		* Missing '@' delimiter.
#		* Multiple '@' delimiters in snapshot name.
#		* The snapshot already exist.
#		* Create snapshot upon the pool. 
#			(Be removed since pool is treated as filesystem as well)
#		* Create snapshot upon a non-existent filesystem.
#		* Too many arguments. 
#
# STRATEGY:
#	1. Create an array of parameters
#	2. For each parameter in the array, execute the sub-command
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_004_neg
#
# DESCRIPTION:
#	Verify recursive snapshotting could not break ZFS. 
#
# STRATEGY:
#	1. Create deeply-nested filesystems until it is too long to create snap
#	2. Verify zfs snapshot -r pool@snap will not break ZFS
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-08-08)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_002_neg
#
# DESCRIPTION: 
#	"zfs snapshot -r" fails with invalid arguments or scenarios.
#	The invalid scenarios may include:
#	(1) The child filesystem already has snapshot with the same name
#	(2) The child volume already has snapshot with the same name
#
# STRATEGY:
#	1. Create an array of invalid arguments
#	2. Execute 'zfs snapshot -r' with each argument in the array, 
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_006_pos
#
# DESCRIPTION:
#	User property could be set via creation time by 'zfs snapshot -o'
#
# STRATEGY:
#	1. Create snapshot and give '-o property=value'
#	2. Verify the snapshot be created and user property have been set.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_007_pos
#
# DESCRIPTION:
#	'zfs snapshot -o' cannot set properties other than user property
#
# STRATEGY:
#	1. Create snapshot and give '-o property=value' with regular property.
#	2. Verify the snapshot creation failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_snapshot_003_neg
#
# DESCRIPTION: 
#	"zfs snapshot" fails with bad options,too many arguments or too long 
#	snapshot name
#
# STRATEGY:
#	1. Create an array of invalid arguments
#	2. Execute 'zfs snapshot' with each argument in the array, 
#	3. Verify an error is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_004_neg
#
# DESCRIPTION:
# Verify that "zfs unshare" issue error message with badly formed parameter.
#
# STRATEGY:
# 1. Define badly formed parameters
# 2. Invoke 'zfs unshare'
# 3. Verify that unshare fails and issue error message.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_001_pos
#
# DESCRIPTION:
# Verify that 'zfs unshare <filesystem|mountpoint>' unshares a given shared 
# filesystem.
#
# STRATEGY:
# 1. Share filesystems
# 2. Invoke 'zfs unshare <filesystem|mountpoint>' to unshare zfs file system
# 3. Verify that the file system is unshared
# 4. Verify that unsharing an unshared file system fails
# 5. Verify that "zfs unshare -a" succeeds to unshare all zfs file systems.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_005_neg
#
# DESCRIPTION:
# Verify that unsharing a dataset and mountpoint other than filesystem fails.
#
# STRATEGY:
# 1. Create a volume, dataset other than a ZFS file system
# 2. Verify that the datasets other than file system are not support by 'zfs unshare'.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_002_pos
#
# DESCRIPTION:
# Verify that 'zfs unshare [-a] <filesystem|mountpoint>' is aware of legacy share.
#
# STRATEGY:
# 1. Set 'zfs set sharenfs=off'
# 2. Use 'share' to share given filesystem
# 3. Verify that 'zfs unshare <filesystem|mountpoint>' is aware of legacy share
# 4. Verify that 'zfs unshare -a' is aware of legacy share.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_unshare_003_pos
#
# DESCRIPTION:
# Verify that a file system and its dependant are unshared when turn off sharenfs
# property.
#
# STRATEGY:
# 1. Create a file system
# 2. Set the sharenfs property on the file system
# 3. Create a snapshot
# 4. Verify that both are shared
# 5. Turn off the sharenfs property
# 6. Verify that both are unshared.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mountpoint_003_pos
#
# DESCRIPTION:
#	Verify FSType-specific option works well with legacy mount.
#
# STRATEGY:
#	1. Set up FSType-specific options and expected keywords array.
#	2. Create a test ZFS file system and set mountpoint=legacy.
#	3. Mount ZFS test filesystem with specific options.
#	4. Verify the filesystem was mounted with specific option.
#	5. Loop check all the options.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-01-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapdir_001_pos
#
# DESCRIPTION:
# Setting a valid snapdir on a dataset, it should be successful.
#
# STRATEGY:
# 1. Create pool, then create filesystem and volume within it.
# 2. Create a snapshot for each dataset.
# 3. Setting different valid snapdir to each dataset.
# 4. Check the return value and make sure it is 0.
# 5. Verify .zfs directory is hidden|visible according to the snapdir setting.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-02-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_set_001_neg
#
# DESCRIPTION:
# Setting invalid value to mountpoint, checksum, atime, readonly, setuid,
# zoned or canmount on a file system, volume. It should be failed.
#
# STRATEGY:
# 1. Create pool, then create file system & volume within it.
# 2. Setting invalid value, it should be failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: share_mount_001_neg
#
# DESCRIPTION:
# Verify that we cannot share or mount legacy filesystems.
#
# STRATEGY:
# 1. Set mountpoint as legacy or none
# 2. Use zfs share or zfs mount to share or mount the filesystem
# 3. Verify that the command returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: canmount_003_pos
#
# DESCRIPTION:
# While canmount=noauto and  the dataset is mounted, 
# zfs must not attempt to unmount it.
#
# STRATEGY:
# 1. Setup a pool and create fs, volume, snapshot clone within it.
# 2. Set canmount=noauto for each dataset and check the return value
#    and check if it still can not be unmounted when the dataset is mounted
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-09-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: user_property_001_pos
#
# DESCRIPTION:
# 	ZFS can set any valid user defined property to the non-readonly dataset.
#
# STRATEGY:
# 	1. Loop pool, fs and volume.
#	2. Combine all kind of valid characters into a valid user defined
#	   property name.
#	3. Random get a string as the value.
#	4. Verify all the valid user defined properties can be set to the
#	   dataset in #1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_001_pos
#
# DESCRIPTION:
# Setting a valid primarycache and secondarycache on file system or volume.
# It should be successful.
#
# STRATEGY:
# 1. Create pool, then create filesystem & volume within it.
# 2. Setting valid cache value, it should be successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: canmount_002_pos
#
# DESCRIPTION:
# Setting valid canmount to filesystem, it is successful.
# Whatever is set to volume or snapshot, it is failed.
# 'zfs set canmount=noauto <fs>'
#
# STRATEGY:
# 1. Setup a pool and create fs, volume, snapshot clone within it.
# 2. Set canmount=noauto for each dataset and check the retuen value
#    and check if it still can be mounted by mount -a.
# 3. mount each dataset(except volume) to see if it can be mounted.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-03-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: readonly_001_pos
#
# DESCRIPTION:
# Setting readonly on a dataset, it should keep the dataset as readonly.
#
# STRATEGY:
# 1. Create pool, then create filesystem and volume within it.
# 2. Setting readonly to each dataset.
# 3. Check the return value and make sure it is 0.
# 4. Verify the stuff under mountpoint is readonly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: user_property_004_neg
#
# DESCRIPTION:
#	User property has no effect to snapshot until 'Snapshot properties' supported.
#
# STRATEGY:
#	1. Verify user properties could be transformed by 'zfs snapshot'
#	2. Verify user properties could be set upon snapshot.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: compression_001_pos
#
# DESCRIPTION:
# Setting a valid compression on file system or volume.
# It should be successful.
#
# STRATEGY:
# 1. Create pool, then create filesystem & volume within it.
# 2. Setting valid value, it should be successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mountpoint_002_pos
#
# DESCRIPTION:
# 	If ZFS is currently managing the file system but it is currently unmoutned, 
#	and the mountpoint property is changed, the file system remains unmounted.
#
# STRATEGY:
# 1. Setup a pool and create fs, ctr within it.
# 2. Unmount that dataset
# 2. Change the mountpoint to the valid mountpoint value.
# 3. Check the file system remains unmounted.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: reservation_001_neg
#
# DESCRIPTION:
# Valid reservation values should be positive integers only.
#
# STRATEGY:
# 1) Form an array of invalid reservation values (negative and
# incorrectly formed)
# 2) Attempt to set each invalid reservation value in turn on a
# filesystem and volume.
# 3) Verify that attempt fails and the reservation value remains
# unchanged
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: user_property_002_pos
#
# DESCRIPTION:
#	User defined property are always inherited from its parent dataset
#	directly.
#
# STRATEGY:
#	1. Create pool, fs, volume, fsclone & volclone.
#	2. Get random user property name and set to the pool
#	3. Verify all dataset user property inherit from pool.
#	4. Set intermediate dataset and verify its children will inherit user
#	   property from it directly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: ro_props_001_pos
#
# DESCRIPTION:
# Verify that read-only properties are immutable.
#
# STRATEGY:
# 1. Create pool, fs, vol, fs@snap & vol@snap.
# 2. Get the original property value and set value to those properties.
# 3. Check return value.
# 4. Compare the current property value with the original one.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: canmount_sharenfs_001_pos
#
# DESCRIPTION:
# Verify canmount=noauto work fine when setting sharenfs or sharesmb.
#
# STRATEGY:
# 1. Create a fs canmount=noauto.
# 2. Set sharenfs or sharesmb.
# 3. Verify the fs is umounted.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: property_alias_001_pos
#
# DESCRIPTION:
# Verify the properties with aliases also work with those aliases
#
# STRATEGY:
# 1. Create pool, then create filesystem & volume within it.
# 2. Set or retrieve property via alias with datasets.
# 3. Verify the result should be successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: user_property_003_neg
#
# DESCRIPTION:
#	ZFS can handle any invalid user defined property.
#
# STRATEGY:
# 	1. Loop pool, fs and volume.
#	2. Combine all kind of invalid user property names.
#	3. Random get a string as the value.
#	4. Verify all the invalid user defined properties can not be set to the
#	   dataset in #1.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: onoffs_001_pos
#
# DESCRIPTION:
# Setting a valid value to atime, readonly, setuid or zoned on file
# system or volume. It should be successful.
#
# STRATEGY:
# 1. Create pool and filesystem & volume within it.
# 2. Setting valid value, it should be successful.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_set_002_neg
#
# DESCRIPTION:
# 'zfs set' should fail with invalid arguments 
#
# STRATEGY:
# 1. Create an array of invalid arguments
# 1. Run zfs set with each invalid argument
# 2. Verify that zfs set returns error
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zfs_set_003_neg
#
# DESCRIPTION:
# 'zfs set mountpoint/sharenfs' should fail when the mountpoint is invlid 
#
# STRATEGY:
# 1. Create invalid scenarios
# 2. Run zfs set mountpoint/sharenfs with invalid value
# 3. Verify that zfs set returns expected errors
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-9)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mountpoint_001_pos
#
# DESCRIPTION:
# Setting valid mountpoint to filesystem, it is successful.
# Whatever is set to volume, it is failed.
# 'zfs set mountpoint=<path>|legacy|none <fs|ctr|vol>'
#
# STRATEGY:
# 1. Setup a pool and create fs, ctr within it.
# 2. Loop all the valid mountpoint value.
# 3. Check the return value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: checksum_001_pos
#
# DESCRIPTION:
# Setting a valid checksum on a pool, file system, volume, it should be 
# successful.
#
# STRATEGY:
# 1. Create pool, then create filesystem and volume within it.
# 2. Setting different valid checksum to each dataset.
# 3. Check the return value and make sure it is 0.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_001_pos
#
# DESCRIPTION:
# Setting invalid primarycache and secondarycache on file system or volume.
# It should fail.
#
# STRATEGY:
# 1. Create pool, then create filesystem & volume within it.
# 2. Setting invalid {primary|secondary}cache value, it should fail.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: version_001_neg
#
# DESCRIPTION:
# Valid version values should be positive integers only.
#
# STRATEGY:
# 1) Form an array of invalid reservation values (negative and
# incorrectly formed)
# 2) Attempt to set each invalid version value in turn on a
# filesystem and volume.
# 3) Verify that attempt fails and the version value remains
# unchanged
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: canmount_001_pos
#
# DESCRIPTION:
# Setting valid canmount to filesystem, it is successful.
# Whatever is set to volume or snapshot, it is failed.
# 'zfs set canmount=on|off <fs>'
#
# STRATEGY:
# 1. Setup a pool and create fs, volume, snapshot clone within it.
# 2. Loop all the valid mountpoint value.
# 3. Check the return value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-30)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: pool_names_002_neg
#
# DESCRIPTION:
#
# Ensure that a set of invalid names cannot be used to create pools.
#
# STRATEGY:
# 1) For each invalid character in the character set, try to create
# and destroy the pool. Verify it fails.
# 2) Given a list of invalid pool names, ensure the pools are not
# created.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: pool_names_001_pos
#
# DESCRIPTION:
#
# Test that a set of valid names can be used to create pools. Further 
# verify that the created pools can be destroyed.
#
# STRATEGY:
# 1) For each valid character in the character set, try to create
# and destroy the pool.
# 2) Given a list of valid pool names, try to create and destroy
# pools with the given names.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-11-21)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: online_offline_001_pos
#
# DESCRIPTION:
# 	Turning a disk offline and back online during I/O completes.
#
# STRATEGY:
#	1. Create a mirror and start some random I/O
#	2. For each disk in the mirror, set it offline and online
#	3. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: online_offline_003_neg
#
# DESCRIPTION:
# 	Turning both disks offline should fail.
#
# STRATEGY:
#	1. Create a multidisk stripe and start some random I/O
#	2. For two disks in the stripe, set them offline sequentially.
#	3. Zpool offline should fail in both cases.
#	4. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: online_offline_002_neg
#
# DESCRIPTION:
# 	Turning both disks offline should fail.
#
# STRATEGY:
#	1. Create a mirror and start some random I/O
#	2. For each disk in the mirror, set them offline sequentially.
#	3. Only one disk can be offline at any one time.
#	4. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_006_pos
#
# DESCRIPTION:
# Migrating test file from UFS fs to ZFS fs using cpio
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Cpio up test file and place on a UFS filesystem
# 3. Extract cpio contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_002_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to UFS fs using tar.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Tar up test file and place on a ZFS filesystem
# 3. Extract tar contents to a UFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_007_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to ZFS fs using dd.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Dd up test file and place on a ZFS filesystem
# 3. Extract dd contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_003_pos
#
# DESCRIPTION:
# Migrating test file from UFS fs to ZFS fs using tar.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Tar up test file and place on a UFS filesystem
# 3. Extract tar contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_012_pos
#
# DESCRIPTION:
# Migrating test file from UFS fs to ZFS fs using cp
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. CP up test file and place on a UFS filesystem
# 3. Extract cp contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_009_pos
#
# DESCRIPTION:
# Migrating test file from UFS fs to ZFS fs using dd.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Dd up test file and place on a UFS filesystem
# 3. Extract dd contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_005_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to UFS fs using cpio
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Cpio up test file and place on a ZFS filesystem
# 3. Extract cpio contents to a UFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_001_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to ZFS fs using tar.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Tar up test file and place on a ZFS filesystem
# 3. Extract tar contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_010_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to ZFS fs using cp
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. CP up test file and place on a ZFS filesystem
# 3. Extract cp contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_011_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to UFS fs using cp
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. CP up test file and place on a ZFS filesystem
# 3. Extract cp contents to a UFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_004_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to ZFS fs using cpio
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Cpio up test file and place on a ZFS filesystem
# 3. Extract cpio contents to a ZFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: migration_008_pos
#
# DESCRIPTION:
# Migrating test file from ZFS fs to UFS fs using dd.
#
# STRATEGY:
# 1. Calculate chksum of testfile
# 2. Dd up test file and place on a ZFS filesystem
# 3. Extract dd contents to a UFS file system
# 4. Calculate chksum of extracted file
# 5. Compare old and new chksums.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: replacement_002_pos
#
# DESCRIPTION:
# 	Attaching disks during I/O should pass for supported pools.
#
# STRATEGY:
#	1. Create multidisk pools (stripe/mirror/raidz) and 
#	   start some random I/O
#	2. Attach a disk to the pool.
#	3. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: replacement_003_pos
#
# DESCRIPTION:
# 	Detaching disks during I/O should pass for supported pools.
#
# STRATEGY:
#	1. Create multidisk pools (stripe/mirror/raidz) and 
#	   start some random I/O
#	2. Detach a disk from the pool.
#	3. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: replacement_001_pos
#
# DESCRIPTION:
# 	Replacing disks during I/O should pass for supported pools.
#
# STRATEGY:
#	1. Create multidisk pools (stripe/mirror/raidz) and 
#	   start some random I/O
#	2. Replace a disk in the pool with anbother disk.
#	3. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-18)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: enospc_001
#
# DESCRIPTION:
# ENOSPC is returned on an attempt to write a second file
# to a file system after a first file was written that terminated
# with ENOSPC on a cleanly initialized file system.
#
# STRATEGY:
# 1. Write a file until the file system is full.
# 2. Ensure that ENOSPC is returned.
# 3. Write a second file while the file system remains full.
# 4. Verify the return code is ENOSPC.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rollback_002_pos
#
# DESCRIPTION:
# Verify that rollbacks are with respect to the latest snapshot.
#
# STRATEGY:
# 1. Empty a file system
# 2. Populate the file system
# 3. Take a snapshot of the file system
# 4. Add new files to the file system
# 5. Take a snapshot
# 6. Remove the original files
# 7. Perform a rollback
# 8. Verify the latest snapshot and file system agree
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_010_pos
#
# DESCRIPTION:
#	Verify 'destroy -r' can correctly destroy a snapshot tree at any point. 
#
# STRATEGY:
# 1. Use the snapshot -r to create snapshot for top level pool 
# 2. Select a middle point of the snapshot tree, use destroy -r to destroy all
#	snapshots beneath the point.
# 3. Verify the destroy results.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_014_pos
#
# DESCRIPTION:
#	verify that creating/destroying snapshots do things clean
#
# STRATEGY:
#	1. create a dataset and set a quota with 500m
#	2. create file of size 400m on the dataset	
#	3. take a snapshot and destroy it
#	4. then create file to use all spaces in the dataset
#	5. verify removing the first file should succeed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_009_pos
#
# DESCRIPTION:
#	Verify 'snapshot -r' and 'destroy -r' can correctly create and destroy 
#	snapshot tree respectively.
#
# STRATEGY:
# 1. Use the snapshot -r to create snapshot for top level pool 
# 2. Verify the children snapshots are created correctly.  
# 3. Use destroy -r to destroy the top level snapshot
# 4. Verify that all children snapshots are destroyed too.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_005_pos
#
# DESCRIPTION:
# to the originally snapshot'd file system, after the file
# system has been changed. Uses 'sum -r'.
#
# STRATEGY:
# 1) Create a file in the zfs dataset
# 2) Sum the file for later comparison
# 3) Create a snapshot of the dataset
# 4) Append to the original file
# 5) Verify both checksums match
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_001_pos
#
# DESCRIPTION:
# A zfs file system snapshot is identical to
# the originally snapshot'd file system, after the file
# system has been changed. Uses 'sum -r'.
#
# STRATEGY:
# 1. Create a file in the zfs file system
# 2. Checksum the file for later comparison
# 3. Create a snapshot of the dataset
# 4. Append to the original file
# 5. Verify the snapshot and file agree
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_004_pos
#
# DESCRIPTION:
# Create a null snapshot i.e. a snapshot created before file system
# activity is empty.
#
# STRATEGY:
# 1. Empty a file system
# 2. Take a snapshot of the empty file system.
# 3. Populate the file system
# 4. Verify the snapshot is still empty
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_008_pos
#
# DESCRIPTION:
# Verify that destroying snapshots returns space to the pool.
#
# STRATEGY:
# 1. Create a file system and populate it while snapshotting.
# 2. Destroy the snapshots and remove the files.
# 3. Verify the space returns to the pool.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-09-29)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rollback_003_pos
#
# DESCRIPTION:
# Verify that rollbacks succeed when there are nested file systems.
#
# STRATEGY:
# 1) Snapshot an empty file system and rollback
# 2) Create a file in the file system
# 3) Rollback the file system to empty
# 4) Create a nested file system with the same name as the file created in (2)
# 5) Verify a rollback succeeds
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-01-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_011_pos
#
# DESCRIPTION:
#	use 'snapshot -r' to create a snapshot tree, add some files to one child 
#	filesystem, rollback the child filesystem snapshot, verify that the child
# 	filesystem gets back to the status while taking the snapshot.	
#
# STRATEGY:
#	1. Add some files to a target child filesystem
#	2. snapshot -r the parent filesystem
#	3. Add some other files to the target child filesystem
#	4. rollback the child filesystem snapshot
#	5. verify that the child filesystem get back to the status while being 
#	   snapshot'd
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_015_pos
#
# DESCRIPTION:
#	Verify snapshot can be created or destroy via mkdir or rm 
#	in .zfs/snapshot.
#
# STRATEGY:
#	1. Verify make directories only successfully in .zfs/snapshot.
#	2. Verify snapshot can be created and destroy via mkdir and remove
#	directories in .zfs/snapshot.
#	3. Verify rollback to previous snapshot can succeed.
#	4. Verify remove directory in snapdir can destroy snapshot.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_006_pos
#
# DESCRIPTION:
# An archive of a zfs dataset and an archive of its snapshot
# changed sinced the snapshot was taken.
#
# STRATEGY:
# 1) Create some files in a ZFS dataset
# 2) Create a tarball of the dataset
# 3) Create a snapshot of the dataset
# 4) Remove all the files in the original dataset
# 5) Create a tarball of the snapshot
# 6) Extract each tarball and compare directory structures
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_002_pos
#
# DESCRIPTION:
# An archive of a zfs file system and an archive of its snapshot
# is identical even though the original file system has
# changed sinced the snapshot was taken.
#
# STRATEGY:
# 1) Create files in all of the zfs file systems
# 2) Create a tarball of the file system
# 3) Create a snapshot of the dataset
# 4) Remove all the files in the original file system
# 5) Create a tarball of the snapshot
# 6) Extract each tarball and compare directory structures
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rollback_001_pos
#
# DESCRIPTION:
# Populate a file system and take a snapshot. Add some more files to the
# file system and rollback to the last snapshot. Verify no post snapshot
# file exist.
#
# STRATEGY:
# 1. Empty a file system
# 2. Populate the file system
# 3. Take a snapshot of the file system
# 4. Add new files to the file system
# 5. Perform a rollback
# 6. Verify the snapshot and file system agree
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_013_pos
#
# DESCRIPTION:
#	verify that the snapshots created by 'snapshot -r' can be used for 
#	zfs send/recv 
#
# STRATEGY:
#	1. create a dataset tree and populate a filesystem
#	2. snapshot -r the dataset tree
#	3. select one snapshot used  for zfs send/recv
#	4. verify the data integrity after zfs send/recv
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_017_pos 
#
# DESCRIPTION:
#
# Directory structure of snapshots reflects filesystem structure.
#
# STRATEGY:
#
# This test makes sure that the directory structure of snapshots is
# a proper reflection of the filesystem the snapshot was taken of.
#
# 1. Create a simple directory structure of files and directories
# 2. Take a snapshot of the filesystem
# 3. Modify original filesystem
# 4. Walk down the snapshot directory structure verifying it
#    checking with both absolute and relative paths
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: clone_001_pos
#
# DESCRIPTION:
#	Create a snapshot from regular filesystem, volume, 
#	or filesystem upon volume, Build a clone file system
#	from the snapshot and verify new files can be written.
#
# STRATEGY:
# 	1. Create snapshot use 3 combination:
#		- Regular filesystem
#		- Regular volume
#		- Filesystem upon volume
# 	2. Clone a new file system from the snapshot
# 	3. Verify the cloned file system is writable
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-08-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_012_pos
#
# DESCRIPTION:
#	Verify 'snapshot -r' can create snapshot for promoted clone, and vice
#	versa, a clone filesystem from the snapshot created by 'snapshot -r' 
#	can be correctly promoted.
#
# STRATEGY:
#	1. Create a dataset tree
#	2. snapshot a filesystem and clone the snapshot
#	3. promote the clone
#	4. snapshot -r the dataset tree
#	5. verify that the snapshot of cloned filesystem is created correctly
#	6. clone a snapshot from the snapshot tree
#	7. promote the clone
#	8. verify that the clone is promoted correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_016_pos
#
# DESCRIPTION:
#	Verify renamed snapshots via mv can be destroyed
#
# STRATEGY:
#	1. Create snapshot
#	2. Rename the snapshot via mv command
#	2. Verify destroying the renamed snapshot via 'zfs destroy' succeeds
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-01-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_007_pos
#
# DESCRIPTION:
# Verify that many snapshots can be made on a zfs dataset.
#
# STRATEGY:
# 1) Create a file in the zfs dataset
# 2) Create a snapshot of the dataset
# 3) Remove all the files from the original dataset
# 4) For each snapshot directory verify consistency
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapshot_003_pos
#
# DESCRIPTION:
# Verify that many snapshots can be made on a zfs file system.
#
# STRATEGY:
# 1) Create a files in the zfs file system
# 2) Create a snapshot of the dataset
# 3) Remove all the files from the original file system
# 4) Verify consistency of each snapshot directory
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_002_pos
#
# DESCRIPTION:
#	Create a  scenario to verify the following zfs subcommands are logged.
# 	    create, destroy, clone, rename, snapshot, rollback, 
#	    set, inherit, receive, promote.
#
# STRATEGY:
#	1. Format zpool history to file $EXPECT_HISTORY.
#	2. Invoke every sub-commands to this mirror.
#	3. Compare 'zpool history' log with expected log.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_007_pos
#
# DESCRIPTION:
#	Verify command history moves with pool while pool being migrated 
#
# STRATEGY:
#	1. Import uniform platform and cross platform pools
#	2. Contract the command history of the imported pool
#	3. Compare imported history log with the previous log.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_003_pos
#
# DESCRIPTION:
#	zpool history can record and output huge log.
#
# STRATEGY:
#	1. Create two 100M virtual disk files.
#	2. Create test pool using the two virtual files.
#	3. Loop 2000 times to set compression to test pool.
#	4. Make sure 'zpool history' output correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_006_neg
#
# DESCRIPTION:
# 	Verify the following zfs subcommands are not logged.
#      	    list, get, mount, unmount, share, unshare, send
#
# STRATEGY:
#	1. Create a test pool.
#	2. Separately invoke zfs list|get|mount|unmount|share|unshare|send
#	3. Verify they was not recored in pool history.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_010_pos
#
# DESCRIPTION:
#	Verify internal long history information are correct.
#
# STRATEGY:
#	1. Create non-root test user and group.
#	2. Do some zfs operation test by root and non-root user.
#	3. Verify the long history information are correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_001_pos
#
# DESCRIPTION:
#	Create a scenario to verify the following zpool subcommands are logged.
#	    create, destroy, add, remove, offline, online, attach, detach, replace,
#	    scrub, export, import, clear, upgrade.
#
# STRATEGY:
#	1. Create three virtual disk files.
#	2. Create a three-way mirror.
#	3. Invoke every sub-commands to this mirror, except upgrade.
#	4. Compare 'zpool history' log with expected log.
#	5. Imported specified pool and upgrade it, verify 'upgrade' was logged.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_009_pos
#
# DESCRIPTION:
#	Verify the delegation internal history are correctly.
#
# 	ul$<id>    identifies permssions granted locally for this userid.
# 	ud$<id>    identifies permissions granted on descendent datasets for
#		   this userid.
#	
#	Ul$<id>    identifies permission sets granted locally for this userid.
#	Ud$<id>    identifies permission sets granted on descendent datasets for
#		   this	userid.
#	
#	gl$<id>    identifies permissions granted locally for this groupid.
#	gd$<id>    identifies permissions granted on descendent datasets for
#		   this groupid.
#	
#	Gl$<id>    identifies permission sets granted locally for this groupid.
#	Gd$<id>    identifies permission sets granted on descendent datasets for
#	           this groupid.
#
#	el$        identifies permissions granted locally for everyone.
#	ed$        identifies permissions granted on descendent datasets for
#		   everyone.
#	
#	El$        identifies permission sets granted locally for everyone.
#	Ed$        identifies permission sets granted to descendent datasets
#		   for everyone.
#	
#	c-$        identifies permission to create at dataset creation time.
#	C-$        identifies permission sets to grant locally at dataset
#		   creation time.
#	
#	s-$@<name> permissions defined in specified set @<name>
#	S-$@<name> Sets defined in named set @<name>
#
# STRATEGY:
#	1. Create test group and user.
#	2. Define permission sets and verify the internal history correctly.
#	3. Separately verify the internal history above is correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_008_pos
#
# DESCRIPTION:
#	Internal journal records all the recursively operations.
#
# STRATEGY:
#	1. Create a filesystem and several sub-filesystems in it.
#	2. Make recursively snapshot.
#	3. Verify internal journal records all the recursively operations.
#	4. Do the same verification to inherit, rollback and destroy.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-12-22)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_004_pos
#
# DESCRIPTION:
#	'zpool history' can copes with many simultaneous command.
#
# STRATEGY:
#	1. Create test pool and test fs.
#	2. Loop 100 times, set properties to test fs simultaneously.
#	3. Wait for all the command execution complete.
#	4. Make sure all the commands was logged by 'zpool history'.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: history_005_neg
#
# DESCRIPTION:
# 	Verify the following zpool subcommands are not logged.
#       	zpool list
#		zpool status
#		zpool iostat 
#
# STRATEGY:
#	1. Create a test pool.
#	2. Separately invoke zpool list|status|iostat
#	3. Verify they was not recored in pool history.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-07-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_clone_002_pos
#
# DESCRIPTION: 
#	If a storage pool has activated hot spares,
#	create clone and then invoke "zpool detach" with the original device,
#	the data in clone should keep integrity.
#
# STRATEGY:
#	1. Create a storage pool with hot spares activated.
#	2. Create some files, create a snapshot & clone upon filesystem
#	3. Activate a spare device to the pool
#	4. Create some files, create an new snapshot & clone upon filesystem
#	5. Do 'zpool detach' with the original device
#	6. Verify the 2 clones are all kept, and verify the data integrity within them.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_import_001_pos
#
# DESCRIPTION: 
#	If a storage pool has hot spare, 
#	regardless it has been activated or NOT,
#	invoke "zpool export" then import with this storage pool 
#	should runs successfully, and the data should keep integrity
#	after import.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Do 'zpool export' then 'zpool import' with following scernarios
#		- the hotspare is only in avaliable list
#		- the hotspare is activated
#		- the hotspare is activated but offline
#		- the hotspare is activated but the basic vdev is offline
#	3. Verify the export/import runs successfully,
#		and the data keep integrity after import
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-14)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_onoffline_004_neg
#
# DESCRIPTION:
#	If a hot spare has been activated,
#	turning that basic vdev offline and back online during I/O completes.
#	Make sure the integrity of the file system and the resilvering. 
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Activate the hot spare
#	3. Start some random I/O
#	4. Try 'zpool offline' & 'zpool online' with the basic vdev 
#	5. Verify the integrity of the file system and the resilvering.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_create_001_neg
#
# DESCRIPTION:
# 'zpool create [-f]' with hot spares will fail 
# while the hot spares belong to the following cases:
#	- existing pool
#	- nonexist device,
#	- part of an active pool,
#	- currently mounted,
#	- devices in /etc/vfstab,
#	- specified as the dedicated dump device,
#	- identical with the basic vdev within the pool,
#
# STRATEGY:
# 1. Create case scenarios
# 2. For each scenario, try to create a new pool with hot spares 
# 	of the virtual devices
# 3. Verify the creation is failed.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_remove_001_pos
#
# DESCRIPTION: 
# 	'zpool remove <pool> <vdev> ...' can successfully remove the specified 
# devices from the hot spares list of the given pool
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. Remove hot spares one by one	
#	3. Verify the devices are removed fromo the spare list 
#		of the given pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_detach_002_pos
#
# DESCRIPTION:
#	If a hot spare have been activated,
#	and invoke "zpool detach" with the original device,
#	then the hot spare will become a functioning device,
#	and automatically be removed from the list of available hot spares
#	then the spare is automatically removed once the replace completes.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Activate a spare device to the pool
#	3. Do 'zpool detach' with the original device
#	4. Verify the spare device will become a functioning device,
#		be removed from the list of available spares as well,
#		and the original drive will removed once replace completes.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_add_001_pos
#
# DESCRIPTION: 
# 	'zpool add <pool> spare <vdev> ...' can successfully add the specified 
# devices to the hot spares list of the given pool
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. Verify the devices are added to the spare list 
#		of the given pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_detach_003_pos
#
# DESCRIPTION:
#	If a hot spare have been activated,
#	and invoke "zpool replace" to replace the original device,
#	then the spare is automatically removed once the replace completes
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Activate a spare device to the pool
#	3. Do 'zpool replace' with the original device
#	4. Verify the original device will replace by the new device,
#		and the spare should return to available once replace completes.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_remove_004_pos
#
# DESCRIPTION: 
# 	'zpool remove <pool> <vdev> ...' can successfully remove the specified 
# devices from the hot spares even it no longer exists.
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. Export the pool
#	4. Remove the hotspare
#	5. Import the pool
#	6. Remove hot spares one by one	
#	7. Verify the devices are removed from the spare list 
#		of the given pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2008-02-25)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_snapshot_001_pos
#
# DESCRIPTION: 
#	If a hot spare have been activated, create snapshot upon filesystem,
#	then invoke "zpool detach" with this hot spare,
#	the data in snapshot should untouched.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Create some files, create a snapshot upon filesystem
#	3. Activate a spare device to the pool
#	4. Create some files, create an new snapshot upon filesystem
#	5. Do 'zpool detach' with the spare in device
#	6. Verify the 2 snapshots are all kept, and verify the data integrity within them.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_replace_003_pos
#
# DESCRIPTION: 
#	If an active spare that itself fails, it should detach the failed one
#       and attatch a different spare in its place
#       
#
# STRATEGY:
#	1. Create 1 storage pools with hot spares
#	2. Fail one vdev in one pool to make 1 hotspare in use.
#	3. Error out the active hotspace
#	4. Verify the failed one was detatched while the other spare attatched.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2009-04-16)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_export_001_neg
#
# DESCRIPTION: 
#	If 2 storage pools have shared hotspares, if the shared hotspare was used by
#	one of the pool, the export of the pool that use hotspare will fail.
#
# STRATEGY:
#	1. Create 2 storage pools with hot spares shared.
#	2. Fail one vdev in one pool to make the hotspare in use.
#	3. Export the pool that currently use the hotspare
#	4. Verify the export will failed with warning message.
#	5. Verify export -f will success.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2008-12-12)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_replace_002_neg
#
# DESCRIPTION: 
# 	'zpool replace <pool> <odev> <ndev>...' should return fail if
#	the size of hot spares is smaller than the basic vdev.
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. Try to replace the basic vdev with the smaller hot spares
#	4. Verify the the replace operation failes
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_onoffline_003_neg
#
# DESCRIPTION:
#	Regardless a hot spare is only in the available hot spare list,
#	or have been activated,
#	invoke "zpool offline" & "zpool online" with this hot spare
#	will fail with a return code of 1 and issue an error message.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Try 'zpool offline' & 'zpool online' with each hot spare 
#		of following condition
#		- only in the list of available hot spares (fail)
#		- have been activated (fail)
#	3. Verify offline/online results as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_remove_003_neg
#
# DESCRIPTION:
# Executing 'zpool remove' command with bad options fails.
#
# STRATEGY:
# 1. Create an array of badly formed 'zpool remove' options.
# 2. Execute each element of the array.
# 3. Verify an error code is returned.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-19)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_detach_001_pos
#
# DESCRIPTION: 
#	If a hot spare have been activated,
#	and invoke "zpool detach" with this hot spare,
#	it will be returned to the set of available spares,
#	the original drive will remain in its current position.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Activate a spare device to the pool
#	3. Do 'zpool detach' with the spare in device
#	4. Verify the spare device returned to the set of available spares,
#		and the original drive will remain in its current position.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_clone_001_pos
#
# DESCRIPTION: 
#	If a storage pool has activated hot spares, 
#	create clone and remove the hot spare,
#	the data in clone should keep integrity.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Create some files, create a snapshot & clone upon filesystem
#	3. Activate a spare device to the pool
#	4. Create some files, create an new snapshot & clone upon filesystem
#	5. Do 'zpool detach' with the spare in device
#	6. Verify the 2 clones are all kept, and verify the data integrity within them.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_snapshot_002_pos
#
# DESCRIPTION: 
#	If a storage pool has activated hot spares,
#	create snapshot and detach the basic vdev,
#	the hot spare should become the functional device, and
#	the data in snapshot should keep integrity.
#
# STRATEGY:
#	1. Create a storage pool with hot spares activated.
#	2. Create some files, create a snapshot upon filesystem
#	3. Activate a spare device to the pool
#	4. Create some files, create an new snapshot upon filesystem
#	5. Do 'zpool detach' with the original device
#	6. Verify the 2 snapshots are all kept, and verify the data integrity within them.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_replace_001_neg
#
# DESCRIPTION: 
# 	'zpool replace <pool> <odev> <ndev>...' should return fail if
#		- the hot spares is not within the hot spares of this pool.
#		- try to replace another hot spare while the basic vdev 
#			has an activated hot spare already. 
#		- try to replace log device.
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. For each scenario, try to replace the basic vdev with the given hot spares
#	4. Verify the the replace operation get failed 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_remove_002_neg
#
# DESCRIPTION: 
# 	'zpool remove <pool> <vdev> ...' should return fail if
#		- notexist device
#		- not within the hot spares of this pool
#		- hot spares that currently spared in
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. For each scenario, try to remove the hot spares
#	4. Verify the the remove operation get failed 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2005-09-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_detach_005_neg
#
# DESCRIPTION:
#	If a hot spare is only in the list of available hot spares
#	but have NOT been activated,
#	invoke "zpool detach" with this hot spare will fail with
#	a return code of 255 and issue an error message.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Do 'zpool detach' with the hot spare device
#	4. Verify the operation fail with return code of 255
#		and issue an error message.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_add_003_neg
#
# DESCRIPTION: 
# 'zpool add' with hot spares will fail
# while the hot spares belong to the following cases:
#	- nonexist device,
#	- part of an active pool,
#	- currently mounted,
#	- devices in /etc/vfstab,
#	- specified as the dedicated dump device,
#	- identical with the basic or spares vdev within the pool,
#	- belong to a exported or potentially active ZFS pool,
#	- a volume device that belong to the given pool,
#
# STRATEGY:
#	1. Create case scenarios
#	2. For each scenario, try to add [-f] the device to the pool
#	3. Verify the add operation failes as expected.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_detach_004_pos
#
# DESCRIPTION:
#	If a hot spare is activated, 
#	and invoke "zpool replace" with this hot spare to another hot spare,
#	the operation should run successfully.
#
# STRATEGY:
#	1. Create a storage pool with multiple hot spares
#	2. Activate a hot spare by 'zpool replace' with the basic dev,
#		make sure there still have enough hot spare in available list.
#	3. Do 'zpool replace' with the hot spare to another AVAIL hot spare.
#	4. Verify the operation runs successfully.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_add_002_pos
#
# DESCRIPTION: 
# 	'zpool add <pool> spare <vdev> ...' can successfully add the specified 
# 	devices to the available list of the given pool while 
#	there has activated hotspare already. 
#
# STRATEGY:
#	1. Create a storage pool
#	2. Add hot spare devices to the pool
#	3. Activate some of the hot spares.
#	3. Verify the following devices could add to the spare list 
#		of the given pool successfully
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: hotspare_scrub_001_pos
#
# DESCRIPTION: 
#	If a storage pool has hot spare, 
#	regardless it has been activated or NOT,
#	invoke "zpool scrub" with this storage pool should successful.
#
# STRATEGY:
#	1. Create a storage pool with hot spares
#	2. Make the storage pool dirty.
#	3. Do 'zpool scrub' with following scernarios
#		- the hotspare is only in avaliable list
#		- the hotspare is activated
#	4. Verify the scrub runs successfully.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING STATUS: COMPLETED (2006-06-07)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_003_pos
#
# DESCRIPTION:
# Ensure that the fsdb(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS directory with a number of files.
# 2. Run fsdb against the raw device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_007_pos
#
# DESCRIPTION:
# Ensure that the fstyp(1M) utility succeeds on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run fstyp(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_002_pos
#
# DESCRIPTION:
# Ensure that the labelit(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run labelit(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_006_pos
#
# DESCRIPTION:
# Ensure that the fsirand(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run fsirand(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_004_pos
#
# DESCRIPTION:
# Ensure that the quotaon(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Enable a quota on a ZFS file system.
# 2. Run quotaon against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_008_pos
#
# DESCRIPTION:
# Ensure that the ncheck(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run ncheck(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_009_pos
#
# DESCRIPTION:
# Ensure that the tunefs(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run tunefs(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_001_pos
#
# DESCRIPTION:
# Ensure that the clri(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS directory with a number of files.
# 2. Run clri against the raw device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: utils_test_005_pos
#
# DESCRIPTION:
# Ensure that the ff(1M) utility fails on a ZFS file system.
#
# STRATEGY:
# 1. Populate a ZFS file system with some files.
# 2. Run ff(1M) against the device.
# 3. Ensure it fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mounttest
#
# DESCRIPTION:
# zfs mount and unmount commands should mount and unmount existing
# file systems.
#
# STRATEGY:
# 1. Call zfs mount command
# 2. Make sure the file systems were mounted
# 3. Call zfs unmount command
# 4. Make sure the file systems were unmounted
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_008_neg
#
# DESCRIPTION:
#	A raidz/raidz2 log is not supported. 
#
# STRATEGY:
#	1. Try to create pool with unsupported type
#	2. Verify failed to create pool.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_005_pos
#
# DESCRIPTION:
#	Detaching a log device passes.
#
# STRATEGY:
#	1. Create pool with mirror log devices.
#	2. Detaching a log device
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_001_pos
#
# DESCRIPTION:
#	Creating a pool with a log device succeeds.
#
# STRATEGY:
#	1. Create pool with separated log devices.
#	2. Display pool status
#	3. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_014_pos
#
# DESCRIPTION:
#	log device can survive when one of pool device get corrupted
#
# STRATEGY:
#	1. Create pool with slog devices
#	2. remove one disk
#	3. Verify the log is fine
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-05-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_011_neg
#
# DESCRIPTION:
#	Offline and online a log device passes.
#
# STRATEGY:
#	1. Create pool with mirror log devices.
#	2. Offine and online a log device
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_010_neg
#
# DESCRIPTION:
#	Slog device can not be replaced with spare device
#
# STRATEGY:
#	1. Create a pool with hotspare and log devices.
#	2. Verify slog device can not be replaced with hotspare device.
#	3. Create pool2 with hotspare
#	4. Verify slog device can not be replaced with hotspare device in pool2.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_009_neg
#
# DESCRIPTION:
#	A raidz/raidz2 log can not be added to existed pool.
#
# STRATEGY:
#	1. Create pool with or without log.
#	2. Add a raidz/raidz2 log to this pool.
#	3. Verify failed to add.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_004_pos
#
# DESCRIPTION:
#	Attaching a log device passes.
#
# STRATEGY:
#	1. Create pool with separated log devices.
#	2. Attaching a log device for existing log device
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_013_pos
#
# DESCRIPTION:
#	Verify slog device can be disk, file, lofi device or any device that
#	presents a block interface.
#
# STRATEGY:
#	1. Create a pool
#	2. Loop to add different object as slog
#	3. Verify it passes
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_012_neg
#
# DESCRIPTION:
#	Pool can survive when one of mirror log device get corrupted
#
# STRATEGY:
#	1. Create pool with mirror slog devices
#	2. Make corrupted on one disk
#	3. Verify the pool is fine
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_006_pos
#
# DESCRIPTION:
#	Replacing a log device passes.
#
# STRATEGY:
#	1. Create pool with log devices.
#	2. Replacing one log device
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_002_pos
#
# DESCRIPTION:
#	Adding a log device to normal pool works.
#
# STRATEGY:
#	1. Create pool
#	2. Add log devices with different configuration
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_007_pos
#
# DESCRIPTION:
#	Exporting and importing pool with log devices passes.
#
# STRATEGY:
#	1. Create pool with log devices.
#	2. Export and import the pool
#	3. Display pool status
#	4. Destroy and import the pool again
#	5. Display pool status
#	6. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: slog_003_pos
#
# DESCRIPTION:
#	Adding an extra log device works
#
# STRATEGY:
#	1. Create pool with separated log devices.
#	2. Add an extra log devices
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-06-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: rename_dirs_001_pos
#
# DESCRIPTION:
# Create two directory trees in ZFS filesystem, and concurently rename
# directory across the two trees. ZFS should be able to handle the race
# situation.
#
# STRATEGY:
# 1. Create a ZFS filesystem
# 2. Make two directory tree in the zfs file system
# 3. Continually rename directory from one tree to another tree in two process
# 4. After the specified time duration, the system should not be panic.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zinject_001_pos
#
# DESCRIPTION:
#
# Inject an error into the plain file contents of a file.
# Verify fmdump will get the expect ereport
#
# STRATEGY:
# 1) Populate ZFS file system
# 2) Inject an error into the plain file contents of a file.
# 3) Verify fmdump get the ereport as expect.
#	<Errno>		<Expect ereport>		<Comments>	
#	io 		ereport.fs.zfs.io
#			ereport.fs.zfs.data
#	checksum	ereport.fs.zfs.checksum		Non-stripe pool
#			ereport.fs.zfs.data
#	checksum	ereport.fs.zfs.data		Stripe pool only
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zinject_004_pos
#
# DESCRIPTION:
#
# Inject an error into the device of a pool.
# Verify fmdump will get the expect ereport, 
# and the fault class of "fault.fs.zfs.vdev.io" be generated.
#
# STRATEGY:
# 1) Populate ZFS file system
# 2) Inject an error into the device of the pool.
# 3) Verify fmdump get the ereport as expected.
#	<Errno>		<Expect ereport>
#	nxio		ereport.fs.zfs.probe_failure
#	io		ereport.fs.zfs.probe_failure
# 4) Verify the fault class of "fault.fs.zfs.vdev.io" be generated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-05-31)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zinject_002_pos
#
# DESCRIPTION:
#
# Inject an error into the metadnode in the block
# corresponding to the dnode for a file or directory
# Verify fmdump will get the expect ereport
#
# STRATEGY:
# 1) Populate ZFS file system
# 2) Inject an error into the metadnode in the block.
# 3) Verify fmdump get the ereport as expect.
#	<Errno>	 <Expect ereport>		<Comments>
#	io		ereport.fs.zfs.io
#			ereport.fs.zfs.data
#	checksum	ereport.fs.zfs.checksum	Non-stripe pool
#			ereport.fs.zfs.data
#	checksum	ereport.fs.zfs.data     Stripe pool only
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: zinject_003_pos
#
# DESCRIPTION:
#
# Inject an error into the first metadnode in the block
# Verify the filesystem unmountable since dnode be injected.
#
# STRATEGY:
# 1) Populate ZFS file system
# 2) Inject an error into the first metadnode in the block.
# 3) Verify the filesystem unmountable, 
#	and 'zpool status -v' will display the error as expect.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-02-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: inherit_001_pos
#
# DESCRIPTION:
# Test that properties are correctly inherited using 'zfs set',
# 'zfs inherit' and 'zfs inherit -r'.
#
# STRATEGY:
# 1) Read a configX.cfg file and create the specified datasets
# 2) Read a stateX.cfg file and execute the commands within it
# and verify that the properties have the correct values
# 3) Repeat steps 1-2 for each configX and stateX files found.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: interop_001_pos
#
# DESCRIPTION:
# Create a SVM device and add this to an existing ZFS pool
#
# STRATEGY:
# 1. Create a SVM metadevice
# 2. Create a ZFS file system
# 3. Add SVM metadevice to the ZFS pool
# 4. Create files and fill the pool.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cachefile_001_pos
#
# DESCRIPTION:
#
# Creating a pool with "cachefile" set doesn't update zpool.cache
#
# STRATEGY:
# 1. Create a pool with the cachefile property set
# 2. Verify that the pool doesn't have an entry in zpool.cache
# 3. Verify the cachefile property is set
# 4. Create a pool without the cachefile property
# 5. Verify the cachefile property isn't set
# 6. Verify that zpool.cache contains an entry for the pool
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-09-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cachefile_004_pos
#
# DESCRIPTION:
#	Verify set, export and destroy when cachefile is set on pool.
#
# STRATEGY:
#	1. Create two pools with one same cahcefile1.
#	2. Set cachefile of the two pools to another same cachefile2.
#	3. Verify cachefile1 not exist.
#	4. Export the two pools.
#	5. Verify cachefile2 not exist.
#	6. Import the two pools and set cachefile to cachefile2.
#	7. Destroy the two pools.
#	8. Verify cachefile2 not exist.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cachefile_002_pos
#
# DESCRIPTION:
#
# Importing a pool with "cachefile" set doesn't update zpool.cache
#
# STRATEGY:
# 1. Create a pool with the cachefile property set
# 2. Verify the pool doesn't have an entry in zpool.cache
# 3. Export the pool
# 4. Import the pool
# 5. Verify the pool does have an entry in zpool.cache
# 6. Export the pool
# 7. Import the pool -o cachefile=<cachefile>
# 8. Verify the pool doesn't have an entry in zpool.cache
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-09-05)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cachefile_003_pos
#
# DESCRIPTION:
#
# Setting altroot=<path> and cachefile=$CPATH for zpool create is succeed
#
# STRATEGY:
# 1. Attempt to create a pool with -o altroot=<path> -o cachefile=<value>
# 2. Verify the command succeed
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-09-10)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mdb_001_pos
#
# DESCRIPTION:
#	Verify that the ZFS mdb dcmds and walkers are working as expected.
#
# STRATEGY:
#	1) Given a list of dcmds and walkers
#	2) Step through each element of the list
#	3) Verify the output by checking for "mdb:" in the output string
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-10-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mv_files_002_pos
#
# DESCRIPTION:
# Doing a 'mv' of a large amount of files between two directories across
# two zfs filesystems works without errors.
#
# STRATEGY:
#
# 1. create a pool and two zfs filesystems
# 2. create a directory in each filesystem
# 3. create a large number of files in a directory of a filesystem
# 4. Move files from the directory to another directory in another
# filesystem and back again
# 5. validate file number
# 6. increase the number of files to $MVNUMFILES + $MVNUMINCR
# 7. repeat steps 3,4,5,6 above
# 8. verify the data integrity
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: mv_files_001_pos
#
# DESCRIPTION:
# Doing a 'mv' of a large amount of files between two directories
# within a zfs filesystem works without errors.
#
# STRATEGY:
#
# 1. create a pool and a zfs filesystem
# 2. create two directories within the filesystem
# 3. create a large number of files within a directory
# 4. Move files from one directory to another and back again
# 5. validate file number
# 6. increase the number of files to $MVNUMFILES + $MVNUMINCR
# 7. repeat steps 3,4,5,6 above
# 8. verify the data integrity
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_002_pos
#
# DESCRIPTION:
#	Quotas are enforced using the minimum of the two properties: 
#	quota & refquota
#
# STRATEGY:
#	1. Set value for quota and refquota. Quota less than refquota.
#	2. Creating file which should be limited by quota.
#	3. Switch the value of quota and refquota.
#	4. Verify file should be limited by refquota.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_003_pos
#
# DESCRIPTION:
#	Sub-filesystem quotas are not enforced by property 'refquota'
#
# STRATEGY:
#	1. Setting quota and refquota for parent. refquota < quota
#	2. Verify sub-filesystem will not be limited by refquota
#	3. Verify sub-filesystem will only be limited by quota
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_006_neg
#
# DESCRIPTION:
#	'zfs set refquota/refreserv' can handle incorrect arguments correctly.
#
# STRATEGY:
#	1. Setup incorrect arguments arrays.
#	2. Set the bad argument to refquota.
#	3. Verify zfs can handle it correctly.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-09)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_005_pos
#
# DESCRIPTION:
#	refquotas are not limited by sub-filesystem snapshots.
#
# STRATEGY:
#	1. Setting refquota < quota for parent
#	2. Create file in sub-filesytem, take snapshot and remove the file
#	3. Verify sub-filesystem snapshot will not consume refquota 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_001_pos
#
# DESCRIPTION:
#	refquota limits the amount of space a dataset can consume, but does
#	not include space used by descendents.
#
# STRATEGY:
#	1. Setting refquota in given filesystem
#	2. Create descendent filesystem
#	3. Verify refquota limits the amount of space a dataset can consume
#	4. Verify the limit does not impact descendents
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-12-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: refquota_004_pos
#
# DESCRIPTION:
#	refquotas are not limited by snapshots.
#
# STRATEGY:
#	1. Setting refquota < quota
#	2. Create file in filesytem, take snapshot and remove the file
#	3. Verify snapshot will not consume refquota 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-11-02)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: scrub_mirror_001_pos
#
# DESCRIPTION:
# The primary side of a zpool mirror can be zeroed without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem mirror
# 2) dd from /dev/zero over the primary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: scrub_mirror_004_pos
#
# DESCRIPTION:
# The secondary side of a zpool mirror can be mangled causing without damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem in the mirrored pool
# 2) dd from /dev/urandom over the secondary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: scrub_mirror_002_pos
#
# DESCRIPTION:
# The secondary side of a zpool mirror can be zeroed without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem in the mirrored pool
# 2) dd from /dev/zero over the secondary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: scrub_mirror_003_pos
#
# DESCRIPTION:
# The primary side of a zpool mirror can be mangled causing without damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem mirror
# 2) dd from /dev/urandom over the primary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-10-20)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_006_neg
#
# DESCRIPTION:
#
# Can't set a quota to less than currently being used by the dataset.
#
# STRATEGY:
# 1) Create a filesystem
# 2) Set a quota on the filesystem that is lower than the space
#	currently in use.
# 3) Verify that the attempt fails.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-09-13)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_003_pos
#
# DESCRIPTION:
# A ZFS file system quota limits the amount of pool space
# available to a file system dataset. Apply a quota and verify
# that no more file creates are permitted.
#
# NOTE: THis test applies to a dataset rather than a file system.
#
# STRATEGY:
# 1) Apply quota to ZFS file system dataset
# 2) Create a file which is larger than the set quota
# 3) Verify that the resulting file size is less than the quota limit
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_002_pos
#
# DESCRIPTION:
# A zfs file system quota limits the amount of pool space
# available to a given ZFS file system. Once exceeded, it is impossible
# to write any more files to the file system.
#
# STRATEGY:
# 1) Apply quota to the ZFS file system
# 2) Exceed the quota
# 3) Attempt to write another file
# 4) Verify the attempt fails with error code 49 (EDQUOTA)
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_004_pos
#
# DESCRIPTION:
# A zfs file system quota limits the amount of pool space
# available to a given ZFS file system dataset. Once exceeded, it
# is impossible to write any more files to the file system.
#
# STRATEGY:
# 1) Apply quota to the ZFS file system dataset
# 2) Exceed the quota
# 3) Attempt to write another file
# 4) Verify the attempt fails with error code 49 (EDQUOTA)
#
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_001_pos
#
# DESCRIPTION:
#
# A ZFS file system quota limits the amount of pool space
# available to a file system. Apply a quota and verify
# that no more file creates are permitted.
#
# STRATEGY:
# 1) Apply quota to ZFS file system
# 2) Create a file which is larger than the set quota
# 3) Verify that the resulting file size is less than the quota limit
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: quota_005_pos
#
# DESCRIPTION:
#
# Verify that quota doesn't inherit its value from parent.
#
# STRATEGY:
# 1) Set quota for parents
# 2) Create a filesystem tree
# 3) Verify that the 'quota' for descendent doesnot inherit the value.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2006-08-17)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: poolversion_001_pos
#
# DESCRIPTION:
#
# zpool set version can upgrade a pool
#
# STRATEGY:
# 1. Taking a version 1 pool
# 2. For all known versions, set the version of the pool using zpool set
# 3. Verify that pools version
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: poolversion_002_pos
#
# DESCRIPTION:
#
# zpool set version can only increment pool version
#
# STRATEGY:
# 1. Set a version 1 pool to be a version 6 pool
# 2. Verify it's set to version 6
# 3. Attempt to set prior versions
# 4. Verify it's still set to version 6
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2007-07-27)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapused_002_pos
#
# DESCRIPTION:
#	Verify usedbychildren is correct.
#
# STRATEGY:
#	1. Create a filesystem.
#	2. Create sub filesystem and make file in it.
#	3. Set reservation of the sub filesystem.
#	4. Create volume under it.
#	5. Snapshot it.
#	6. Check usedbychildren is correct. 
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapused_003_pos
#
# DESCRIPTION:
#	Verify usedbydataset is correct.
#
# STRATEGY:
#	1. Create a filesystem.
#	2. Make file in the filesystem.
#	3. Snapshot it.
#	4. Clone it and make file in the cloned filesystem.
#	5. Check usedbydataset is correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapused_001_pos 
#
# DESCRIPTION:
#	Verify used is correct.
#
# STRATEGY:
#	1. Create a filesystem.
#	2. Set refreservation of the filesystem.
#	3. Make file in the filesystem.
#	4. Create sub filesystem and make file in it.
#	5. Create volume under it.
#	6. Snapshot it.
#	7. Check used=usedbychildren+usedbydataset+
#		usedbyrefreservation+usedbysnapshots.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapused_005_pos
#
# DESCRIPTION:
#	Verify usedbysnapshots is correct.
#
# STRATEGY:
#	1. Create a filesystem.
#	2. Make file in the filesystem.
#	3. Snapshot it.
#	4. Check check_usedbysnapshots is correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: snapused_004_pos
#
# DESCRIPTION:
#	Verify usedbyrefreservation is correct.
#
# STRATEGY:
#	1. Create a filesystem.
#	2. Set refreservation of the filesystem.
#	3. Make file in the filesystem.
#	4. Create sub filesystem and make file in it.
#	5. Set refreservation of the sub filesystem.
#	6. Create volume under it.
#	7. Snapshot it.
#	8. Clone it and set refreservation of the cloned filesystem.
#	9. Makefile the cloned filesystem.
#	10. Check usedbyrefreservation is correct.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2009-04-28)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: grow_pool_001_pos
#
# DESCRIPTION:
# A ZFS file system is limited by the amount of disk space
# available to the pool. Growing the pool by adding a disk
# increases the amount of space.
#
# STRATEGY:
# 1) Fill a ZFS filesystem until ENOSPC by creating a large file
# 2) Grow the pool by adding a disk
# 3) Verify that more data can now be written to the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: sparse_001_pos
#
# DESCRIPTION:
# Holes in ZFS files work correctly.
#
# STRATEGY:
# 1. Open file
# 2. Write random blocks in random places
# 3. Read each block back to check for correctness.
# 4. Repeat steps 2 and 3 lots of times
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: clean_mirror_001_pos
#
# DESCRIPTION:
# The primary side of a zpool mirror can be zeroed without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem mirror
# 2) dd from /dev/zero over the primary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: clean_mirror_002_pos
#
# DESCRIPTION:
# The secondary side of a zpool mirror can be zeroed without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem in the mirrored pool
# 2) dd from /dev/zero over the secondary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: clean_mirror_004_pos
#
# DESCRIPTION:
# The secondary side of a zpool mirror can be mangled without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem in the mirrored pool
# 2) dd from /dev/urandom over the secondary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: clean_mirror_003_pos
#
# DESCRIPTION:
# The primary side of a zpool mirror can be mangled without causing damage
# to the data in the pool
#
# STRATEGY:
# 1) Write several files to the ZFS filesystem mirror
# 2) dd from /dev/urandom over the primary side of the mirror
# 3) verify that all the file contents are unchanged on the file system
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_010_neg
#
# DESCRIPTION:
#	Verify cache device can only be disk or slice.
#
# STRATEGY:
#	1. Create a pool
#	2. Loop to add different object as cache
#	3. Verify it fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_011_pos
#
# DESCRIPTION:
#	Remove cache device from pool with spare device should succeed.
#
# STRATEGY:
#	1. Create pool with cache devices and spare devices
#	2. Remove cache device from the pool
#	3. The upper action should succeed
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-12-01)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_005_neg
#
# DESCRIPTION:
#	Replacing a cache device fails.
#
# STRATEGY:
#	1. Create pool with cache devices.
#	2. Replacing one cache device
#	3. Verify replace fails
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-03-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_001_pos
#
# DESCRIPTION:
#	Creating a pool with a cache device succeeds.
#
# STRATEGY:
#	1. Create pool with separated cache devices.
#	2. Display pool status
#	3. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_009_pos
#
# DESCRIPTION:
#	Offline and online a cache device succeed.
#
# STRATEGY:
#	1. Create pool with mirror cache devices.
#	2. Offine and online a cache device
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_004_neg
#
# DESCRIPTION:
#	Attaching a cache device fails.
#
# STRATEGY:
#	1. Create pool with separated cache devices.
#	2. Attaching a cache device for existing cache device
#	3. Verify the operation fails
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-03-26)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_008_neg
#
# DESCRIPTION:
#	A mirror/raidz/raidz2 cache can not be added to existed pool.
#
# STRATEGY:
#	1. Create pool with or without cache.
#	2. Add a mirror/raidz/raidz2 cache to this pool.
#	3. Verify failed to add.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_003_pos
#
# DESCRIPTION:
#	Adding an extra cache device works
#
# STRATEGY:
#	1. Create pool with separated cache devices.
#	2. Add an extra cache devices
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_002_pos
#
# DESCRIPTION:
#	Adding a cache device to normal pool works.
#
# STRATEGY:
#	1. Create pool
#	2. Add cache devices with different configuration
#	3. Display pool status
#	4. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_006_pos
#
# DESCRIPTION:
#	Exporting and importing pool with cache devices passes.
#
# STRATEGY:
#	1. Create pool with cache devices.
#	2. Export and import the pool
#	3. Display pool status
#	4. Destroy and import the pool again
#	5. Display pool status
#	6. Destroy and loop to create pool with different configuration.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: cache_007_neg
#
# DESCRIPTION:
#	A mirror/raidz/raidz2 cache is not supported. 
#
# STRATEGY:
#	1. Try to create pool with unsupported type
#	2. Verify failed to create pool.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2008-04-24)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: write_dirs_001_pos
#
# DESCRIPTION:
# Create as many directories with 50 big files each until the file system
# is full. The zfs file system should be stable and works well.
#
# STRATEGY:
# 1. Create a pool & dateset
# 2. Make directories in the zfs file system
# 3. Create 50 big files in each directories
# 4. Test case exit when the disk is full.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start
#
# ID: write_dirs_002_pos
#
# DESCRIPTION:
# Create as many directories with 5000 files each until the file system
# is full. The zfs file system should be work well and stable.
#
# STRATEGY:
# 1. Create a pool & dateset
# 2. Make directories in the zfs file system
# 3. Create 5000 files in each directories
# 4. Test case exit when the disk is full
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-04)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: atime_001_pos
# 
# DESCRIPTION:
# When atime=on, verify the access time for files is updated when read. It
# is available to fs and clone. To snapshot, it is unavailable.
#
# STRATEGY:
# 1. Create pool and fs.
# 2. Create '$TESTFILE' for fs.
# 3. Create snapshot and clone.
# 4. Setting atime=on on datasets except snapshot, and read '$TESTFILE'.
# 5. Expect the access time is updated on datasets except snapshot.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
###############################################################################
# __stc_assertion_start 
#
# ID: atime_002_neg
# 
# DESCRIPTION:
# When atime=off, verify the access time for files is not updated when read. 
# It is available to pool, fs snapshot and clone.
#
# STRATEGY:
# 1. Create pool, fs. 
# 2. Create '$TESTFILE' for fs.
# 3. Create snapshot and clone.
# 4. Setting atime=off on dataset and read '$TESTFILE'.
# 5. Verify the access time is not updated.
#
# TESTABILITY: explicit
#
# TEST_AUTOMATION_LEVEL: automated
#
# CODING_STATUS: COMPLETED (2005-07-11)
#
# __stc_assertion_end
###############################################################################
